<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: AQL | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/aql/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-01-14T01:39:31+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Returning Results From AQL INSERT/REMOVE/REPLACE Operations]]></title>
    <link href="http://jsteemann.github.io/blog/2015/01/13/returning-results-from-aql-insert-update-remove-operations/"/>
    <updated>2015-01-13T22:53:40+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/01/13/returning-results-from-aql-insert-update-remove-operations</id>
    <content type="html"><![CDATA[<p>ArangoDB provides many options for finding and modifying data. Though there are
several more specialized operation, data-modification AQL queries are the most
general solution in ArangoDB. They allow to find documents using arbitrary filter
criteria, and to modify or remove the documents once found.</p>

<p>Such <em>find-and-modify</em> operations can be executed with multiple queries (one for
the find step, one for the modification step), or with a single query. Putting
both steps into a single query will often save roundtrips between the application
and the database and thus may be preferred over executing the steps separately.
Putting both the find and the modify step into the same query also prevents other
operations from interfering in between and tampering with the underlying data.</p>

<p>Now what if the application not only requires the data to be updated, but also needs
to keep track of which documents were found and modified by a <em>find-and-modify</em>
query? This is often required when an application needs to keep database
data in sync with data in some other datastore (e.g. the filesystem or a remote
service).</p>

<p>The pattern I would dub <em>find-modify-return</em> would be useful for this.</p>

<!-- more -->


<p>Unfortunately it hasn&rsquo;t been supported in AQL until very recently.
We have been asked for this so many times that I stopped counting.</p>

<p>I am glad that this got better with ArangoDB 2.4. Finding, modifying and returning
documents is now possible from the same AQL query. The solution is not yet perfect,
but at least it provides basic <em>find-modify-return</em> functionality for a lot of
use cases, including multi-document, multi-collection queries.</p>

<h2>INSERT</h2>

<p>Let&rsquo;s start with an <code>INSERT</code> operation as it is the most simple to explain:</p>

<p><code>plain insert query, not returning anything
FOR i IN 1..10
  INSERT { name: CONCAT("user", i), someValue: RAND() } IN testAccounts
</code></p>

<p>The above query does not explicitly specify the <code>_key</code> attribute for the inserted
documents, meaning the database will create the documents keys automatically.
Additionally, the query assigns a random value to an attribute <code>someValue</code>.
Getting to know the created keys or random values required an extra lookup query
before 2.4. Extra queries are something one wants to avoid for efficiency
reasons.</p>

<p>With 2.4, adding two lines at the end of the query will solve the problem so
that everything can be done in one query:</p>

<p><code>plain insert query, returning the inserted documents
FOR i IN 1..10
  INSERT { name: CONCAT("user", i), someValue: RAND() } IN testAccounts
  LET inserted = NEW
  RETURN inserted
</code></p>

<p>Using a <code>LET</code> with the pseudo-value of <code>NEW</code> after the <code>INSERT</code> together with
a final <code>RETURN</code> statement will make the inserted documents appear in the query
result. Note that the full documents will be returned and not just the specified
attributes. This allows the application to track of all document attributes,
even the auto-generated ones.</p>

<h2>REMOVE</h2>

<p>The mechanism works for <code>REMOVE</code> statements, too.</p>

<p>Time to use a different example for this. Let&rsquo;s assume session metadata are
stored in the database, and some bigger session files are stored somewhere
in the filesystem. If the application needs to remove expired sessions, it
will need to clean up in both places. It will first query the database to
find the expired sessions, only to remove them from the filesystem (if the
filesystem sessions were organized by session id from the database) and to
finally remove the sessions from database itself.</p>

<p>The following query could be used to find removal candidates:</p>

<p><code>plain query for finding removal candidates
FOR session IN sessions
  FILTER session.dateExpires &lt; DATE_NOW()
  LIMIT 1000
  RETURN session._key
</code></p>

<p>The results of this query can be used for cleaning up sessions in the filesystem,
and for finally removing the sessions from the database. However, this would require
an extra <code>REMOVE</code> query. And didn&rsquo;t we say we would like to avoid extra queries?</p>

<p>The good news is that in 2.4 we can by putting at least the database part into a
single query:</p>

<p><code>plain query for removing and returning expired sessions
FOR session IN sessions
  FILTER session.dateExpires &lt; DATE_NOW()
  LIMIT 1000
  REMOVE session IN sessions
  LET removed = OLD
  RETURN removed
</code></p>

<p>The last two lines of the above query make sure the removed sessions are
returned to the application, so the application can perform any filesystem
cleanup using the session ids from the database.</p>

<p>Note that in case of <code>REMOVE</code> one has to use the pseudo-value <code>OLD</code> because
<code>REMOVE</code> can only return documents before removal. In the case of <code>INSERT</code>
we can only refer to the pseudo-value <code>NEW</code>.</p>

<h2>UPDATE and REPLACE</h2>

<p>So far we saw <code>INSERT</code> and <code>REMOVE</code>, but there are also <code>UPDATE</code> and <code>REPLACE</code>.
I will handle <code>UPDATE</code> and <code>REPLACE</code> in one go. All the following refers to
<code>UPDATE</code>, but does apply to <code>REPLACE</code> as well.</p>

<p>The mechanism to return documents from a query is the same as
already demonstrated: simply append the <code>LET ... RETURN</code> sequence to the end
of the original query.</p>

<p>In addition, <code>UPDATE</code> allows to return either the <em>old</em> document revisions
(before modification) or the <em>new</em> document revisions (after modification).
This can be expressed by using either <code>OLD</code> or <code>NEW</code> in the final <code>LET</code>
statement.</p>

<p>Following is a more complex example for an <code>UPDATE</code> query that aggregates data
from one collection (<code>phraseOccurrences</code>) in order to find and modify matching
documents in another collection (<code>phrases</code>). It will return the documents from
<code>phrases</code> before they got modified:</p>

<p><code>plain update query returning "old" documents
FOR po IN phraseOccurrences
  FILTER ! po.isSuspicious
  FILTER po.dateMentioned &gt;= '2015-01-01' &amp;&amp; po.dateMentioned &lt;= '2015-01-09'
  COLLECT phrase = po.phrase WITH COUNT INTO occurrences
  FILTER occurrences % 42 != 23
  LIMIT 13
  UPDATE phrase WITH { isSuspicious: true } IN phrases
  LET previous = OLD  /* returns document revisions before UPDATE */
  RETURN previous
</code></p>

<p>If we are interested in what the documents in <code>phrases</code> look like with the
<code>UPDATE</code> applied, we can use <code>NEW</code> instead:</p>

<p><code>plain update query returning "new" documents
FOR po IN phraseOccurrences
  FILTER ! po.isSuspicious
  FILTER po.dateMentioned &gt;= '2015-01-01' &amp;&amp; po.dateMentioned &lt;= '2015-01-09'
  COLLECT phrase = po.phrase WITH COUNT INTO occurrences
  FILTER occurrences % 42 != 23
  LIMIT 13
  UPDATE phrase WITH { isSuspicious: true } IN phrases
  LET modified = NEW  /* returns document revisions after UPDATE */
  RETURN modified
</code></p>

<p>Note that the full documents will be returned here, and not just the attributes
specified in or modified by the <code>UPDATE</code> operation.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[99 Bottles of Beer]]></title>
    <link href="http://jsteemann.github.io/blog/2014/12/14/99-bottles-of-beer/"/>
    <updated>2014-12-14T13:14:28+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/12/14/99-bottles-of-beer</id>
    <content type="html"><![CDATA[<p><a href="http://www.99-bottles-of-beer.net/">99 bottles of beer</a> in AQL:</p>

<p>```
FOR quant IN 99..0
  LET toPrint = (</p>

<pre><code>quant &gt; 1 ? 
  CONCAT(TO_STRING(quant), " bottles of beer on the wall, ", TO_STRING(quant), " bottles of beer.") : 
  (quant == 1 ? 
    "1 bottle of beer on the wall, 1 bottle of beer." :
    "No more bottles of beer on the wall, no more bottles of beer."
  )
</code></pre>

<p>  )</p>

<p>  LET suffix = (</p>

<pre><code>quant &gt; 2 ? 
  CONCAT(TO_STRING(quant - 1), " bottles of beer on the wall.") :
  (quant == 2 ? 
    "1 bottle of beer on the wall." :
    "no more bottles of beer on the wall."
  )
</code></pre>

<p>  )</p>

<p>  LET result = (</p>

<pre><code>quant &gt; 0 ? 
  CONCAT(toPrint, "\nTake one down, pass it around, ", suffix) : 
  CONCAT(toPrint, "\nGo to the store and buy some more, 99 bottles of beer on the wall.")
</code></pre>

<p>  )</p>

<p>  RETURN result
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Improvements for 2.4]]></title>
    <link href="http://jsteemann.github.io/blog/2014/12/12/aql-improvements-for-24/"/>
    <updated>2014-12-12T23:35:08+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/12/12/aql-improvements-for-24</id>
    <content type="html"><![CDATA[<p>While on a retreat in Belgium, we found some spare time to
work on improvements for AQL. These will be shipped with
ArangoDB version 2.4, and are already available in the devel
version for testing from now on.</p>

<p>Here&rsquo;s a short overview of the improvements:</p>

<!-- more -->


<h1>COLLECT WITH COUNT</h1>

<p>A common use case in query languages is to count the number of
documents returned by a query. The AQL solution for this has been
to use the <code>LENGTH</code> function and a subquery:</p>

<p>```
RETURN LENGTH((
  FOR doc IN collection</p>

<pre><code>FILTER doc.someAttribute == someValue
RETURN doc
</code></pre>

<p>  )
)
```</p>

<p>This works but is probably unintuitive for people which have
used SQL for years.</p>

<p>We therefore now allow using the following alternative syntax,
using the new <code>COLLECT ... WITH COUNT INTO ...</code> clause:</p>

<p><code>
FOR doc IN collection
  FILTER doc.someAttribute == someValue
  COLLECT WITH COUNT INTO length
  RETURN length
</code></p>

<p>This query returns just the total number of matches, but not the
matches themselves. As this query is made for counting only, it
can be executed more efficiently than the original query.
In the query with the <code>COUNT INTO ...</code> clause, the documents found
by the filter condition will only be counted and then instantly
discarded. They will not be shipped around inside the query, from
the subquery to the top level into the <code>LENGTH()</code> function.</p>

<p>This new variant will be drastically faster than the old variant
if there is no filter condition at all. When there is a filter
condition, evaluating the filter condition might be the most computationally
expensive part of the query. But even then, the new variant should
be faster than the old one and use less memory.</p>

<p>As a bonus, there is no need to use a subquery anymore, though the
subquery variant is still fully supported and will be.</p>

<p><code>COLLECT ... WITH COUNT</code> also works for counting the number of items
per group:</p>

<p><code>
FOR doc IN collection
  COLLECT value = doc.someAttribute WITH COUNT INTO length
  RETURN { value: value, length : length }
</code></p>

<p>This returns the number of matches for each distinct <code>value</code>.</p>

<p>A quick unscientific benchmark reveals that the specialized
<code>WITH COUNT</code> clause seems to be faster than the old variant.
The following results show the differences for a collection with
500,000 small documents:</p>

<p>The old variant that counts the number of documents per age runs
in 4.75 seconds on my laptop:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age INTO g
  RETURN { age: age, length: LENGTH(g) }
</code></p>

<p>The new variant produces the same result, but runs in 0.6 seconds locally:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age WITH COUNT INTO length
  RETURN { age: age, length: length }
</code></p>

<p>A notable speedup can also be observed if only a fraction of the
groups is built (here: 1/8). The old variant for this runs in 0.6
seconds:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age INTO g
  RETURN { age: age, length: LENGTH(g) }
</code></p>

<p>The new variant runs in 0.12 seconds:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age WITH COUNT INTO length
  RETURN { age: age, length: length }
</code></p>

<p>The absolute times may vary greatly depending on the type of documents and
the hardware used, but in general the new variant should provide a
speedup.</p>

<h1>COLLECT with group expression</h1>

<p>Finally, <code>COLLECT ... INTO</code> has been extended to support just another variant
that can reduce the amount of copying inside a query.</p>

<p>Let&rsquo;s have a look at this example query:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age INTO g
  RETURN { age: age, maxDate: MAX(g[*].doc.dateRegistered) }
</code></p>

<p>In the above query, for each distinct <code>age</code> value, all documents are collected
into variable <code>g</code>. When the collecting phase is over, there will be an iteration
over all the collected documents again, to extract their <code>dateRegistered</code> value.
After that, the <code>dateRegistered</code> values will be passed into the <code>MAX()</code> function.</p>

<p>This query can be made more efficient now as follows:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age INTO g = doc.dateRegistered
  RETURN { age: age, maxDate: MAX(g) }
</code></p>

<p>The new thing about this variant is the expression following the <code>INTO</code>.
Having an expression there allows controlling what values are collected for
each group. Using a projection expression here can greatly reduce the
amount of copying afterwards, and thus make the query more efficient than if
it had to copy the entire documents.</p>

<h1>Removing filters covered by indexes</h1>

<p><code>FILTER</code> conditions which are completely covered by indexes will
now be removed from the execution plan if it is safe to do so.
Dropping the <code>FILTER</code> statements allows the optimizer to get rid
of not only the <em>FilterNode</em>, but also its corresponding <em>CalculationNode</em>.
This will save a lot of computation if the condition needs to be checked
for many documents.</p>

<p>For example, imagine the following query:</p>

<p><code>
FOR doc IN collection
  FILTER doc.value &lt; 10
  RETURN doc
</code></p>

<p>If there is a (skiplist) index on <code>doc.value</code>, the optimizer may
decide to use this index. It will replace the query&rsquo;s <em>EnumerateCollectionNode</em>
with an <em>IndexRangeNode</em> instead first. The <em>IndexRangeNode</em> will scan the index
on <code>doc.value</code> for the range [-inf, 10).</p>

<p>Following that, the optimizer rule <code>remove-filter-covered-by-index</code>
should fire and detect that the <code>FILTER</code> condition is already covered
by the <em>IndexRangeNode</em> alone. It can thus remove the <em>FilterNode</em>.
This also makes the <em>CalculationNode</em> of the <em>FilterNode</em> obsolete,
so these two nodes will be removed and computation is saved.</p>

<h1>Removing brackets for subquery function call parameters</h1>

<p>Since the beginning of AQL, the parser required the user the put
subqueries that were used as function parameters inside two pairs of
brackets.</p>

<p>For example, the following query did not parse in 2.3 and before:
<code>
RETURN LENGTH(FOR doc IN collection RETURN doc)
</code></p>

<p>Instead, it needed to be written as:
<code>
RETURN LENGTH((FOR doc IN collection RETURN doc))
</code></p>

<p>If you didn&rsquo;t notice the difference, the latter version of the query had
duplicate parentheses. The requirement to use duplicate parentheses has
caused several support questions over time, and this can be taken as a
proof that it was not intuitive.</p>

<p>The requirement for duplicate parentheses was an artifact required by the
AQL parser grammar in order to parse the query correctly.</p>

<p>For 2.4, the AQL grammar has been cleaned up in this respect.
Duplicate parentheses are still allowed and work fine in 2.4 but they are not
required anymore. This should make the first steps with AQL a bit easier
and more intuitive.</p>

<p>We&rsquo;re 1.5 days into our retreat now. Maybe there&rsquo;ll be some more
AQL-related improvements in the end. Let&rsquo;s see.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Tour Around the New AQL Query Optimizer]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer/"/>
    <updated>2014-11-07T22:30:10+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer</id>
    <content type="html"><![CDATA[<p>The major new feature in ArangoDB 2.3 is the shiny new
AQL query optimizer and executor. These parts of ArangoDB have been
rewritten in 2.3 to make AQL much better for our end users.</p>

<!-- more -->


<p>Since one of the initial releases, ArangoDB has been shipped with
AQL, the <em>ArangoDB Query Language</em>. AQL has since then been ArangoDB&rsquo;s
most versatile way of executing simple and also the not-so-simple
queries.</p>

<p>I&rsquo;ll start with an overview of query execution in previous versions
of ArangoDB, and then explain the new engine and explain the differences.</p>

<h2>History: query execution in pre-2.3</h2>

<p>Previous versions of ArangoDB executed any AQL query in the following
steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>collect filter conditions in AST and look for index usage opportunities</li>
<li>generate code</li>
<li>execute code</li>
</ul>


<p>This approach was simple and has worked for a lot of queries, but it also
had a few quirks:</p>

<p>First of all, most of the steps were carried out directly on the
abstract syntax tree, with the AST nodes being modified in place.
There was also just the one AST per query, so the old AQL executor
could not generate multiple, potentially very different execution
plan candidates for a given query.</p>

<p>The &ldquo;old&rdquo; optimizer was able to move AST nodes around during optimization
and it was already able to consider multiple index candidates for a query,
but it would not compare multiple plans and make a cost-based decision.
It was also limited in the amount and scope of transformations it
could safely apply to the AST.</p>

<p>When it came to code generation and execution, the &ldquo;old&rdquo; executor
fully relied on V8 to execute the queries. Result sets were created
using V8&rsquo;s value objects. Documents from collections that queries
needed to iterate over had to be made available to V8. While some
optimization was used for this, the conversions could have summed up
to significant overhead for certain kinds of queries.</p>

<p>The representation of queries via an AST also made it hard to generate
code that supported lazy evaluation during query execution.</p>

<p>Finally, the AQL optimizer so far did not provide much support for
queries that were to be executed in a distributed fashion inside a cluster
of servers.</p>

<h2>Query execution in ArangoDB 2.3</h2>

<p>We wanted to address all these issues with a rewrite of the AQL
infrastructure. Starting with ArangoDB 2.3, AQL queries are executed
in these steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>transform AST into execution plan</li>
<li>optimize and permute execution plans</li>
<li>estimate costs for execution plans and pick optimal plan</li>
<li>instanciate execution engine from optimal plan</li>
<li>(in cluster only) send execution plan parts to cluster nodes</li>
<li>execute query</li>
</ul>


<p>Tokenization and parsing of AQL queries hasn&rsquo;t changed much in 2.3:
query strings are still parsed using a Bison/Flex-based parser and
lexer combo. The AST structure has proven to be good during the parsing
stage, so the parser creates an initial AST from the query string first.</p>

<p>After that, simple optimizations are performed directly on the AST,
such as constant folding and constant propagation. Deterministic functions
with constant operands will be executed already in this stage and the
results be injected into the AST.</p>

<p>A major change in 2.3 is that no further transformations will be
carried out on the AST. Instead, the AST will be transformed into
an initial <em>execution plan</em>.</p>

<p>This execution plan is the starting point for the <em>query optimizer</em>.
It will take the initial execution plan and apply transformations to
it. Transformations will either update the existing plan in place or
create a new, modified plan. The result of the transformations carried
out will form the input for further transformations that can be carried
out by query optimizer.</p>

<p>The result of the query optimization stage is one or many execution
plans. For each plan, the optimizer will estimate a cost value, and
then finally pick the plan with the lowest total estimated cost.
This plan is considered to be the <em>optimal plan</em>. All other execution
plans will be discarded by the optimizer as it has considered them non-optimal.</p>

<p>The optimal execution plan is then executed by the <em>execution engine</em>.
For a single-server AQL query, this is straightforward: for each step
in the execution plan, a C++ object is created that is supposed to
execute the particular step. Query execution is then started by asking
the first of these objects for its results.</p>

<p>The objects for multiple processing steps are linked in a pipelined fashion
with lazy evaluation. Pulling data from the first object will eventually
trigger pulling data from the second object etc., until there are no more
results to produce.</p>

<p>For a distributed query, this is a bit more complicated. The different
execution steps will likely be shipped to different servers in the
cluster, and the objects need to be instanciated in different servers, too.
The different parts of the query may pull data from each other via HTTP
calls between cluster nodes.</p>

<h2>How execution plans work</h2>

<p>An execution plan is a sequence of query execution steps. Let&rsquo;s
start with a very simple example:</p>

<p><code>
FOR doc IN mycollection
  RETURN doc._key
</code></p>

<p>This query will be transformed into the following execution plan:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: iterates over all documents of a collection
and provides the current document in an output variable. In our example,
it will iterate over collection <code>mycollection</code> and provide each
document in variable <code>doc</code></li>
<li><em>CalculationNode</em>: evaluates an expression and returns its result.
In the example, it will calculate <code>doc._key</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>If this plan is going to be executed, the execution engine will start
pulling data from the node at the bottom, that is, the <em>ReturnNode</em>. The
<em>ReturnNode</em> at this stage cannot provide any data, so it will ask its
predecessor node, which in the example is the <em>CalculationNode</em>. The
<em>CalculationNode</em> again does not have own data yet, so it must ask the
node in front of it. The <em>EnumerateCollectionNode</em> will first ask the
<em>SingletonNode</em> for input data. So the execution flow has bubbled up from
the bottom of the sequence to the top.</p>

<p>The <em>SingletonNode</em> will now produce a single empty return value. It will
also internally set its processing status to <em>done</em>, so it will not produce
any more values if asked again. This is all a <em>SingletonNode</em> will ever do.
We&rsquo;ll see later why such a node may still be useful.</p>

<p>The single empty value will be provided as input to the <em>EnumerateCollectionNode</em>.
This node will now go through all the documents in the underlying collection,
and return them once for each input value its got. As its input value was
the singleton, it will return the documents of the collection just once.</p>

<p>Processing is executed in blocks of size 1000 by default. The
<em>EnumerateCollectionNode</em> will thus not return all documents to its successor
node, but just 1,000. The return value will be a vector with 1,000 documents,
stored under variable name <code>doc</code>.</p>

<p>The <em>CalculationNode</em>, still waiting for input data, can now execute its
expression <code>doc._key</code> on this input value. It will execute this expression
1,000 times, once for each input value. The expression results will be
stored in another variable. This variable is anonymous, as it hasn&rsquo;t been
named explicitly in the original query. The vector of results produced by
the <em>CalculationNode</em> is then returned to the <em>ReturnNode</em>, which will then
return it to the caller.</p>

<p>If the caller requests more documents, the procedure will repeat. Whenever
a processing step cannot produce any more data, it will ask its predecessor
step for more data. If the predecessor step already has status <em>done</em>, the
current step will set itself to <em>done</em> as well, so a query will actually
come to an end if there are no more results.</p>

<p>As can be seen, steps are executed with batches of values. We thought this
would be a good way to improve efficiency and reduce the number of hops
between steps.</p>

<h2>Joins</h2>

<p>Let&rsquo;s say we want to join documents from two collections, based on common
attribute values. Let&rsquo;s use <code>users</code> and <code>logins</code>, joined by their <code>id</code> and
<code>userId</code> attributes:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Provided that there are no indexes, the query may be turned into this
execution plan by the optimizer:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>logins</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>Now we can see why the <em>SingletonNode</em> is useful: it can be used as an
input to another node, telling this node to execute just once. Having the
<em>SingletonNode</em> will ensure that the outermost <em>EnumerateCollection</em>
will only iterate once over the documents in its underlying collection <code>users</code>.</p>

<p>The inner <em>EnumerateCollectionNode</em> for collection <code>logins</code> is now fed by
the outer <em>EnumerateCollectionNode</em> on <code>users</code>. Thus these two nodes will
produce a cartesian product. This will be done lazily, as producing results
will normally happen in chunks of 1,000 values each.</p>

<p>The results of the cartesian product are then post-filtered by the <code>FilterNode</code>,
which will only let those documents pass that match the filter condition of
the query. The <code>FilterNode</code> employs its predecessor, the <code>CalculationNode</code>,
to determine which values satisfy the condition.</p>

<h2>Using indexes</h2>

<p>Obviously creating cartesian products is not ideal. The optimizer will try
to avoid generating such plans if it can, but it has no choice if there are
no indexes present.</p>

<p>If there are indexes on attributes that are used in <code>FILTER</code> conditions of
a query, the optimizer will try to turn <code>EnumerateCollectionNode</code>s into
<code>IndexRangeNode</code>s. The purpose of an <code>IndexRangeNode</code> is to iterate over a
specific range in an index. This is normally more efficient than iterating
over all documents of a collection.</p>

<p>Let&rsquo;s assume there is an index on <code>logins.userId</code>. Then the optimizer might
be able to generate a plan like this:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>IndexRangeNode</em>: will iterate over the values in index <code>logins.userId</code> that
match the value of <code>users.id</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>To run this query, the execution engine must still iterate over all documents
in collection <code>users</code>, but for each of those, it only needs to find the documents
in <code>logins</code> that match the join condition. This most likely means a lot less
lookups and thus much faster execution.</p>

<h2>Permutation of loops</h2>

<p>Now consider adding an extra <code>FILTER</code> statement to the original query so we
end up with this:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
FILTER login.ts == 1415402319       /* added this one! */
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>The optimizer is free to permute the order of <code>FOR</code> loops as long as this
won&rsquo;t change the results of a query. In our case, permutation of the two
<code>FOR</code> loops is allowed (the query does not contain a <code>SORT</code> instruction so
the order of results is not guaranteed).</p>

<p>If the optimizer exchanges the two loops, it can also pull out the <code>FILTER</code>
statement on <code>login.ts</code> out of the inner loop, and move up into the outer loop.
It might come up with a plan like this, which may be more efficient if a
lot of documents from <code>logins</code> can be filtered out early:</p>

<p>```
FOR login IN logins
  FILTER login.ts == 1415402319
  FOR user IN users</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Exchanging the order of <code>FOR</code> loops may also allow the optimizer to use
additional indexes.</p>

<p>A last note on indexes: the optimizer in 2.3 is able to use (sorted)
skiplist indexes to eliminate extra <code>SORT</code> operations. For example, if
there is a skiplist index on <code>login.ts</code>, the <code>SORT</code> in the following
query can be removed by the optimizer:</p>

<p><code>
FOR login IN logins
  FILTER login.ts &gt; 1415402319
  SORT login.ts
  RETURN login
</code></p>

<p>The AQL optimizer in 2.3 can optimize away a <code>SORT</code> even if the sort
order is backwards or if no <code>FILTER</code> statement is used in the query at
all.</p>

<h2>Analyzing plans</h2>

<p>One particular improvement over 2.2 is that in ArangoDB 2.3 the optimizer
provides functionality for retrieving full execution plan information for
queries <strong>without</strong> executing them. The execution plan information can be
inspected by developers or DBAs, and, as it is JSON-encoded, can also be
analyzed programmatically.</p>

<p>Retrieving the execution plan for a query is straight-forward:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain();
</code></p>

<p>By default, the optimizer will return just the <em>optimal plan</em>, containing
all the plan&rsquo;s execution nodes with lots of extra information plus cost estimates.</p>

<p>The optimizer is also able to return the alternative plans it produced but
considered to be non-optimal:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain({ allPlans: true });
</code></p>

<p>This will hopefully allow developers and DBAs to get a better idea of how an
AQL query will be executed internally.</p>

<p>Additionally, simple execution statistics are returned by default when executing
a query. This statistics can also be used to get an idea of the runtime costs of
a query <strong>after</strong> execution.</p>

<h2>Writing optimizer rules</h2>

<p>The AQL optimizer itself is dumb. It will simply try to apply all transformations
from its rulebook to each input execution plan it is feeded with. This
will produce output execution plans, on which further transformations
may or may not be applied.</p>

<p>The more interesting part of the AQL optimizer stage is thus the rulebook.
Each rule in the rulebook is a C++ function that is executed for an input plan.</p>

<p>Adding a new optimizer rule to the rulebook is intentionally simple. One of
the design goals of the new AQL optimizer was to keep it flexible and extensible.
All that&rsquo;s need to be to add an optimizer rule is to implement a C++ function
with the following signature:</p>

<p><code>cpp
(Optimizer*, ExecutionPlan*, Optimizer::Rule const*) -&gt; int
</code></p>

<p>and register it once in the Optimizer&rsquo;s rulebook.</p>

<p>An optimizer rule function is called with an instance of the query optimizer
(it can use it to register a new plan), the current execution plan and some
information about the rule itself (this is the information about the rule from
the rulebook).</p>

<p>The optimizer rule function can then analyze the input execution plan, modifiy
it in place, and/or create additional plans. It must return a status code to
the optimizer to indicate if something went wrong.</p>

<h2>Outlook</h2>

<p>The AQL optimizer features described here are available in ArangoDB 2.3, which
is currently in <a href="https://www.arangodb.com/install-beta-version">beta stage</a>.</p>

<p>Writing a perfect query optimizer is a never-ending endeavour. Other databases
provide new optimizer features and fixes even decades after the initial version.</p>

<p>Our plan is to ship 2.3 with several essential and useful optimizer rules. We
will likely add more in future releases. We&rsquo;re also open to contributions.
If you can think of rules that are missing but you would like to see in ArangoDB,
please let us know. If you would like to contribute to the optimizer and write some
rule code, consider sending a pull request or an email to
<a href="hackers@arangodb.org">hackers@arangodb.org</a>.</p>
]]></content>
  </entry>
  
</feed>
