<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Concepts | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/concepts/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2016-06-16T19:39:17+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Improved Deadlock Detection]]></title>
    <link href="http://jsteemann.github.io/blog/2015/11/18/improved-deadlock-detection/"/>
    <updated>2015-11-18T19:24:24+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/11/18/improved-deadlock-detection</id>
    <content type="html"><![CDATA[<p>The upcoming ArangoDB version 2.8 (currently in devel) will
provide a much better deadlock detection mechanism than its
predecessors.</p>

<p>The new deadlock detection mechanism will kick in automatically
when it detects operations that are mutually waiting for each other.
In case it finds such deadlock, it will abort one of the operations
so that the others can continue and overall progress can be made.</p>

<!-- more -->


<p>In previous versions of ArangoDB, deadlocks could make operations
wait forever, requiring the server to be stopped and restarted.</p>

<h2>How deadlocks can occur</h2>

<p>Here&rsquo;s a simple example for getting into a deadlock state:</p>

<p>Transaction A wants to write to collection c1 and to read from collection
c2. In parallel, transaction B wants to write to collection c2 and read
from collection c1. If the sequence of operations is interleaved as follows,
then the two transactions prevent each other from making progress:</p>

<ul>
<li>transaction A successfully acquires write-lock on c1</li>
<li>transaction B sucessfull acquires write-lock on c2</li>
<li>transaction A tries to acquire read-lock on c2 (and must wait for B)</li>
<li>transaction B tries to acquire read-lock on c1 (and must wait for A)</li>
</ul>


<p>Here&rsquo;s these such two transactions being started from two ArangoShell
instances in parallel (left is A, right is B):</p>

<p><img src="/downloads/screenshots/deadlock.png"></p>

<p>(note that this screenshot is from 2.8 and the automatic deadlock detection
had already detected the deadlock and aborted one of the transactions)</p>

<p>In general, deadlocks can occur only when multiple operations (AQL
queries or other transactions) try to access the same resources
(collections) at the same time, and only if the operations already
have already acquired some locks on these resources. And finally
each operation needs to involve more than one collection, so there
is the potential for already having acquired some locks but having
to wait for others.</p>

<h2>Dynamically added collections</h2>

<p>Most operations will just work fine and will not cause any deadlocks.
This is especially true for all operations that involve only a single
collection. This leaves multi-collection AQL queries and multi-collection
userland transactions.</p>

<p>Normally these will also work fine. This is because when a query or
transaction starts, it will tell the transaction manager about the resources
(collections) it will need. The transaction manager can then acquire the
required resources in a deterministic fashion that prevents deadlocks.
If all queries and transactions properly announce upfront which collections
they will access, there will also be no deadlocks.</p>

<p>But for some operations its hard to predict at transaction start which
collections will be accessed. This includes some AQL functions that
can dynamically access collection data without having to specify the
collection name anywhere in the query.</p>

<p>A good example for this is the <code>GRAPH_EDGES</code> AQL function, which will get
a graph name as its first input parameter, but not the names of the underlying
edge collection(s). When this function is used in an AQL query, the
query parser will just find a function parameter containing a graph name
but doesn&rsquo;t know it&rsquo;s a collection name.</p>

<p><code>
GRAPH_EDGES("myEdges", [ { type: "friend" } ])
</code></p>

<p>The <code>"myEdges"</code> graph name will look like any other string to the parser.
It does not know about the contexts in which strings may have special meanings.</p>

<p>Note that even if this would be fixed, the problem won&rsquo;t go away entirely:
a function call parameter in a query isn&rsquo;t necessarily a constant but can
be an arbitrary expression:</p>

<p><code>
FOR doc IN collection
  RETURN GRAPH_EDGES(CONCAT(doc.graphName, '-test'), [ doc.example ])
</code></p>

<p>At least in this case the AQL query parser won&rsquo;t find a collection name,
so when the AQL query starts it is yet unknown which collections will be
accessed. Only at runtime when the function is actually executed, the
collection names will be looked up by finding the graph description in the
<code>_graphs</code>system collection. Then the edge collections participating in
the graph will be added to the query dynamically. Only this dynamic addition
adds the potential for deadlock.</p>

<p>This dynamic addition of collections in unavoidable for conveniently
querying data from collections whose names are unknown when the query starts.</p>

<h2>Deadlock detection</h2>

<p>Whenever transaction manager detects a deadlock in ArangoDB 2.8, it will
automatically abort one of the blocking transactions. The transaction will
be rolled back and all modifications it has made will be reverted. The
operation will fail with error code 29 (<em>deadlock detected</em>) and raise an
exception that the user can handle in the calling code.</p>

<p>Deadlocks will be found if two transactions mutually lock each other as
seen in the screenshot above, but also for more complex setups. The following
screenshot shows four parallel transactions that block each other indirectly.</p>

<p><img src="/downloads/screenshots/threeway-deadlock.png"></p>

<p>The top left window (transaction 1) will block the one in the top right
(transaction 2), and is itself blocked by the transaction in the bottom left
(transaction 3).</p>

<p>The transaction in the top right window (transaction 2) blocks the one in the
bottom left (transaction 3), and is itself blocked by the one in the top left
(transaction 1).</p>

<p>Transaction 3 (bottom left) is blocked by transaction 2 (top right).
Transaction 4 (bottom right) does exactly the same as transaction 3.</p>

<p>With these transactions, we end up in this waiting state:</p>

<ul>
<li>T1 waits for T3 and T4</li>
<li>T2 waits for T1</li>
<li>T3 waits for T2</li>
<li>T4 waits for T2</li>
</ul>


<p>This waiting state is cyclic (T1 &lt; T3 &lt; T2 &lt; T1) and therefore no progress
can be made. This is exactly a situation in which the transaction manager
will abort one of the transactions.</p>

<p>No configuration is required for the deadlock detection mechanism. It will
always be active and cannot be configured or turned off.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How V8 Is Used in ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2015/08/01/how-v8-is-used-in-arangodb/"/>
    <updated>2015-08-01T19:06:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/08/01/how-v8-is-used-in-arangodb</id>
    <content type="html"><![CDATA[<p>ArangoDB allows running user-defined JavaScript code in the database.
This can be used for more complex, <em>stored procedures</em>-like database operations.
Additionally, ArangoDB&rsquo;s <a href="https://www.arangodb.com/foxx/">Foxx framework</a> can
be used to make any database functionality available via an HTTP REST API.
It&rsquo;s easy to build microservices with it, using the scripting functionality
for tasks like access control, data validation, sanitation etc.</p>

<p>We often get asked how the scripting functionality is implemented under the hood.
Additionally, several people have asked how ArangoDB&rsquo;s JavaScript functionality
relates to node.js.</p>

<p>This post tries to explain that in detail.</p>

<!-- more -->


<h1>The C++ parts</h1>

<p><em>arangosh</em>, the ArangoShell, and <em>arangod</em>, the database server, are written in
C++ and they are shipped as native code executables. Some parts of both <em>arangosh</em>
and <em>arangod</em> itself are written in JavaScript (more on that later).</p>

<p>The I/O handling in <em>arangod</em> is written in C++ and uses libev (written in C)
for the low-level event handling. All the socket I/O, working scheduling and
queueing is written in C++, too. These are parts that require high parallelism,
so we want this to run in multiple threads.</p>

<p>All the indexes, the persistence layer and many of the fundamental operations,
like the ones for document inserts, updates, deletes, imports are written in C++ for
effective control of memory usage and parallelism. AQL&rsquo;s query parser is written
using the usual combination of Flex and Bison, which generate C files that are
compiled to native code. The AQL optimizer, AQL executor and many AQL functions are
writting in C++ as well.</p>

<p>Some AQL functions however, are written in JavaScript. And if an AQL query
invokes a user-defined function, this function will be a JavaScript function, too.</p>

<h1>How ArangoDB uses V8</h1>

<p>How is JavaScript code executed in ArangoDB?</p>

<p>Both <em>arangosh</em> and <em>arangod</em> are linked against the <a href="https://developers.google.com/v8/">V8 JavaScript engine</a>
library. V8 (itself written in C++) is the component that runs the JavaScript
code in ArangoDB.</p>

<p>V8 requires JavaScript code to run in a so-called <em>isolate</em> (note: I&rsquo;ll be
oversimplifying a bit here &ndash; in reality there are isolates and contexts).
As the name suggests, isolates are completely isolated from each other.
Especially, data cannot be shared or moved across isolates, and each isolate
can be used by only one thread at a time.</p>

<p>Let&rsquo;s look at how <em>arangosh</em>, the ArangoShell, uses V8. All JavaScript commands
entered in <em>arangosh</em> will be compiled and executing with V8 immediately.
In <em>arangosh</em>, this happens using a single V8 isolate.</p>

<p>On the server side, things are a bit different. In <em>arangod</em>, there are multiple
V8 isolates. The number of isolates to create is a startup configuration
option (<code>--javascript.v8-contexts</code>). Creating multiple isolates allows running
JavaScript code in multiple threads, truly parallel. Apart from that, <em>arangod</em>
has multiple I/O threads (<code>--scheduler.threads</code> configuration option) for handling
the communication with client applications.</p>

<p>As mentioned earlier, part of ArangoDB&rsquo;s codebase itself is written in JavaScript,
and this JavaScript code is executed the same way as any user-defined will be executed.</p>

<h1>Executing JavaScript code with V8</h1>

<p>For executing any JavaScript code (built-in or user-defined), ArangoDB will invoke
V8&rsquo;s JIT compiler to compile the script code into native code and run it.</p>

<p>The JIT compiler in V8 will not try extremely hard to optimize the code on the
first invocation. On initial compilation, it will aim for a good balance of
optimizations and fast compilation time. If it finds some code parts are called
often, it may re-try to optimize these parts more aggressively automatically.
To make things even more complex, there are different JIT compilers in V8
(i.e. Crankshaft and Turbofan) with different sweet spots. JavaScript modes
(i.e. <em>strict mode</em> and <em>strong mode</em>) can also affect the level of optimizations
the compilers will carry out.</p>

<p>Now, after the JavaScript code has been compiled to native code, V8 will run it
until it returns or fails with an uncaught exception.</p>

<p>But how can the JavaScript code access the database data and server internals?
In other words, what actually happens if a JavaScript command such as the following
is executed?</p>

<p><code>js example JavaScript command
db.myCollection.save({ _key: "test" });
</code></p>

<h2>Accessing server internals from JavaScript</h2>

<p>Inside <em>arangod</em>, each V8 isolate is equipped with a global variable named <code>db</code>.
This JavaScript variable is a wrapper around database functionality written in C++.
When the <code>db</code> object is created, we tell V8 that its methods are C++ callbacks.</p>

<p>Whenever the <code>db</code> object is accessed in JavaScript, the V8 engine will therefore
call C++ methods. These provide full access to the server internals, can do whatever
is required and return data in the format that V8 requires. V8 then makes the
return data accessible to the JavaScript code.</p>

<p>Executing <code>db.myCollection.save(...)</code> is effectively two operations: accessing the
property <code>myCollection</code> on the object <code>db</code> and then calling function <code>save</code> on that
property. For the first operation, V8 will invoke the object&rsquo;s <code>NamedPropertyHandler</code>,
which is a C++ function that is responsible for returning the value for the property
with the given name (<code>myCollection</code>). In the case of <code>db</code>, we have a C++ function
that collection object if it exists, or <code>undefined</code> if not.</p>

<p>The collection object again has C++ bindings in the background, so calling function
<code>save</code> on it will call another C++ function. The collection object also has a (hidden)
pointer to the C++ collection. When <code>save</code> is called, we will extract that pointer
from the <code>this</code> object so we know which C++ data structures to work on. The <code>save</code>
function will also get the to-be-inserted document data as its payload. V8 will
pass this to the C++ function as well so we can validate it and convert it into
our internal data format.</p>

<p>On the server side, there are several objects exposed to JavaScript that have C++
bindings. There are also non-object functions that have C++ bindings. Some of these
functions are also bolted on regular JavaScript objects.</p>

<h2>Accessing server internals from ArangoShell</h2>

<p>When running the same command in <em>arangosh</em>, things will be completely different.
The ArangoShell may run on the same host as the <em>arangod</em> server process, but it may
also run on a completely different one. Providing <em>arangosh</em> access to server internals
such as pointers will therefore not work in general. Even if <em>arangosh</em> and <em>arangod</em>
do run on the same host, they are independent processes with no access to the each
other&rsquo;s data. The latter problem could be solved by having a shared memory segment
that both <em>arangosh</em> and <em>arangod</em> can use, but why bother with that special case
which will provide no help in the general case when the shell can be located on
<strong>any</strong> host.</p>

<p>To make the shell work in all these situations, it uses the HTTP REST API provided
by the ArangoDB server to talk to it. For <em>arangod</em>, any ArangoShell client is just
another client, with no special treatments or protocols.</p>

<p>As a consequence, all operations on databases and collections run from the ArangoShell
are JavaScript wrappers that call their respective server-side HTTP APIs.</p>

<p>Recalling the command example again (<code>db.myCollection.save(...)</code>), the shell will first
access the property <code>myCollection</code> of the object <code>db</code>. In the shell <code>db</code> is a regular
JavaScript object with no C++ bindings. When the shell is started, it will make an
HTTP call to <em>arangod</em> to retrieve a list of all available collections, and register
them as properties in its <code>db</code> object. Calling the <code>save</code> method on one of these
objects will trigger an HTTP POST request to the server API at <code>/_api/document?collection=myCollection</code>,
with the to-be-inserted data in its request body. Eventually the server will respond
and the command will return with the data retrieved from the server.</p>

<h2>Considerations</h2>

<p>Consider running the following JavaScript code:</p>

<p><code>js code to insert 1000 documents
for (var i = 0; i &lt; 1000; ++i) {
  db.myCollection.save({ _key: "test" + i });
}
</code></p>

<p>When run from inside the ArangoShell, the code will be executed in there. The shell will
perform an HTTP request to <em>arangod</em> for each call to <code>save</code>. We&rsquo;ll end up with 1,000
HTTP requests.</p>

<p>Running the same code inside <em>arangod</em> will trigger no HTTP requests, as the server-side
functions are backed with C++ internals and can access the database data directly. It will
be a lot faster to run this loop on the server than in <em>arangosh</em>. A while ago I wrote
<a href="/blog/2014/08/30/understanding-where-operations-are-executed/">another article</a> about this.</p>

<p>When replacing the ArangoShell with another client application, things are no different.
A client application will not have access to the server internals, so all it can do is to
make requests to the server (by the way, the principle would be no different if we used
MySQL or other database servers, only the protocols would vary).</p>

<p>Fortunately, there is a fix for this: making the code run server-side. For example, the
above code can be put into a Foxx route. This way it is not only fast but will be made
accessible via an HTTP REST API so client applications can call it with a single HTTP request.</p>

<p>In reality, database operations will be more complex than in the above example. And this
is where having a full-featured scripting language like JavaScript helps. It provides all
the features that are needed for more complex tasks such as validating and sanitizing input
data, access control, executing database queries and postprocessing results.</p>

<h1>The differences to node.js</h1>

<p>To start with: ArangoDB is not node.js, and vice versa. ArangoDB is not a node.js module
either. ArangoDB and node.js are completely indepedent.</p>

<p>But there is a commonality: both ArangoDB and node.js use the V8 engine for running
JavaScript code.</p>

<h2>Threading</h2>

<p>AFAIK, standard node.js only has a single V8 isolate to run all code in.
While that made the implementation easier (no hassle with multi-threading) it
also limits node.js to using only a single CPU.</p>

<p>It&rsquo;s not unusual to see a multi-core server with a node.js instance maxing out
one CPU while the other CPUs are sitting idle. In order to max out a multi-core
server, people often start multiple node.js instances on a single server. That will
work fine, but the node.js instances will be independent, and sharing data between
them is not possible in plain JavaScript.</p>

<p>And because a node.js instance is single-threaded, it is also important that
code written for node.js is non-blocking. Code that blocks while waiting for
some I/O operation would block the only available CPU. Using non-blocking
I/O operations allows node.js to queue the operation, and execute other code
in the meantime, allowing overall progress. This also makes it look like it
would be executing multiple actions in parallel, while it is actually executing
them sequentially.</p>

<p>Contrary, <em>arangod</em> is a multi-threaded server. It can serve multiple requests in
parallel, using multiple CPUs. Because <em>arangod</em> has multiple V8 isolates that
each can execute JavaScript code, it can run JavaScript in multiple threads in parallel.</p>

<p><em>arangosh</em>, the ArangoShell, is single-threaded and provides only a single V8 isolate.</p>

<h2>Usage of modules</h2>

<p>Both node.js and ArangoDB can load code at runtime so it can be organized into
modules or libraries. In both, extra JavaScript modules can be loaded using the
<code>require</code> function.</p>

<p>There is often confusion about whether node.js modules can be used in ArangoDB.
This is probably because the answer is &ldquo;<em>it depends!</em>&rdquo;.</p>

<p>node.js packages can be written in JavaScript but they can also compile to native
code using C++. The latter can be used to extend the functionality of node.js with
features that JavaScript alone wouldn&rsquo;t be capable of. Such modules however often
heavily depend on a specific V8 version (so do not necessarily compile in a node.js
version with a different version of V8) and often rely on node.js internals.</p>

<p>ArangoDB can load modules that are written in pure JavaScript. Modules that
depend on non-JavaScript functionality (such as native modules for node.js) or modules
that rely on node.js internals cannot be loaded in ArangoDB. As a rule of thumb,
any module will run in ArangoDB that is implemented in pure JavaScript, does not
access global variables and only requires other modules that obey the same restrictions.</p>

<p>ArangoDB also uses several externally maintained JavaScript-only libraries, such as
underscore.js. This module will run everywhere because it conforms to the mentioned
restrictions.</p>

<p>ArangoDB also uses several other modules that are maintained on npm.js.
An example module is <a href="https://www.npmjs.com/package/aqb">AQB</a>, a query builder for AQL.
It is written in pure JavaScript too, so it can be used from a node.js application and
from within ArangoDB. If there is an updated version of this module, we use npm to
install it in a subdirectory of ArangoDB. As per npm convention, the node.js modules
shipped with ArangoDB reside in a directory named <code>node_modules</code>. Probably this is
what caused some of the confusion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[COLLECTing With a Hash Table]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/22/collecting-with-a-hash-table/"/>
    <updated>2015-04-22T13:53:10+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/22/collecting-with-a-hash-table</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.6 will feature an alternative <strong>hash</strong> implementation of the AQL <code>COLLECT</code>
operation. The new implementation can speed up some AQL queries that can not exploit indexes
on the <code>COLLECT</code> group criteria.</p>

<p>This blog post provides a preview of the feature and shows some nice performance improvements.
It also explains the <code>COLLECT</code>-related optimizer parts and how the optimizer will decide whether
to use the new or the traditional implementation.</p>

<!-- more -->


<h2>Introduction to COLLECT</h2>

<p>A quick recap: in AQL, the <code>COLLECT</code> operation can be used for grouping and optionally counting values.</p>

<p>Here&rsquo;s an example, using flight data:</p>

<p><code>plain AQL COLLECT example
FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count
  RETURN { from: from, count: count }
</code></p>

<p>This query will iterate over all documents in collection <code>flights</code>, and count the
number of flights per different <code>_from</code> value (origin airport). The query result will
contain only unique <code>from</code> values plus a counter for each:</p>

<p><code>json query result, grouped by from
[
  { "from" : "airports/ABE", "count" : 6205 },
  { "from" : "airports/ABQ", "count" : 39346 },
  { "from" : "airports/ACV", "count" : 362 },
  ...
  { "from" : "airports/YAP", "count" : 285 },
  { "from" : "airports/YKM", "count" : 879 },
  { "from" : "airports/YUM", "count" : 2275 }
]
</code></p>

<p>As the <code>COLLECT</code> will group its result according to the specified group criteria (<code>flights._from</code>
in the above query), it needs a way of figuring out to which group any input value does belong.</p>

<p>Before ArangoDB 2.6, there was a single method for determining the group. Starting with ArangoDB
2.6, the query optimizer can choose between two different <code>COLLECT</code> methods, the <strong>sorted</strong> method
and the <strong>hash</strong> method.</p>

<h2>Sorted COLLECT method</h2>

<p>The traditional method for determining the group values is the <strong>sorted</strong> method. It has been
available in ArangoDB since the very start.</p>

<p>The sorted method of <code>COLLECT</code> requires its input to be sorted by the group criteria specified
in the <code>COLLECT</code> statement. Because there is no guarantee that the input data are already sorted
in the same way, the query optimizer will automatically insert a <code>SORT</code> statement into the query
in front of the <code>COLLECT</code>. In case there is a sorted index present on the group criteria attributes,
the optimizer may be able to optimize away the <code>SORT</code> again. If there is no sorted index present
on the group criteria attributes, the <code>SORT</code> will remain in the execution plan.</p>

<p>Here is the execution plan for the above query using the <strong>sorted</strong> method of <code>COLLECT</code>. We can see
the extra <code>SortNode</code> with id #7 being added by the optimizer in front of the <code>COLLECT</code>:</p>

<p><img src="/downloads/screenshots/collect-sorted.png"></p>

<p>The <strong>sorted</strong> method of <code>COLLECT</code> is efficient because it can write out a group result whenever
an input value will start a new group. Therefore it does not need to keep the whole <code>COLLECT</code>
result in memory. The downside of using the sorted method is that it requires its input to be
sorted, and that this requires adding an extra <code>SORT</code> for not properly sorted input.</p>

<h2>Hash COLLECT method</h2>

<p>Since ArangoDB 2.6, the query optimizer can also employ the <strong>hash</strong> method for <code>COLLECT</code>. The
hash method works by assigning the input values of the <code>COLLECT</code> to slots in a hash table. It
does not require its input to be sorted. Because the entries in the hash table do not have a
particular order, the query optimizer will add a post-<code>COLLECT</code> <code>SORT</code> statement. With this extra
sort of the <code>COLLECT</code> result, the optimizer ensures that the output of the sorted <code>COLLECT</code> will
be the same as the output of the hash <code>COLLECT</code>.</p>

<p>Here is the execution plan for the above query when using the <strong>hash</strong> method of <code>COLLECT</code>.
Here we can see the extra <code>SortNode</code> with id #7 being added post-<code>COLLECT</code>:</p>

<p><img src="/downloads/screenshots/collect-hash.png"></p>

<p>The <strong>hash</strong> method is beneficial because it does not require sorted input and thus no extra
<code>SORT</code> step in front. However, as the input is not sorted, it is never clear when a group is
actually finished. The hash method therefore needs to build the whole <code>COLLECT</code> result in memory
until the input is exhausted. Then it can safely write out all group results. Additionally,
the result of the hash <code>COLLECT</code> is unsorted. Therefore the optimizer will add a post-<code>COLLECT</code>
sort to ensure the result will be identical to a <strong>sorted</strong> <code>COLLECT</code>.</p>

<h2>Which method will be used when?</h2>

<p>The query optimizer will always take the initial query plan and specialize its <code>COLLECT</code> nodes to
using the <strong>sorted</strong> method. It will also add the pre-<code>COLLECT</code> <code>SORT</code> in the original plan.</p>

<p>In addition, for every <code>COLLECT</code> statement not using an <code>INTO</code> clause, the optimizer will create
a plan variant that uses the <strong>hash</strong> method. In that plan variant, the post-<code>COLLECT</code> <code>SORT</code>
will be added. Note that a <code>WITH COUNT INTO</code> is still ok here, but that using a regular <code>INTO</code>
clause will disable the usage of the <strong>hash</strong> method:</p>

<p><code>plain a query that cannot use the hash method
FOR flight IN flights
  COLLECT from = flight._from INTO allFlights
  RETURN { from: from, flights: allFlights }
</code></p>

<p>If more than one <code>COLLECT</code> method can be used for a query, the created plans will be shipped through
the regular optimization pipeline. In the end, the optimizer will pick the plan with the lowest
estimated total cost as it will do for all other queries.</p>

<p>The <strong>hash</strong> variant does not require an up-front sort of the <code>COLLECT</code> input, and will thus be
preferred over the <strong>sorted</strong> method if the optimizer estimates many input elements for the <code>COLLECT</code>
and cannot use an index to process them in already sorted order. In this case, the optimizer
will estimate that post-sorting the result of the <strong>hash</strong> <code>COLLECT</code> will be more efficient than
pre-sorting the input for the <strong>sorted</strong> <code>COLLECT</code>.</p>

<p>The main assumption behind this estimation is that the result of any <code>COLLECT</code> statement will
contain at most as many elements as there are input elements to it. Therefore, the output of
a <code>COLLECT</code> is likely to be smaller (in terms of rows) than its input, making post-sorting more
efficient than pre-sorting.</p>

<p>If there is a sorted index on the <code>COLLECT</code> group criteria that the optimizer can exploit, the
optimizer will pick the <strong>sorted</strong> method because thanks to the index it can optimize away the
pre-<code>COLLECT</code> sort, leaving no sorts left in the final execution plan.</p>

<p>To override the optimizer decision, <code>COLLECT</code> statements now have an <code>OPTIONS</code> modifier. This
modifier can be used to force the optimizer to use the <strong>sorted</strong> variant:</p>

<p><code>plain forcing the use of the sorted variant
FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count OPTIONS { method: "sorted" }
  RETURN { from: from, count: count }
</code></p>

<p>Note that specifying <strong>hash</strong> in <code>method</code> will not force the optimizer to use the <strong>hash</strong> method.
The reason is that the <strong>hash</strong> variant cannot be used for all queries (only <code>COLLECT</code> statements
without an <code>INTO</code> clause are eligible). If <code>OPTIONS</code> are omitted or any other method than <code>sorted</code>
is specified, the optimizer will ignore it and use its regular cost estimations.</p>

<h2>Understanding execution plans</h2>

<p>Which method is actually used in a query can found out by explaining it and looking at its
execution plan.</p>

<p>A <code>COLLECT</code> is internally handled by an object called <code>AggregateNode</code>, so we have to look for that.
In the above screenshots, the <code>AggregateNode</code>s are tagged with either <strong>hash</strong> or <strong>sorted</strong>. This can
also be checked programatically by looking at the <code>aggregationOptions.method</code> attributes in the
JSON result of an explain().</p>

<p>Here is some example code to extract this information, limited to the <code>AggregateNode</code>s of the
query already:</p>

<p><code>js extracting just the AggregateNodes from an explain
var query = `
  FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count
  RETURN { from: from, count: count }
`;
var stmt = db._createStatement(query);
var plan = stmt.explain().plan;
plan.nodes.filter(function(node) {
  return node.type === 'AggregateNode';
});
</code></p>

<p>For the above query, this will produce something like this:</p>

<p>```json JSON explain result for AggregateNode
[
  {</p>

<pre><code>"type" : "AggregateNode", 
...
"aggregationOptions" : { 
  "method" : "hash" 
}  
</code></pre>

<p>  }
]
```</p>

<p>Here we can see that the query is using the <strong>hash</strong> method.</p>

<h2>Optimizing away post-COLLECT sorts</h2>

<p>If a query uses the <strong>hash</strong> method for a <code>COLLECT</code> but the sort order of the <code>COLLECT</code> result
is irrelevant to the user, the user can provide a hint to the optimizer to remove the
post-<code>COLLECT</code> sort.</p>

<p>This can be achieved by simply appending a <code>SORT null</code> to the original <code>COLLECT</code> statement.
Here we can see that this removes the post-<code>COLLECT</code> sort:</p>

<p><img src="/downloads/screenshots/collect-nosort.png"></p>

<h2>Performance improvements</h2>

<p>The improvements achievable by using the <strong>hash</strong> method instead of the <strong>sorted</strong> method obviously
depend on whether there are appropriate indexes present for the group criteria. If an index can
be exploited, the <strong>sorted</strong> method may be just fine. However, there are cases when no indexes are
present, for example, when running arbitrary ad-hoc queries or when indexes are too expensive
(indexes need to be updated on insert/update/remove and also will use memory).</p>

<p>Following are a few comparisons of the <strong>sorted</strong> and the <strong>hash</strong> methods in case no indexes can be
used.</p>

<p>Here&rsquo;s the setup for the test data. This generates 1M documents with both unique and repeating
string and numeric values. For the non-unique values, we&rsquo;ll use 20 different categories:</p>

<p>```js setting up test data
var test = db._create(&ldquo;test&rdquo;);
for (var i = 0; i &lt; 1000000; ++i) {
  test.insert({</p>

<pre><code>uniqueNumber: i, 
uniqueString: String("test" + i), 
repeatingNumber: (i % 20), 
repeatingString: String("test" + (i % 20)) 
</code></pre>

<p>  });
}
```</p>

<p>Now let&rsquo;s run the following query on the data and measure its execution time:</p>

<p><code>plain test query
FOR v IN test
  COLLECT value = v.@attribute WITH COUNT INTO count
  RETURN { value: value, count: count }
</code></p>

<p>The worst case is when the <code>COLLECT</code> will produce as many output rows as there are input
rows. This will happen when using a unique attribute as the grouping criterion. We&rsquo;ll run
tests on both numeric and string values.</p>

<p>Here are the execution times for unique inputs. It can be seen that the <strong>hash</strong> method
here will be beneficial if the post-<code>COLLECT</code> sort can be optimized away. As demonstrated
above, this can be achieved by adding an extra <code>SORT null</code> after the <code>COLLECT</code> statement.
If the post-<code>COLLECT</code> sort is not optimized away, it will make the hash method a bit more
expensive than the <strong>sorted</strong> method:</p>

<p>```plain COLLECT performance with unique inputs</p>

<h2>collect method       @attribute                duration</h2>

<p>sorted               uniqueNumber               11.92 s
hash                 uniqueNumber               13.40 s
hash (sort null)     uniqueNumber               10.13 s
sorted               uniqueString               22.04 s
hash                 uniqueString               27.35 s
hash (sort null)     uniqueString               12.12 s
```</p>

<p>Now let&rsquo;s check the results when we group on an attribute that is non-unique. Following
are the results for numeric and string attributes with 20 different categories each:</p>

<p>```plain COLLECT performance with non-unique inputs</p>

<h2>collect method       @attribute                duration</h2>

<p>sorted               repeatingNumber             5.56 s
hash                 repeatingNumber             0.94 s
hash (sort null)     repeatingNumber             0.94 s
sorted               repeatingString            10.56 s
hash                 repeatingString             1.09 s
hash (sort null)     repeatingString             1.09 s
```</p>

<p>In these cases, the result of the <code>COLLECT</code> will be much smaller than its input (we&rsquo;ll
only get 20 result rows out instead of 1M). Therefore the post-<code>COLLECT</code> sort for the <strong>hash</strong>
method will not make any difference, but the pre-<code>COLLECT</code> sort for the <strong>sorted</strong> method
will still need to sort 1M input values. This is also the reason why the <strong>hash</strong> method
is significantly faster here.</p>

<p>As usual, your mileage may vary, so please run your own tests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating Highscore Lists]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/20/creating-highscore-lists/"/>
    <updated>2015-04-20T20:18:59+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/20/creating-highscore-lists</id>
    <content type="html"><![CDATA[<p>I just came across a question about how to create highscore lists or
leaderboards in ArangoDB, and how they would work when compared to
<a href="http://redis.io/topics/data-types-intro#sorted-sets">Redis sorted sets</a>.</p>

<p>This blog post tries to give an answer on the topic and also detailed
instructions and queries for setting up highscore lists with ArangoDB.</p>

<!-- more -->


<h2>A highscore list in Redis</h2>

<p>Highscore lists are normally used to quickly determine who&rsquo;s currently at
the top, so we obviously need some sorted data structure.</p>

<p>Redis has a specialized datatype named <em>sorted set</em> which can be used for
exactly this purpose. A sorted set in Redis is a value consisting of multiple
key/value pairs, and that is automatically sorted by values. The sorted
set is stored under a key so it can be accessed as a whole.</p>

<p>Here&rsquo;s how one would create a sorted set named <code>highscores</code> and populate
it with 5 key/value pairs in Redis (using <em>redis-cli</em>):</p>

<p>```plain creating a sorted set in Redis</p>

<blockquote><p>ZADD highscores frank 50 jan 20 willi 35 thomas 75 ingo 60
```</p></blockquote>

<p>Adding a new entry to a sorted set is done using <code>ZADD</code> too.
Inserting into a Redis sorted set has logarithmic complexity.</p>

<p>Updating a score in the sorted set is done using <code>ZINCRBY</code>. This command works
regardless of whether the to-be-updated key already exists in the sorted set.
If it exists, its score will be increased by the specified value, and if it does
not exist, it will be created with the specified value:</p>

<p>```plain updating a value in a sorted set</p>

<blockquote><p>ZINCRBY highscores 80 max
(integer) 1
```</p></blockquote>

<p>In this case the return value <code>1</code> indicates that a new key was added to the set
and that it didn&rsquo;t update an existing one.</p>

<p>Querying the entries with the lowest scores from a Redis sorted set is trivial.</p>

<p>The <code>ZRANGE</code> command will query the entries in the sorted set from lowest to
highest score. As the entries are already stored in sorted order, this is very
efficient.</p>

<p>The following command queries the bottom 3 keys from the sorted set:</p>

<p><code>plain querying the sorted set, from lowest to highest
ZRANGE highscores 0 2
1) "jan"
2) "willi"
3) "frank"
</code></p>

<p>For querying in reverse order, there is <code>ZREVRANGE</code>. Both commands can be
accompanied by the <code>WITHSCORES</code> flag to also return the associated values (i.e.
the scores). Here are the top 3 key/value pairs in the sorted set:</p>

<p>```plain querying the sorted set, from highest to lowest and with scores</p>

<blockquote><p>ZREVRANGE highscores 0 2 WITHSCORES
1) &ldquo;max&rdquo;
2) &ldquo;80&rdquo;
3) &ldquo;thomas&rdquo;
4) &ldquo;70&rdquo;
5) &ldquo;ingo&rdquo;
6) &ldquo;60&rdquo;
```</p></blockquote>

<p>For removing an entry from a sorted set there is <code>ZREM</code>:</p>

<p>```plain removing a key from a sorted set</p>

<blockquote><p>ZREM highscores jan
(integer) 1
```</p></blockquote>

<p>There are many more specialized Redis commands for working with sorted sets. The
<a href="http://redis.io/commands">Redis commands</a> prefixed with a <code>Z</code> are sorted set
commands.</p>

<h2>A highscore list in ArangoDB</h2>

<p>Now let&rsquo;s try to mimic that with ArangoDB.</p>

<p>In ArangoDB, there is no such thing as a sorted set and no direct equivalent.
Instead, data in ArangoDB are stored in collections. Collections are a
general-purpose storage mechanism and they are not limited to storing just
scores.</p>

<p>We also need a mechanism for keeping highscores sorted. By default, no
specific sort order is maintained for data in a collection. To have the
collection entries sorted by highscore values, we have to explicitly create
a (sorted) skiplist index on some attribute. We&rsquo;ll use an attribute named
<code>score</code> for this.</p>

<p>The following shell commands create the collection and the index on <code>score</code>:</p>

<p><code>js creating the highscores collection in ArangoDB
db._create("highscores");
db.highscores.ensureIndex({ type: "skiplist", fields: [ "score" ] });
</code></p>

<p>Once the collection is set up, we can switch to AQL for the following operations
(though we could achieve the same with Shell commands).</p>

<p>To insert the same initial data as in the Redis case, we can run the following
five AQL queries:</p>

<p><code>plain inserting initial scores
INSERT { _key: "frank", score: 50 } IN highscores
INSERT { _key: "jan", score: 20 } IN highscores
INSERT { _key: "willi", score: 35 } IN highscores
INSERT { _key: "thomas", score: 75 } IN highscores
INSERT { _key: "ingo", score: 60 } IN highscores
</code></p>

<p>Note that I have been using the <code>_key</code> attribute for saving the user id. Using the
<code>_key</code> attribute is normally beneficial because it is the collection&rsquo;s primary key.
It is always present and automatically unique, so exactly what we need for maintaining
a highscore list. Note that there are some restrictions for what can be stored inside
the <code>_key</code> attribute, but as long as values are only ASCII letters or digits, there
is nothing to worry about.</p>

<p>Inserting into the collection will also automatically populate the indexes.
Inserting into a skiplist should have about logarithmic complexity on average
(though this is not guaranteed &ndash; this is because the skiplist is a probabilistic
data structure and internally it will be flipping coins. In theory there is a chance
that it becomes badly balanced. But in practice it should be quite close to an
average logarithmic complexity).</p>

<p>As we have some initial documents, we can now query the lowest and highest scores.
This will also be efficient as the queries will use the sorted index on <code>score</code>:</p>

<p><code>plain querying the users with lowest scores
FOR h IN highscores
  SORT h.score ASC
  LIMIT 3
  RETURN { user: h._key, score: h.score }
</code></p>

<p><code>plain querying the users with highest scores
FOR h IN highscores
  SORT h.score DESC
  LIMIT 3
  RETURN { user: h._key, score: h.score }
</code></p>

<p>To store a highscore for a user without knowing in advance whether a value has already
been stored before for this user, one can use <code>UPSERT</code>. The <code>UPSERT</code> will either insert
a new highscore entry, or update an existing one if already present:</p>

<p><code>plain using UPSERT
UPSERT { _key: "max" }
  INSERT { _key: "max", score: 80 }
  UPDATE { score: OLD.score + 80 } IN highscores
  RETURN { user: NEW._key, score: NEW.score }
</code></p>

<p>If there is already an entry with a key <code>max</code>, its scores will be increased by 80.
If such entry does not exist, it will be created. In both cases, the new score will
be returned.</p>

<p>Note: the <code>UPSERT</code> command has been added in ArangoDB version 2.6.</p>

<p>Finally, removing an entry from a highscore list is a straight-forward remove operation:</p>

<p><code>plain removing an element
REMOVE { _key: "jan" } IN highscores
</code></p>

<h2>Extensions</h2>

<p>We&rsquo;ll now build on this simple example and create slightly more advanced highscore list
use cases. The following topics will be covered:</p>

<ul>
<li>multi-game highscore lists</li>
<li>joining data</li>
<li>maintaining a &ldquo;last updated&rdquo; date</li>
</ul>


<h3>Multi-game highscore lists</h3>

<p>We&rsquo;ll start with generalizing the single-game highscore list into a multi-game
highscore list.</p>

<p>In Redis, one would create multiple sorted sets for handling the highscore lists of
multiple games. Multiple Redis sorted sets are stored under different keys, so they
are isolated from each other.</p>

<p>Though Redis provides a few commands to aggregate data from multiple sorted sets
(<code>ZUNIONSTORE</code> and <code>ZINTERSTORE</code>) into a new sorted set, other cross-set operations are
not supported. This is not a problem if the client application does not have to
perform cross-set queries or cleanup tasks.</p>

<p>In ArangoDB, multi-game highscore lists can be implemented in two variants.<br/>
In order to decide which variant is better suited, we need to be clear about whether
all highscores should be stored in the same collection or if we prefer using multiple
collections (e.g. one per game).</p>

<p>Storing highscores for different games in separate collections has the advantage that
they&rsquo;re really isolated. It is easy to get rid of a specific highscore list by simply
dropping its collection. It is also easy to get right query-wise.</p>

<p>All that needs to be changed to turn the above examples into a multi-game highscore
list solution is to change the hard-coded collection name <code>highscores</code> and make it a
bind parameter, so the right collection name can be injected by the client application
easily.</p>

<p>On the downside, the multi-collection solution will make cross-game operations difficult.
Additionally, having one collection per game may get out of hand when there are many,
many highscore lists to maintain. In case there are many but small highscore lists to
maintain, it might be better to put them into a single collection and add a <code>game</code>
attribute to tell the individual lists apart in it.</p>

<p>Let&rsquo;s focus on this and put all highscores of all games into a single collection.</p>

<p>The first adjustment that needs to be made is that we cannot use <code>_key</code> for user ids
anymore. This is because user ids may repeat now (a user may be contained in more than
one list). So we will change the design and make the combination of <code>game</code> and <code>user</code>
a new unique key:</p>

<p><code>js creating a multi-game highscore collection
db._drop("highscores");
db._create("highscores");
db.highscores.ensureIndex({ type: "hash", unique: true, fields: [ "user", "game" ] });
db.highscores.ensureIndex({ type: "skiplist", fields: [ "game", "score" ] });
</code></p>

<p>We can use the unique hash index on <code>user</code> and <code>game</code> to ensure there is at most one entry
for per user per game. It also allows use to find out quickly whether we already have
an entry for that particular combination of game and user. Because we are not using
<code>_key</code> we could now also switch to numeric ids if we preferred that.</p>

<p>The other index on <code>game</code> and <code>score</code> is sorted. It can be used to quickly retrieve the
leaderboard for a given game. As it is primarily sorted by <code>game</code>, it can also be used
to enumerate all entries for a given game.</p>

<p>The following Shell command populates the multi-game highscores collection with 55,000
highscores:</p>

<p>```js populating the multi-game collection
for (var game = 0; game &lt; 10; ++game) {
  for (var user = 0; user &lt; (game + 1) * 1000; ++user) {</p>

<pre><code>db.highscores.save({ 
  game: game, 
  user: String(user),
  score: (game + user) % 997  /* arbitrary score */
}); 
</code></pre>

<p>  }
}
```</p>

<p>The game ids used above are between 0 and 9, though any other game ids would work, too.
User ids are stringified numbers.</p>

<p>We can now find out the leaderboard for game 2 with the following adjusted AQL query.
The query will use the (sorted) skiplist index:</p>

<p><code>plain querying the leaderboard of a specific game
FOR h IN highscores
  FILTER h.game == 2
  SORT h.score DESC
  LIMIT 3
  RETURN { user: h.user, score: h.score }
</code></p>

<p>Removing all scores for a specific game is also efficient due to the the same index:</p>

<p><code>plain removing all scores for game 5
FOR h IN highscores
  FILTER h.game == 5
  REMOVE h IN highscores
</code></p>

<p>On a side note: when storing all highscores in the same collection, we could also
run cross-game queries if we wanted to. All that needs to be done for this is adjusting
the <code>FILTER</code> conditions in the queries.</p>

<p>Inserting or updating a user score can be achieved using an <code>UPSERT</code>.
Here&rsquo;s a query to increase the score of user <code>"1571"</code> in game <code>2</code> by a value of 5:</p>

<p><code>plain updating a score for a specific user/game combination
UPSERT { game: 2, user: "1571" }
  INSERT { game: 2, user: "1571", score: 5 }
  UPDATE { score: OLD.score + 5 } IN highscores
  RETURN { user: NEW._key, score: NEW.score }
</code></p>

<p>The same index on <code>[ "user", "game" ]</code> is used in the following query that will
delete the highscore of a given user in a specific game:</p>

<p><code>plain removing a score for a specific user/game combination
FOR h IN highscores
  FILTER h.game == 6
  FILTER h.user == '3894'
  REMOVE h IN highscores
</code></p>

<h3>Joining data</h3>

<p>Querying the leaderboard for a specific game was easy. However, so far we have only
queried user ids and associated scores in games. In reality, we probably want to display
some more user information in a leaderboard, for example their screen names.</p>

<p>In Redis, no extra information can be stored in sorted sets. So extra user information
must be stored under separate keys. There is no concept of joins in Redis. The scores
contained in the sorted set need to be queried by the client application, and extra
user information have to be queried by the client application separately.</p>

<p>In ArangoDB, we could store the screen names in the highscores collection along with
the highscores so we can easily query them with the leaderboard query. This is also how it
would be done in MongoDB due to the absence of joins there.</p>

<p>While this would work, it will create lots of redundant data if the screen names are
also used and stored elsewhere.</p>

<p>So let&rsquo;s pick the option that stores highscores and screen names in separate places,
and brings them together only when needed in a leaderboard query.</p>

<p>Let&rsquo;s store screen names in a collection named <code>users</code>. The following Shell commands
will create the collection and set up 100K users with dummy screen names:</p>

<p>```js creating test users
db._create(&ldquo;users&rdquo;);
for (var i = 0; i &lt; 100000; ++i) {
  db.users.insert({</p>

<pre><code>_key: String(i), 
name: "test user #" + i 
</code></pre>

<p>  });
}
```</p>

<p>We can now query the highscores plus the screen name in one go:</p>

<p>```plain joining highscores with user data
FOR h IN highscores
  FILTER h.game == 2
  SORT h.score DESC
  LIMIT 3
  FOR u IN users</p>

<pre><code>FILTER h.user == u._key 
RETURN { user: h.user, name: u.name, score: h.score } 
</code></pre>

<p>```</p>

<h3>Maintaining a &ldquo;last updated&rdquo; date</h3>

<p>Finally, let&rsquo;s try to keep track of when a highscore was last updated. There are
a few use cases for this, for example displaying the date and time of when a highscore
was achieved or for revmoing older highscores.</p>

<p>In Redis, the sorted set values are just the numeric scores, so we cannot store
anything else (such as a date) inside the sorted sets. We would really need to store
the update date for each highscore entry outside the sorted set, either under a
separate key, or using a Redis hash. However, this is complex to manage and keep
consistent so we won&rsquo;t do it.</p>

<p>For implementing the automatic expiration, it would be good if we could use the
built-in automatic key expiration of Redis. Each key can optionally be given a time-to-live
or an expiration date, and it will automatically expire and vanish then without further
ado. This may be exactly what we need to remove older highscore entries, but we cannot
use it. The reason is that expiration only works for keys at the top level, but not
for individual keys inside a sorted set. So we cannot really implement this sanely.</p>

<p>Let&rsquo;s switch to ArangoDB now. Here we work with arbitrarily structured documents.
That means we can store any other attributes along with a highscore. We can store the
timestamp of when a highscore was last set or updated in an attribute named <code>date</code>:</p>

<p><code>plain storing the date of last update
LET now = DATE_NOW()
UPSERT { game: 2, user: "1571" }
  INSERT { game: 2, user: "1571", score: 10, date: now }
  UPDATE { score: OLD.score + 10, date: now } IN highscores
  RETURN { user: NEW._key, score: NEW.score }
</code></p>

<p>The <code>date</code> attribute can now be used for display purposes already.</p>

<p>We can also use the <code>date</code> attribute for identifying the oldest entries in the
highscore list in case we want the list to be periodically cleaned up.</p>

<p>Obviously we will be indexing <code>date</code> for this, but we need to decide whether we want to use
the same expiration periods for all games, or if we want to use game-specific expirations.</p>

<p>If the expiration date is the same for all games, then we can index just <code>date</code>:</p>

<p><code>js creating the index on date
db.highscores.ensureIndex({ type: "skiplist", fields: [ "date" ] });
</code></p>

<p>If we now want to remove entries older than roughly 2 days, regardless of the
associated game, the removal query looks like this:</p>

<p><code>plain deleting oldest entries
LET compare = DATE_NOW() - 2 * 86400 * 1000
FOR h IN highscores
  FILTER h.date &lt; compare
  LIMIT 1000
  REMOVE h IN highscores
</code></p>

<p>If we instead want to find (and remove) the oldest entries for individual games,
we need to create the index on <code>game</code> and <code>date</code>:</p>

<p><code>js creating the index on game and date
db.highscores.ensureIndex({ type: "skiplist", fields: [ "game", "date" ] });
</code></p>

<p>This index allows to efficiently get rid of the oldest entries per game:</p>

<p><code>plain remvoin oldest entries for a game
LET compare = DATE_NOW() - 2 * 86400 * 1000
FOR h IN highscores
  FILTER h.game == 2
  FILTER h.date &lt; compare
  LIMIT 1000
  REMOVE h IN highscores
</code></p>

<p>On a side note: the <code>REMOVE</code> was limited to the <em>oldest</em> 1000 entries. This
was done to make the query return fast. The removal query can be repeated while
there are still entries to remove.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Tour Around the New AQL Query Optimizer]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer/"/>
    <updated>2014-11-07T22:30:10+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer</id>
    <content type="html"><![CDATA[<p>The major new feature in ArangoDB 2.3 is the shiny new
AQL query optimizer and executor. These parts of ArangoDB have been
rewritten in 2.3 to make AQL much better for our end users.</p>

<!-- more -->


<p>Since one of the initial releases, ArangoDB has been shipped with
AQL, the <em>ArangoDB Query Language</em>. AQL has since then been ArangoDB&rsquo;s
most versatile way of executing simple and also the not-so-simple
queries.</p>

<p>I&rsquo;ll start with an overview of query execution in previous versions
of ArangoDB, and then explain the new engine and explain the differences.</p>

<h2>History: query execution in pre-2.3</h2>

<p>Previous versions of ArangoDB executed any AQL query in the following
steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>collect filter conditions in AST and look for index usage opportunities</li>
<li>generate code</li>
<li>execute code</li>
</ul>


<p>This approach was simple and has worked for a lot of queries, but it also
had a few quirks:</p>

<p>First of all, most of the steps were carried out directly on the
abstract syntax tree, with the AST nodes being modified in place.
There was also just the one AST per query, so the old AQL executor
could not generate multiple, potentially very different execution
plan candidates for a given query.</p>

<p>The &ldquo;old&rdquo; optimizer was able to move AST nodes around during optimization
and it was already able to consider multiple index candidates for a query,
but it would not compare multiple plans and make a cost-based decision.
It was also limited in the amount and scope of transformations it
could safely apply to the AST.</p>

<p>When it came to code generation and execution, the &ldquo;old&rdquo; executor
fully relied on V8 to execute the queries. Result sets were created
using V8&rsquo;s value objects. Documents from collections that queries
needed to iterate over had to be made available to V8. While some
optimization was used for this, the conversions could have summed up
to significant overhead for certain kinds of queries.</p>

<p>The representation of queries via an AST also made it hard to generate
code that supported lazy evaluation during query execution.</p>

<p>Finally, the AQL optimizer so far did not provide much support for
queries that were to be executed in a distributed fashion inside a cluster
of servers.</p>

<h2>Query execution in ArangoDB 2.3</h2>

<p>We wanted to address all these issues with a rewrite of the AQL
infrastructure. Starting with ArangoDB 2.3, AQL queries are executed
in these steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>transform AST into execution plan</li>
<li>optimize and permute execution plans</li>
<li>estimate costs for execution plans and pick optimal plan</li>
<li>instanciate execution engine from optimal plan</li>
<li>(in cluster only) send execution plan parts to cluster nodes</li>
<li>execute query</li>
</ul>


<p>Tokenization and parsing of AQL queries hasn&rsquo;t changed much in 2.3:
query strings are still parsed using a Bison/Flex-based parser and
lexer combo. The AST structure has proven to be good during the parsing
stage, so the parser creates an initial AST from the query string first.</p>

<p>After that, simple optimizations are performed directly on the AST,
such as constant folding and constant propagation. Deterministic functions
with constant operands will be executed already in this stage and the
results be injected into the AST.</p>

<p>A major change in 2.3 is that no further transformations will be
carried out on the AST. Instead, the AST will be transformed into
an initial <em>execution plan</em>.</p>

<p>This execution plan is the starting point for the <em>query optimizer</em>.
It will take the initial execution plan and apply transformations to
it. Transformations will either update the existing plan in place or
create a new, modified plan. The result of the transformations carried
out will form the input for further transformations that can be carried
out by query optimizer.</p>

<p>The result of the query optimization stage is one or many execution
plans. For each plan, the optimizer will estimate a cost value, and
then finally pick the plan with the lowest total estimated cost.
This plan is considered to be the <em>optimal plan</em>. All other execution
plans will be discarded by the optimizer as it has considered them non-optimal.</p>

<p>The optimal execution plan is then executed by the <em>execution engine</em>.
For a single-server AQL query, this is straightforward: for each step
in the execution plan, a C++ object is created that is supposed to
execute the particular step. Query execution is then started by asking
the first of these objects for its results.</p>

<p>The objects for multiple processing steps are linked in a pipelined fashion
with lazy evaluation. Pulling data from the first object will eventually
trigger pulling data from the second object etc., until there are no more
results to produce.</p>

<p>For a distributed query, this is a bit more complicated. The different
execution steps will likely be shipped to different servers in the
cluster, and the objects need to be instanciated in different servers, too.
The different parts of the query may pull data from each other via HTTP
calls between cluster nodes.</p>

<h2>How execution plans work</h2>

<p>An execution plan is a sequence of query execution steps. Let&rsquo;s
start with a very simple example:</p>

<p><code>
FOR doc IN mycollection
  RETURN doc._key
</code></p>

<p>This query will be transformed into the following execution plan:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: iterates over all documents of a collection
and provides the current document in an output variable. In our example,
it will iterate over collection <code>mycollection</code> and provide each
document in variable <code>doc</code></li>
<li><em>CalculationNode</em>: evaluates an expression and returns its result.
In the example, it will calculate <code>doc._key</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>If this plan is going to be executed, the execution engine will start
pulling data from the node at the bottom, that is, the <em>ReturnNode</em>. The
<em>ReturnNode</em> at this stage cannot provide any data, so it will ask its
predecessor node, which in the example is the <em>CalculationNode</em>. The
<em>CalculationNode</em> again does not have own data yet, so it must ask the
node in front of it. The <em>EnumerateCollectionNode</em> will first ask the
<em>SingletonNode</em> for input data. So the execution flow has bubbled up from
the bottom of the sequence to the top.</p>

<p>The <em>SingletonNode</em> will now produce a single empty return value. It will
also internally set its processing status to <em>done</em>, so it will not produce
any more values if asked again. This is all a <em>SingletonNode</em> will ever do.
We&rsquo;ll see later why such a node may still be useful.</p>

<p>The single empty value will be provided as input to the <em>EnumerateCollectionNode</em>.
This node will now go through all the documents in the underlying collection,
and return them once for each input value its got. As its input value was
the singleton, it will return the documents of the collection just once.</p>

<p>Processing is executed in blocks of size 1000 by default. The
<em>EnumerateCollectionNode</em> will thus not return all documents to its successor
node, but just 1,000. The return value will be a vector with 1,000 documents,
stored under variable name <code>doc</code>.</p>

<p>The <em>CalculationNode</em>, still waiting for input data, can now execute its
expression <code>doc._key</code> on this input value. It will execute this expression
1,000 times, once for each input value. The expression results will be
stored in another variable. This variable is anonymous, as it hasn&rsquo;t been
named explicitly in the original query. The vector of results produced by
the <em>CalculationNode</em> is then returned to the <em>ReturnNode</em>, which will then
return it to the caller.</p>

<p>If the caller requests more documents, the procedure will repeat. Whenever
a processing step cannot produce any more data, it will ask its predecessor
step for more data. If the predecessor step already has status <em>done</em>, the
current step will set itself to <em>done</em> as well, so a query will actually
come to an end if there are no more results.</p>

<p>As can be seen, steps are executed with batches of values. We thought this
would be a good way to improve efficiency and reduce the number of hops
between steps.</p>

<h2>Joins</h2>

<p>Let&rsquo;s say we want to join documents from two collections, based on common
attribute values. Let&rsquo;s use <code>users</code> and <code>logins</code>, joined by their <code>id</code> and
<code>userId</code> attributes:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Provided that there are no indexes, the query may be turned into this
execution plan by the optimizer:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>logins</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>Now we can see why the <em>SingletonNode</em> is useful: it can be used as an
input to another node, telling this node to execute just once. Having the
<em>SingletonNode</em> will ensure that the outermost <em>EnumerateCollection</em>
will only iterate once over the documents in its underlying collection <code>users</code>.</p>

<p>The inner <em>EnumerateCollectionNode</em> for collection <code>logins</code> is now fed by
the outer <em>EnumerateCollectionNode</em> on <code>users</code>. Thus these two nodes will
produce a cartesian product. This will be done lazily, as producing results
will normally happen in chunks of 1,000 values each.</p>

<p>The results of the cartesian product are then post-filtered by the <code>FilterNode</code>,
which will only let those documents pass that match the filter condition of
the query. The <code>FilterNode</code> employs its predecessor, the <code>CalculationNode</code>,
to determine which values satisfy the condition.</p>

<h2>Using indexes</h2>

<p>Obviously creating cartesian products is not ideal. The optimizer will try
to avoid generating such plans if it can, but it has no choice if there are
no indexes present.</p>

<p>If there are indexes on attributes that are used in <code>FILTER</code> conditions of
a query, the optimizer will try to turn <code>EnumerateCollectionNode</code>s into
<code>IndexRangeNode</code>s. The purpose of an <code>IndexRangeNode</code> is to iterate over a
specific range in an index. This is normally more efficient than iterating
over all documents of a collection.</p>

<p>Let&rsquo;s assume there is an index on <code>logins.userId</code>. Then the optimizer might
be able to generate a plan like this:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>IndexRangeNode</em>: will iterate over the values in index <code>logins.userId</code> that
match the value of <code>users.id</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>To run this query, the execution engine must still iterate over all documents
in collection <code>users</code>, but for each of those, it only needs to find the documents
in <code>logins</code> that match the join condition. This most likely means a lot less
lookups and thus much faster execution.</p>

<h2>Permutation of loops</h2>

<p>Now consider adding an extra <code>FILTER</code> statement to the original query so we
end up with this:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
FILTER login.ts == 1415402319       /* added this one! */
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>The optimizer is free to permute the order of <code>FOR</code> loops as long as this
won&rsquo;t change the results of a query. In our case, permutation of the two
<code>FOR</code> loops is allowed (the query does not contain a <code>SORT</code> instruction so
the order of results is not guaranteed).</p>

<p>If the optimizer exchanges the two loops, it can also pull out the <code>FILTER</code>
statement on <code>login.ts</code> out of the inner loop, and move up into the outer loop.
It might come up with a plan like this, which may be more efficient if a
lot of documents from <code>logins</code> can be filtered out early:</p>

<p>```
FOR login IN logins
  FILTER login.ts == 1415402319
  FOR user IN users</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Exchanging the order of <code>FOR</code> loops may also allow the optimizer to use
additional indexes.</p>

<p>A last note on indexes: the optimizer in 2.3 is able to use (sorted)
skiplist indexes to eliminate extra <code>SORT</code> operations. For example, if
there is a skiplist index on <code>login.ts</code>, the <code>SORT</code> in the following
query can be removed by the optimizer:</p>

<p><code>
FOR login IN logins
  FILTER login.ts &gt; 1415402319
  SORT login.ts
  RETURN login
</code></p>

<p>The AQL optimizer in 2.3 can optimize away a <code>SORT</code> even if the sort
order is backwards or if no <code>FILTER</code> statement is used in the query at
all.</p>

<h2>Analyzing plans</h2>

<p>One particular improvement over 2.2 is that in ArangoDB 2.3 the optimizer
provides functionality for retrieving full execution plan information for
queries <strong>without</strong> executing them. The execution plan information can be
inspected by developers or DBAs, and, as it is JSON-encoded, can also be
analyzed programmatically.</p>

<p>Retrieving the execution plan for a query is straight-forward:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain();
</code></p>

<p>By default, the optimizer will return just the <em>optimal plan</em>, containing
all the plan&rsquo;s execution nodes with lots of extra information plus cost estimates.</p>

<p>The optimizer is also able to return the alternative plans it produced but
considered to be non-optimal:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain({ allPlans: true });
</code></p>

<p>This will hopefully allow developers and DBAs to get a better idea of how an
AQL query will be executed internally.</p>

<p>Additionally, simple execution statistics are returned by default when executing
a query. This statistics can also be used to get an idea of the runtime costs of
a query <strong>after</strong> execution.</p>

<h2>Writing optimizer rules</h2>

<p>The AQL optimizer itself is dumb. It will simply try to apply all transformations
from its rulebook to each input execution plan it is feeded with. This
will produce output execution plans, on which further transformations
may or may not be applied.</p>

<p>The more interesting part of the AQL optimizer stage is thus the rulebook.
Each rule in the rulebook is a C++ function that is executed for an input plan.</p>

<p>Adding a new optimizer rule to the rulebook is intentionally simple. One of
the design goals of the new AQL optimizer was to keep it flexible and extensible.
All that&rsquo;s need to be to add an optimizer rule is to implement a C++ function
with the following signature:</p>

<p><code>cpp
(Optimizer*, ExecutionPlan*, Optimizer::Rule const*) -&gt; int
</code></p>

<p>and register it once in the Optimizer&rsquo;s rulebook.</p>

<p>An optimizer rule function is called with an instance of the query optimizer
(it can use it to register a new plan), the current execution plan and some
information about the rule itself (this is the information about the rule from
the rulebook).</p>

<p>The optimizer rule function can then analyze the input execution plan, modifiy
it in place, and/or create additional plans. It must return a status code to
the optimizer to indicate if something went wrong.</p>

<h2>Outlook</h2>

<p>The AQL optimizer features described here are available in ArangoDB 2.3, which
is currently in <a href="https://www.arangodb.com/install-beta-version">beta stage</a>.</p>

<p>Writing a perfect query optimizer is a never-ending endeavour. Other databases
provide new optimizer features and fixes even decades after the initial version.</p>

<p>Our plan is to ship 2.3 with several essential and useful optimizer rules. We
will likely add more in future releases. We&rsquo;re also open to contributions.
If you can think of rules that are missing but you would like to see in ArangoDB,
please let us know. If you would like to contribute to the optimizer and write some
rule code, consider sending a pull request or an email to
<a href="hackers@arangodb.org">hackers@arangodb.org</a>.</p>
]]></content>
  </entry>
  
</feed>
