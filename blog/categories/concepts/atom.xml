<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Concepts | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/concepts/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2014-11-08T02:20:24+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Tour Around the New AQL Query Optimizer]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer/"/>
    <updated>2014-11-07T22:30:10+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer</id>
    <content type="html"><![CDATA[<p>The major new feature in ArangoDB 2.3 is the shiny new
AQL query optimizer and executor. These code parts have been
rewritten in 2.3 to make AQL much better for our end users.</p>

<!-- more -->


<p>Since one of the initial releases, ArangoDB has been shipped with
AQL, the <em>ArangoDB Query Language</em>. AQL has since then been ArangoDB&rsquo;s
most versatile way of executing simple and also the not-so-simple
queries.</p>

<p>I&rsquo;ll start with an overview of query execution in previous versions
of ArangoDB, and then explain the new engine and explain the differences.</p>

<h2>History: query execution in pre-2.3</h2>

<p>Previous versions of ArangoDB executed any AQL query in the following
steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>collect filter conditions in AST and look for index usage opportunities</li>
<li>generate code</li>
<li>execute code</li>
</ul>


<p>This approach was simple and has worked for a lot of queries, but it also
had a few quirks:</p>

<p>First of all, most of the steps were carried out directly on the
abstract syntax tree, with the AST nodes being modified in place.
There was also just the one AST per query, so the old AQL executor
could not generate multiple, potentially very different execution
plan candidates for a given query.</p>

<p>The &ldquo;old&rdquo; optimizer was able to move AST nodes around during optimization
and it was already able to consider multiple index candidates for a query,
but it would not compare multiple plans and make a cost-based decision.
It was also limited in the amount and scope of transformations it
could safely apply to the AST.</p>

<p>When it came to code generation and execution, the &ldquo;old&rdquo; executor
fully relied on V8 to execute the queries. Result sets were created
using V8&rsquo;s value objects. Documents from collections that queries
needed to iterate over had to be made available to V8. While some
optimization was used for this, the conversions could have summed up
to significant overhead for certain kinds of queries.</p>

<p>The representation of queries via an AST also made it hard to generate
code that supported lazy evaluation during query execution.</p>

<p>Finally, the AQL optimizer so far did not provide much support for
queries that were to be executed in a distributed fashion inside a cluster
of servers.</p>

<h2>Query execution in ArangoDB 2.3</h2>

<p>We wanted to address all these issues with a rewrite of the AQL
infrastructure. Starting with ArangoDB 2.3, AQL queries are executed
in these steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>transform AST into execution plan</li>
<li>optimize and permute execution plans</li>
<li>estimate costs for execution plans and pick optimal plan</li>
<li>instanciate execution engine from optimal plan</li>
<li>(in cluster only) send execution plan parts to cluster nodes</li>
<li>execute query</li>
</ul>


<p>Tokenization and parsing of AQL queries hasn&rsquo;t changed much in 2.3:
query strings are still parsed using a Bison/Flex-based parser and
lexer combo. The AST structure has proven to be good during the parsing
stage, so the parser creates an initial AST from the query string first.</p>

<p>After that, simple optimizations are performed directly on the AST,
such as constant folding and constant propagation. Deterministic functions
with constant operands will be executed already in this stage and the
results be injected into the AST.</p>

<p>A major change in 2.3 is that no further transformations will be
carried out on the AST. Instead, the AST will be transformed into
an initial <em>execution plan</em>.</p>

<p>This execution plan is the starting point for the <em>query optimizer</em>.
It will take the initial execution plan and apply transformations to
it. Transformations will either update the existing plan in place or
create a new, modified plan. The result of the transformations carried
out will form the input for further transformations that can be carried
out by query optimizer.</p>

<p>The result of the query optimization stage is one or many execution
plans. For each plan, the optimizer will estimate a cost value, and
then finally pick the plan with the lowest total estimated cost.
This plan is considered to be the <em>optimal plan</em>. All other execution
plans will be discarded by the optimizer as it has considered them non-optimal.</p>

<p>The optimal execution plan is then executed by the <em>execution engine</em>.
For a single-server AQL query, this is straightforward: for each step
in the execution plan, a C++ object is created that is supposed to
execute the particular step. Query execution is then started by asking
the first of these object for its results.</p>

<p>The objects for multiple processing steps are linked in a pipelined fashion
with lazy evaluation. Pulling data from the first object will eventually
trigger pulling data from the second object etc., until there are no more
results to produce.</p>

<p>For a distributed query, this is a bit more complicated. The different
execution steps will likely be shipped to different servers in the
cluster, and the objects need to be instanciated in different servers, too.
The different parts of the query may pull data from each other via HTTP
calls between cluster nodes.</p>

<h2>How execution plans work</h2>

<p>An execution plan is a sequence of query execution steps. Let&rsquo;s
start with a very simple example:</p>

<p><code>
FOR doc IN mycollection
  RETURN doc._key
</code></p>

<p>This query will be transformed into the following execution plan:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: iterates over all documents of a collection
and provides the current document in an output variable. In our example,
it will iterate over collection <code>mycollection</code> and provide each
document in variable <code>doc</code></li>
<li><em>CalculationNode</em>: evaluates an expression and returns its result.
In the example, it will calculate <code>doc._key</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>If this plan is going to be executed, the execution engine will start
pulling data from the node at the bottom, that is, the <em>ReturnNode</em>. The
<em>ReturnNode</em> at this stage cannot provide any data, so it will ask its
predecessor node, which in the example is the <em>CalculationNode</em>. The
<em>CalculationNode</em> again does not have own data yet, so it must ask the
node in front of it. The <em>EnumerateCollectionNode</em> will first ask the
<em>SingletonNode</em> for input data. So the execution flow has bubbled up from
the bottom of the sequence to the top.</p>

<p>The <em>SingletonNode</em> will now produce a single empty return value. It will
also internally set its processing status to <em>done</em>, so it will not produce
any more values if asked again. This is all a <em>SingletonNode</em> will ever do.
We&rsquo;ll see later why such a node may still be useful.</p>

<p>The single empty value will be provided as input to the <em>EnumerateCollectionNode</em>.
This node will now go through all the documents in the underlying collection,
and return them once for each input value its got. As its input value was
the singleton, it will return the documents of the collection just once.</p>

<p>Processing is executed in blocks of size 1000 by default. The
<em>EnumerateCollectionNode</em> will thus not return all documents to its successor
node, but just 1,000. The return value will be a vector with 1,000 documents,
stored under variable name <code>doc</code>.</p>

<p>The <em>CalculationNode</em>, still waiting for input data, can now execute its
expression <code>doc._key</code> on this input value. It will execute this expression
1,000 times, once for each input value. The expression results will be
stored in another variable. This variable is anonymous, as it hasn&rsquo;t been
named explicitly in the original query. The vector of results produced by
the <em>CalculationNode</em> is then returned to the <em>ReturnNode</em>, which will then
return it to the caller.</p>

<p>If the caller requests more documents, the procedure will repeat. Whenever
a processing step cannot produce any more data, it will ask its predecessor
step for more data. If the predecessor step already has status <em>done</em>, the
current step will set itself to <em>done</em> as well, so a query will actually
come to an end if there are no more results.</p>

<p>As can be seen, steps are executed with batches of values. We thought this
would be a good way to improve efficiency and reduce the number of hops
between steps.</p>

<h2>Joins</h2>

<p>Let&rsquo;s say we want to join documents from two collections, based on common
attribute values. Let&rsquo;s use <code>users</code> and <code>logins</code>, joined by their <code>id</code> and
<code>userId</code> attributes:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Provided that there are no indexes, the query may be turned into this
execution plan by the optimizer:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>logins</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>Now we can see why the <em>SingletonNode</em> is useful: it can be used as an
input to another node, telling this node to execute just once. Having the
<em>SingletonNode</em> will ensure that the outermost <em>EnumerateCollection</em>
will only iterate once over the documents in its underlying collection <code>users</code>.</p>

<p>The inner <em>EnumerateCollectionNode</em> for collection <code>logins</code> is now fed by
the outer <em>EnumerateCollectionNode</em> on <code>users</code>. Thus these two nodes will
produce a cartesian product. This will be done lazily, as producing results
will normally happen in chunks of 1,000 values each.</p>

<p>The results of the cartesian product are then post-filtered by the <code>FilterNode</code>,
which will only let those documents pass with match the filter condition of
the query. The <code>FilterNode</code> employs its predecessor, the <code>CalculationNode</code>,
to determine which values satisfy the condition.</p>

<h2>Using indexes</h2>

<p>Obviously creating cartesian products is not ideal. The optimizer will try
to avoid generating such plans if it can, but it has no choice if there are
no indexes present.</p>

<p>If there are indexes on attributes that are used in <code>FILTER</code> conditions of
a query, the optimizer will try to turn <code>EnumerateCollectionNode</code>s into
<code>IndexRangeNode</code>s. The purpose of an <code>IndexRangeNode</code> is to iterate over a
specific range in an index. This is normally more efficient than iterating
over all documents of a collection.</p>

<p>Let&rsquo;s assume there is an index on <code>logins.userId</code>. Then the optimizer might
be able to generate a plan like this:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>IndexRangeNode</em>: will iterate over the values in index <code>logins.userId</code> that
match the value of <code>users.id</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>To run this query, the execution engine must still iterate over all documents
in collection <code>users</code>, but for each those, it must only find the documents
in <code>logins</code> that match the join condition. Probably that means a lot less
documents and thus much faster execution.</p>

<h2>Permutation of loops</h2>

<p>Now consider adding an extra <code>FILTER</code> statement to the original query so we
end up with this:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
FILTER login.ts == 1415402319       /* added this one! */
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>The optimizer is free to permute the order of <code>FOR</code> loops as long as this
won&rsquo;t change the results of a query. In our case, permutation of the two
<code>FOR</code> loops is allowed. If the optimizer exchanges the two loops, it can
also pull out the <code>FILTER</code> statement on <code>login.ts</code> out of the inner loop
into the outer loop.</p>

<p>It might come up with a plan like this, which may be more efficient if a
lot of documents from <code>logins</code> can be filtered out early:</p>

<p>```
FOR login IN logins
  FILTER login.ts == 1415402319
  FOR user IN users</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Exchanging the order of <code>FOR</code> loops may also allow the optimizer to use
additional indexes.</p>

<p>A last note on indexes: the optimizer in 2.3 is able to use (sorted)
skiplist indexes to eliminate extra <code>SORT</code> operations. For example, if
there is a skiplist index on <code>login.ts</code>, the <code>SORT</code> in the following
query can be removed by the optimizer:</p>

<p><code>
FOR login IN logins
  FILTER login.ts &gt; 1415402319
  SORT login.ts
  RETURN login
</code></p>

<p>The AQL optimizer in 2.3 can optimize away a <code>SORT</code> even if the sort
order is backwards or if no <code>FILTER</code> statement is used in the query at
all.</p>

<h2>Analyzing plans</h2>

<p>One particular improvement over 2.2 is that in ArangoDB 2.3 the optimizer
provides functionality for retrieving full execution plan information for
queries <strong>without</strong> executing them. The execution plan information can be
inspected by developers or DBAs, and, as it is JSON-encoded, can also be
analyzed programmatically.</p>

<p>Retrieving the execution plan for a query is straight-forward:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain();
</code></p>

<p>By default, the optimizer will return just the <em>optimal plan</em>, containing
all the plan&rsquo;s execution nodes with lots of extra information plus cost estimates.</p>

<p>The optimizer is also able to return the alternative plans it produced but
considered to be non-optimal:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain({ allPlans: true });
</code></p>

<p>This will hopefully allow developers and DBAs to get a better idea of how an
AQL query will be executed internally.</p>

<p>Additionally, simple execution statistics are returned by default when executing
a query. This statistics can also be used to get an idea of the runtime costs of
a query <strong>after</strong> execution.</p>

<h2>Writing optimizer rules</h2>

<p>The AQL optimizer itself is dumb. It will simply try to apply all transformations
from its rulebook to each input execution plan it is feeded with. This
will produce output execution plans, on which further transformations
may or may not be applied.</p>

<p>The more interesting part of the AQL optimizer stage is thus the rulebook.
Each rule in the rulebook is a C++ function that is executed for an input plan.</p>

<p>Adding a new optimizer rule to the rulebook is intentionally simple. One of
the design goals of the new AQL optimizer was to keep it flexible and extensible.
All that&rsquo;s need to be to add an optimizer rule is to implement a C++ function
with the following signature:</p>

<p><code>cpp
(Optimizer*, ExecutionPlan*, Optimizer::Rule const*) -&gt; int
</code></p>

<p>and register it once in the Optimizer&rsquo;s rulebook.</p>

<p>An optimizer rule function is called with an instance of the query optimizer
(it can use it to register a new plan), the current execution plan and some
information about the rule itself (this is the information about the rule from
the rulebook).</p>

<p>The optimizer rule function can then analyze the input execution plan, modifiy
it in place, and/or create additional plans. It must return a status code to
the optimizer to indicate if something went wrong.</p>

<h2>Outlook</h2>

<p>Writing a perfect query optimizer is a never-ending endeavour. Other databases
provide new optimizer features and fixes even decades after the initial version.</p>

<p>Our plan is to ship 2.3 with several essential and useful optimizer rules. We
will likely add more in future releases. We&rsquo;re also open to contributions.
If you can think of rules that are missing but you would like to see in ArangoDB,
please let us know. If you would like to contribute to the optimizer and write some
rule code, consider sending a pull request or an email to
<a href="hackers@arangodb.org">hackers@arangodb.org</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why JSON?]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/14/why-json/"/>
    <updated>2014-08-14T22:27:27+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/14/why-json</id>
    <content type="html"><![CDATA[<h1>To JSON or not?</h1>

<p>We&rsquo;re often asked why ArangoDB uses <a href="http://json.org">JSON</a> as its
data-interchange format for transferring documents from clients to
the database server and back. This is often accompanied by the
question if we could use <code>&lt;insert fancy format here&gt;</code> instead.</p>

<p>In the following article, I&rsquo;ll try to outline the reasons for why
we picked JSON as the interchange format, and why we still use it.</p>

<p>I&rsquo;ll start with a discussion of the pros and cons of JSON, look at
some alternative formats and present my personal opinion on why
using a different format may not provide too much benefit for us,
at least at the moment.</p>

<!-- more -->


<p>This post does not intend to say that any of these formats are better
or worse in general. I think there are applications for all of them.</p>

<p>However, I wanted to look at the different formats with our specific
use case, i.e. a RESTful database, in mind.</p>

<h1>What I don&rsquo;t like about JSON</h1>

<p>JSON is often criticized for its inefficiency and lack of <strong>real</strong>
data types. I&rsquo;ll often criticize it myself.</p>

<p>Following are my personal top 3 pain points.</p>

<h2>Parsing and memory allocation</h2>

<p>I have to admit that parsing JSON is painful from the efficiency
perspective. When the JSON parser encounters a <code>{</code> token,
it will know this is the start of an object, but it has no idea how
many object members will follow and need to be stored with the
object. The same is true for lists (starting with <code>[</code>).</p>

<p>String values are no different: when the parser encounters a <code>"</code>,
the length of the string is still unknown. To determine the length
of the string, one must read until the end of the string, taking
into account escape sequences for special characters, e.g. <code>\/</code>,
<code>\n</code>, <code>\t</code>, <code>\\</code>, but also Unicode escape sequences.</p>

<p>For example, the escaped 36-byte string <code>In K\u00f6ln, it's 15 \u00b0 outside</code>
will be parsed into the 28-byte UTF-8 string <code>In Köln, it's 15 ° outside</code>.</p>

<p>With the overall size of objects, lists or strings unknown at the
start of a token, it&rsquo;s hard to reserve the <strong>right</strong> amount of memory.
Instead, memory either needs to be allocated on the fly as JSON
tokens are parsed, or (potentially too big) chunk(s) of memory
needs to be put aside at the start of parsing. The parser can
then use this already allocated memory to store whatever is found
afterwards.</p>

<h2>Verbosity</h2>

<p>JSON data can also become very fluffy. I already mentioned that
serializing strings to JSON might incur some overhead due to escape
sequences.</p>

<p>But there&rsquo;s more things like this: each boolean value requires 4
(<code>true</code>) or 5 (<code>false</code>) bytes respectively. Repeating object member
names need to be stored repeatedly, as JSON does not provide string
interning or similar mechanisms.</p>

<h2>Data types</h2>

<p>Apart from that, the JSON type system is limited. There is only one
type to represent numbers. Different types for representing numbers
of different value ranges are (intentionally) missing. For example,
one might miss 64 bit integer data types or arbitrary precision
numbers. A date type (for calendar dates and times) is often missed, too.</p>

<p>And yes, binary data cannot be represented in JSON without converting
them into a JSON string first. This may require base64-encoding or
something similar.</p>

<p>In general, the available data types in JSON are very limited, and the
format by itself is not extensible. Extending JSON with own type information
will either create ill-formed JSON (read: <em>non-portable</em>) or would
introduce special meaning members that other programs and tools won&rsquo;t
understand (read: <em>non-portable</em>).</p>

<h1>Why still use JSON?</h1>

<p>So what are the reasons to still stick with JSON?
From my point of view, there are still a few good reasons to do so:</p>

<h2>Simplicity</h2>

<p>The <a href="http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf">JSON specification</a>
fits on five pages (including images). It is simple and intuitive.</p>

<p>Additionally, JSON-encoded data is instantly comprehensible. There is
simply no need to look up the meanings of binary magic values in format
specifications. It is also very easy to spot errors in ill-formed JSON.</p>

<p>In my eyes, looking at JSON data during a debugging session is much
easier than looking at binary data (and I do look at binary data sometimes).</p>

<h2>Flexibility</h2>

<p>JSON requires no schema to be defined for data. This is good, as it allows to
get something done earlier. Schemas also tend to change over time, and this
can become a problem with other formats that have schemas. With schema-less JSON,
a schema change becomes a no-brainer &ndash; just change the data inside the JSON
and you&rsquo;re done. No need to maintain a separate schema.</p>

<p>The schema-relaxed approach of JSON also plays quite well with languages that
are loosely typed or allow runtime modifications of data structures. Most
scripting languages are in this category.</p>

<h2>Language support</h2>

<p>JSON is supported in almost every environment. Support for JSON is
sometimes built into languages directly (JavaScript) or the languages come
with built-in serialization and deserialization functions (e.g. PHP).
Just go and use it.</p>

<p>For any other language without built-in support for JSON, it won&rsquo;t be hard to find
a robust implementation for JSON serialization/deserialization.</p>

<p>In the ArangoDB server, we use a lot of JavaScript code ourselves. Users
can also extend the server functionality with JavaScript. Guess what happens
when a JSON request is sent to the server and its payload is handed to a
JavaScript-based action handler in the server? Yes, we&rsquo;ll take the request
body and create JavaScript objects from it. This is as simple as it can be,
because we have native JSON support in JavaScript, our server-side programming
language.</p>

<p>We also encourage users to use ArangoDB as a back end for their JavaScript-based
front ends. Especially when running in a browser, using JSON as the interchange
format inside AJAX* requests makes sense. You don&rsquo;t want to load serialization/deserialization
libraries that handle binary format into front ends for various reasons.</p>

<p>Many tools, including browsers, also support inspecting JSON data or can
import or export JSON-encoded data.</p>

<p>*Pop quiz: does anyone remember what was the meaning of the &ldquo;X&rdquo; in AJAX??</p>

<h1>Alternatives</h1>

<p>As I have tried to outline above, I think JSON has both strengths and
weaknesses. Is there an alternative format that is superior? I am listing
a few candidate formats below, and try to assess them quickly.</p>

<p>One thing that they all have in common is that they are not as much supported
by programming languages and tools as JSON is at the moment. For most of the
alternative formats, you would have to install some library in the environment
of your choice. XML is already available in many environments by default, with
the notable exception of JavaScript.</p>

<p>Even if a format is well supported by most programming languages, there are
other tools that should handle the format, too.</p>

<p>If there aren&rsquo;t any tools that allow converting existing data into the format,
then this is a severe limitation. Browsers, for example, are important tools.
Most of the alternative formats cannot be inspected easily with a browser,
which makes debugging data transfers from browser-based applications hard.</p>

<p>Additionally, one should consider how much example datasets are available.
I think at the moment it&rsquo;s much more likely that you&rsquo;ll find a JSON-encoded
dump of Wikipedia somewhere on the Internet than in one of the alternative
formats.</p>

<h2>Proprietary format</h2>

<p>An alternative to using JSON would be to create and our own binary format.
We could use a protocol tailored to our needs, and make it very very
efficient. The disadvantages of using a proprietary format are
that it is nowhere supported, so writing clients for ArangoDB in
another language becomes much harder for ourselves and for third-party
contributors. Effectively, we would need to write an adapter for
our binary protocol for each environment we want to have ArangoDB
used in.</p>

<p>This sounds like it would take a lot of time and keep us from doing
other things.</p>

<h2>XML</h2>

<p>It&rsquo;s human-readable, understandable, has a good standard type system
and is extensible. But if you thought that JSON is already inefficient
and verbose, try using XML and have fun. A colleague of mine even
claimed that XML is not human-readable due to its chattyness.</p>

<p>XML also hasn&rsquo;t been adopted much in the JavaScript community, and we
need to find a format that plays nicely with JavaScript.</p>

<h2>Smile</h2>

<p>There is also the <a href="http://wiki.fasterxml.com/SmileFormat">Smile</a> format.
Its goals are to provide an efficient alternative to JSON. It looks
good, but it does not seem to be used much outside of <a href="http://wiki.fasterxml.com/JacksonHome">Jackson</a>.
As mentioned earlier, we need a format that is supported in a variety of
environments.</p>

<h2>BSON</h2>

<p>Then there is <a href="http://bsonspec.org/">BSON</a>, made popular by MongoDB.
We had a look at it. It is not as space-efficient as it could be, but
it makes memory allocation very easy and allows for fast packing and
unpacking. It is not so good when values inside the structure need to
be updated. There are BSON-libraries for several languages</p>

<p>Still, it is a binary format. Using it for communication in the ArangoDB
cases includes using it from arbitrary JavaScript programs (including
applications run in a browser), using it in AJAX calls etc. This sounds
a bit like debugging hell.</p>

<h2>Msgpack</h2>

<p><a href="http://msgpack.org/">Msgpack</a> so far looks like the most-promising
alternative. It seems to become available in more and more programming
language environments. The format also seems to be relatively efficient.</p>

<p>A major drawback is that as a binary format, it will still be hard to debug.
Tool support is also not that great yet. Using Msgpack with a browser also
sounds like fun. I&rsquo;d like if tools like Firebug could display Msgpack
packet internals.</p>

<h2>Protocol buffers</h2>

<p>Two years ago, we also experimented with <a href="https://code.google.com/p/protobuf/">Protocol buffers</a>.
Protocol buffers require to set up a schema for the data first, and
then provide efficient means to serialize data from the wire into
programming-language objects.</p>

<p>The problem is that there are no fixed schemas in a document database
like ArangoDB. Users can structure their documents as they like. Each
document can have a completely different structure.</p>

<p>We ended up defining a schema for something JSON-like inside Protocol
buffers, and it did not make much sense in our use case.</p>

<h1>Conclusion</h1>

<p>There are alternative formats out there that address some of the issues
that JSON has from my point of view. However, none of the other formats
is yet that widely supported and easy to use as JSON.</p>

<p>This may change over time.</p>

<p>For our use case, it looks like Msgpack could fit quite well, but
probably only as a second, alternative interface for highest-efficiency
data transfers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How ArangoDB's Write-ahead Log Works]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/06/how-arangodbs-write-ahead-log-works/"/>
    <updated>2014-08-06T21:14:00+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/06/how-arangodbs-write-ahead-log-works</id>
    <content type="html"><![CDATA[<p>Since version 2.2, ArangoDB stores all data-modification operations in its
<em>write-ahead log</em> (abbreviated <em>WAL</em>). The introduction of the WAL massively
changed how data are stored in ArangoDB.</p>

<!-- more -->


<h1>What&rsquo;s in the WAL?</h1>

<p>The WAL contains data of all data-modification operations that were executed in
the ArangoDB server instance. Operations are written to the WAL in the order of
execution. The following types of operations are logged to the WAL:</p>

<ul>
<li>creating, updating, replacing or removing documents</li>
<li>creating, modifying or dropping collections and their indexes</li>
<li>creating or dropping databases</li>
</ul>


<p>The WAL is used for all databases of an ArangoDB server. Database ids are stored
in the WAL in order to tell data from different databases apart.</p>

<h1>Recovery using the WAL</h1>

<p>Should the ArangoDB server crash, it will replay its write-ahead
logs at restart. Replaying the logs will make the server recover the same
state of data as before the crash.</p>

<p>Any document-modification operations might belong to a transaction. Transaction
data are also stored in the write-ahead log, allowing the recovery of committed
transactions and preventing the recovery of aborted or unfinished transactions.</p>

<p>Let&rsquo;s assume the following operations are executed in an ArangoDB server in this
order&hellip;
<code>
Seq#  |  Operation type       |  Transaction#  |  Context
------+-----------------------+----------------+---------------------------------------
   1  |  start transaction    |           773  |  database "_system"
   2  |  insert document      |           773  |  collection "test", key "foo"
   3  |  start transaction    |           774  |  database "_system"
   4  |  insert document      |           774  |  collection "mycollection", key "bar"
   5  |  start transaction    |           775  |  database "_system"
   6  |  update document      |           775  |  collection "boom", key "test"
   7  |  abort transaction    |           774  |  -                
   8  |  remove document      |           773  |  collection "test", key "baz"
   9  |  commit transaction   |           773  |  -     
</code>
&hellip;and then the server goes down due to a power outage.</p>

<p>On server restart, the WAL contents will be replayed, so the server will redo the
above operations. It will find out that operations #2 and #8 belong to transaction #773.
Transaction #773 was already committed, so all of its operations must and will be
recovered.</p>

<p>Further it will find out that operation #4 belongs to transaction #774, which was
aborted by the user. Therefore, this operation will not be replayed but ignored.</p>

<p>Finally, it will find operation #6 belongs to transaction #775. For this transaction,
there is neither an abort nor a commit operation in the log. Because the transaction
was never committed, all of its operations are not replayed at restart and the server
will behave as if the transaction never happened.</p>

<h1>WAL and replication</h1>

<p>A side-effect of having a write-ahead log is that it can also be used for replication.
When a slave server fetches the latest changes from the master, the master can simply
read the operations from its WAL. Data in the WAL are self-contained, meaning the
master can efficiently compile the list of changes using only the WAL and without
performing lookups elsewhere.</p>

<p>The WAL is there and will be used anyway, enabling any ArangoDB server to be used as
a replication master without any configuration. Previous versions of ArangoDB (without
the WAL) required setting up an extra component for replication logging. This
requirement is now gone.</p>

<h1>Organization of the WAL</h1>

<p>The WAL is actually a collection of logfiles. Logfiles are named <code>logfile-xxxx.db</code>
(with xxxx being the logfile&rsquo;s id). Logfiles with lower ids are older than logfiles
with higher ids. By default, the logfiles reside in the <em>journals</em> sub-directory of
ArangoDB&rsquo;s database directory.</p>

<p>At any point in time, one of the logfiles will be the <em>active</em> logfile. ArangoDB will
write all data-modifications to the active logfile. Writing is append-only, meaning
ArangoDB will never overwrite existing logfile data. To ensure logfile integrity,
a CRC32 checksum is calculated for each logfile entry. This checksum is validated when
a logfile is replayed. When there is a checksum mismatch, this indicates a disk error
or an incompletely written operation &ndash; in both cases it won&rsquo;t be safe to recover and
replay the operation.</p>

<p>If an operation can&rsquo;t be written into the active logfile due to lack of space, the
active logfile will be closed and a new logfile will become the active logfile.</p>

<p>A background thread will open new logfiles before the current active one is fully
filled up. This is done to ensure that no waiting is required when there is a switch
of the active logfile.</p>

<p>By default, each logfile has a size of 32 MB, allowing lots of operations to be stored
in it. If you want to adjust the default size, the option <code>--wal.logfile-size</code> is for you.</p>

<h1>Logfile synchronization</h1>

<p>Writes to logfiles are synchronized to disk automatically in a configurable interval
(the option to look for is <code>--wal.sync-interval</code>). To get immediate synchronization
of operations, operations can be run with the <code>waitForSync</code> attribute set to <code>true</code>,
or on collections with the <code>waitForSync</code> attribute being set.</p>

<p>For example, the following operations will have been synchronized to disk when the
operations return:</p>

<p><code>``js
// operation will be synchronized because the</code>waitForSync` attribute
// is set on operation level
db.test.save({ foo: &ldquo;bar&rdquo; }, { waitForSync: true });</p>

<p>// operation will be synchronized because the <code>waitForSync</code> attribute
// is set on collection level
db.mycollection.properties({ waitForSync: true });
db.mycollection.save({ foo: &ldquo;bar&rdquo; });
```</p>

<p>When no immediate synchronization has been requested, ArangoDB will have a background
thread periodically call <code>msync</code> for not-yet synchronized logfile regions. Multiple
operations are synchronized together because they reside in adjacent memory regions.
That means automatic synchronization can get away with far less calls to <code>msync</code> than
there are operations.</p>

<h1>Storage overhead</h1>

<p>Documents stored in the WAL (as part of an insert, update or replace operation) are
stored in a format that contains the document values plus the document&rsquo;s shape.
This allows reading a document fully from a WAL entry without looking up shape
information elsewhere, making it faster and also more reliable.</p>

<p>Storing shape information in the WAL has a storage space overhead though. The overhead
should not matter much if a logfile contains a lot of documents with identical shapes.
ArangoDB will make sure each shape is only stored once per WAL logfile. This has
turned out to be a rather good solution: it reduces WAL storage space requirements
greatly, and still is reliable and fast, as shape lookups are local to the current
WAL logfile only.</p>

<p>The overhead of storing shape information in the WAL will matter most when documents
have completely different shapes. In this case, no shape information will ever be
re-used. While this may happen in benchmarks with synthetic data, we found that in
reality there are often lots of identically-structured documents and thus a lot of
potential for re-using shapes.</p>

<p>Note that storing shape information in the WAL can be turned off to reduce overhead.
ArangoDB provides the option <code>--wal.suppress-shape-information</code> for this purpose.
When set to <code>true</code>, no shape information will be written to the WAL. Note that by
default, the option is set to <code>false</code> and that the option shouldn&rsquo;t be changed if
the server is to be used as a replication master. If documents aren&rsquo;t too heterogenous,
setting the option to <code>true</code> won&rsquo;t help much. It will help a lot if all documents
that are stored have different shapes (which we consider unrealistic, but we still
provide the option to reduce overhead in this case).</p>

<h1>WAL cleanup</h1>

<p>WAL logfiles that are completely filled are subject to garbage collection. WAL
garbage collection is performed by a separate garbage collector thread. The thread
will copy over the still-relevant operations into the collection datafiles.
After that, indexes will be adjusted to point to the new storage locations.
Documents that have become obsolete due to later changes will not be copied from
the WAL into the collection datafiles at all.</p>

<p>Garbage-collected logfiles are deleted by ArangoDB automatically if there exist
more of these &ldquo;historic&rdquo; logfiles than configured. The number of historic logfiles
to keep before deletion is configured using the option <code>--wal.historic-logfiles</code>.</p>

<p>If no replication is to be used, there is no need to keep any historic logfiles.
They have no purpose but to provide a history of recent changes. The more history
there is on a master server, the longer is the period for which slave servers can
request changes for.
How much history is needed depends on how reliable the network connection between
a replication slave and the master is. If the network connection is known to fail
periodically, it may be wise to keep a few historic logfiles on the master, so the
slave can catch up from the point it stopped when the network connection is
re-established.</p>

<p>If network connections are reliable or no replication is to be used at all, the
number of historic logfiles can be set to a low value to save disk space.</p>

<h1>Side-effects of the WAL</h1>

<p>The WAL can be used for replication, removing the requirement to explicitly turn
on the separate logging of operations for replication purposes. This is a clear
improvement over previous versions of ArangoDB.</p>

<p>The introduction of the WAL also caused a few other minor changes:</p>

<p>While documents are stored in a WAL logfile, their sizes won&rsquo;t be included in
the output of the <code>figures</code> method of the collection. When a WAL logfile gets
garbage-collected, documents will physically be moved into the collection logfiles
and the figures will be updated.</p>

<p>Note that the output of the <code>count</code> method is not affected by whether a document
is stored in the WAL or in a collection logfile.</p>

<p>Another side-effect of storing operations in the WAL first is that no collection
logfiles will be created when the first document is inserted. So there will be
collections with documents but without any logfiles, at least temporarily until
the WAL garbage collection kicks in and will transfer data from the WAL to the
collection logfiles.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Schema Handling in ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2014/06/03/schema-handling-in-arangodb/"/>
    <updated>2014-06-03T22:57:58+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/06/03/schema-handling-in-arangodb</id>
    <content type="html"><![CDATA[<h1>Schemas vs. schema-free</h1>

<p>In a relational database, all rows in a table have the same structure.
The structure is saved once for the table, and the invidiual rows only
contain the row&rsquo;s values. This is an efficient approach if all records
have the exact same structure, i.e. the same attributes (same names and
same data types).</p>

<!-- more -->


<p><code>plain Example records
firstName (varchar)  |  lastName (varchar)  |  status (varchar)
---------------------+----------------------+------------------
"fred"               |  "foxx"              |  "active"
"john"               |  "doe"               |  "inactive"
</code></p>

<p>This is not a good fit if the data structure changes. In this case, an
<code>ALTER TABLE</code> command would need to be issued in the relational database,
converting all existing rows into the new structure. This is an expensive
operation because it normally requires rewriting all existing rows.</p>

<p>The situation becomes really difficult when there is no definite structure
for a table &ndash; if rows shall have a dynamic or variable structure, then
it can be quite hard to define a sensible relational table schema!</p>

<p>This is where NoSQL databases enter the game &ndash; mostly they don&rsquo;t require
defining a schema for a &ldquo;table&rdquo; at all. Instead, each individual record
will not only contain its data values, but also its own schema. This means
much higher flexibility as every record can its completely own data
structure.</p>

<p>This flexibility has a disadvantage though: storing schemas in individual
records requires more storage space than storing the schema only once for
the complete table. This is especially true if most (or even all) records
in the table do have the same structure. A lot of storage space can be
wasted while storing the same structure information again and again and again&hellip;</p>

<h1>Schemas in ArangoDB</h1>

<p>ArangoDB tries to be different in this respect: on the one hand it is a
schema-free database and thus allows <em>flexible storage</em>. All documents in a
collection (the ArangoDB lingo for <em>record</em> and <em>table</em>) can have the same
or totally different structures. We leave this choice up to the user.</p>

<p>On the other hand, ArangoDB will exploit the similarities in document
structures to <em>save storage space</em>. It will detect identical document
schemas, and only save each unique schema once. This process is called
<strong>shaping</strong> in ArangoDB.</p>

<h2>Shaping</h2>

<p>We optimized ArangoDB for this use case because we found that in reality, the
documents in a collection will either have absolutely the same schema, or
there will only be a few different schemas in use.</p>

<p>From the user perspective there are no schemas in ArangoDB: there is no way
to create or alter the schema of a collection at all. Instead, ArangoDB
will use the attribute names and data types contained in the JSON data of
each document. All of this happens automatically.</p>

<p>For each new document in a collection, ArangoDB will first figure out the
schema. It will then check if it has already processed a document with the
same schema. If yes, then there is no need to save the schema information
again. Instead, the new document will only contain a pointer to an already
existing schema. This does not require much storage space.</p>

<p>If ArangoDB figures out that it has not yet processed a document with the
same schema, it will store the document schema once, and store a pointer
to the schema in the new document. This is a slightly more expensive
operation, but it pays out when there are multiple documents in a
collection with the same structure.</p>

<p>When ArangoDB looks at document schemas, it takes into account the attribute
names and the attribute value data types contained in a document. All
attribute names and data types in a document make the so-called <em>shape</em>.</p>

<p>Each shape is only stored once for each collection. Any attribute name used
in a collection is also stored only once, and then reused from any shape that
contains the attribute name.</p>

<h2>Examples</h2>

<p>The following documents do have different values but still their schemas are
identical:</p>

<p><code>json
{ "name" : { "first" : "fred", "last" : "foxx" }, "status" : "active" }
{ "name" : { "first" : "john", "last" : "doe" }, "status" : "inactive" }
</code></p>

<p>Both documents contain attributes named <code>name</code> and <code>status</code>. <code>name</code> is an
array with two sub-attributes <code>first</code> and <code>last</code>, which are both strings.
<code>status</code> also has string values in both documents.</p>

<p>ArangoDB will save this schema only once in a so-called <em>shape</em>. The documents
will store their own data values plus a pointer to this (same) shape.</p>

<p>The next two documents have different, yet unknown schemas. ArangoDB will
therefore store these two schemas in two new shapes:</p>

<p><code>json
{ "firstName" : "jack", "lastName" : "black", "status" : "inactive" }
{ "name" : "username", "status" : "unknown" }
</code></p>

<p>We would end up with three diferent <em>shapes</em> for the four documents. This
might not sound impressive, but if more documents are saved with one of the
existing shapes, then storing each shape just once might really pay out.</p>

<h2>A note on attribute names</h2>

<p>Even though the latter two example documents had unique schemas, we saw in
the examples that attribute names were already repeating. For example, all
documents shown so far had an attribute named <code>status</code>, and some also
had a <code>name</code> attribute.</p>

<p>ArangoDB figures out when attribute names repeat, and it will not store the
same attribute name more than once in a collection. Given that many
documents in a collection use a fixed set of repeating attribute names,
this approach can lead to considerable storage space reductions.</p>

<p>As an aside, reusing attribute name information allows using descriptive
(read: long) attribute names in ArangoDB with very low storage overhead.</p>

<p>For example, in ArangoDB it will not cost much extra space to use long
attribute names like these in lots of documents:
<code>json
{ "firstNameOfTheUser" : "jack", "lastNameOfTheUser" : "black" }
</code></p>

<p>Each unique attribute name is only stored once per collection. In ArangoDB
there is thus no need to <em>artifically</em> shorten the attribute names in data
like it sometimes is done in other schema-free databases to save storage
space:
<code>json
{ "fn" : "jack", "ln" : "black" }
</code>
This artifical crippling of the attribute names makes the meaning of the
attributes quite unclear and should be avoided. As mentioned, it is not
necessary to do this in ArangoDB as it will save attribute names separate
from attribute values, and repeating attribute names are not stored
repeatedly.</p>
]]></content>
  </entry>
  
</feed>
