<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Schemas | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/schemas/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-04T15:28:52+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting Up Test Data]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data/"/>
    <updated>2014-11-04T22:14:21+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data</id>
    <content type="html"><![CDATA[<p>Today I was asked to look at code that was supposed to read data
from a MySQL data source, process it and then import it into ArangoDB.</p>

<p>To run and debug the code I had to have some MySQL data source. So I
thought I&rsquo;d quickly set up a simple example table with a few rows.
It turned out that this took more time than what I had expected.</p>

<p>Maybe I&rsquo;m spoilt by JavaScript-enabled, schema-free databases where
creating such test setups is so much easier.</p>

<!-- more -->


<p>I worked with MySQL databases in production for 10+ years and spent
much time working with the mysql client. I always liked MySQL, but in
the past few years, I was driven away from it and lost contact.
Instead, I got sucked into the NoSQL landscape and enjoy it pretty much.</p>

<p>Getting back to the original problem: I needed some MySQL table with a
few thousand rows for a test. It turned out I didn&rsquo;t even have MySQL
installed on my computer, so I needed to install it first.</p>

<p>After setting up the MySQL server, I created a table <code>examples</code> for
storing my test data:</p>

<p><code>sql
CREATE DATABASE test;
USE test;
CREATE TABLE examples (attribute1 VARCHAR(20), attribute2 VARCHAR(20));
</code></p>

<p>Not really the black belt of schema design, but good enough for a quick
test.</p>

<p>Now the table needed some rows. 100,000 rows should be enough. I wrote
some bash script to create them as there is no sane way to do this with
the MySQL client alone:</p>

<p><code>``bash
for i in</code>seq 1 100000`
  do</p>

<pre><code>echo "INSERT INTO examples VALUES (\"test$i\", \"test$i\");" &gt;&gt; import.sql 
</code></pre>

<p>  done
```</p>

<p>Time to import the data!</p>

<p><code>bash
mysql -u user test &lt; import.sql
</code></p>

<p>At first I was a bit surprised this command did not return instantly. I let it
run for about a minute, and then began checking the import progress with a second mysql
client. It turned out only very few records had been imported, and the import
script continued to create only around 30-35 records per second.</p>

<p>Seems I had forgotten that I am working with a No-NoSQL database, with full
ACID semantics for everything. My import file contained 100,000 <code>INSERT</code>
statements, so I was asking to perform 100,000 transactions and fsync operations.
That import would have taken forever with my slow HDD!</p>

<p>I quickly changed the InnoDB setting to make it commit only about once per second:
<code>
mysql&gt; SET GLOBAL innodb_flush_log_at_trx_commit = 2;
Query OK, 0 rows affected (0.00 sec)
</code></p>

<p>Now the import finished in 7 seconds.</p>

<p>I finally got the data in MySQL, but overall it took me about 10 minutes to get
it done. Probably a bit less if I still were an active user of MySQL and had
remembered the default behavior right from the start.</p>

<p>Still, my feeling is that it takes too much time to get something so simple
done.</p>

<p>I don&rsquo;t blame the database for trying to commit all 100,000 single-row
<code>INSERT</code> operations and fsync them to disk. It simply cannot know if the data
are important or just throw-away test records.</p>

<p>But there are other reasons: I had to write a bash script to produce the
test data, as there is no sane way to do this with the MySQL client alone.
Writing bash scripts is fine, and in general I like it, but I don&rsquo;t want to
do it for a dead-simple test setup.</p>

<p>And by the way, what if it turns out that I need to generate slightly more
complex test data? In the MySQL case I probably would have resorted to sed
or awk or would have thrown away my bash script and had rewritten it in some
other language. So I would have wasted even more time.</p>

<p>I personally prefer the ability to use a scripting language for such tasks.
JavaScript is ubiquituous these days, and I want to use it in a database&rsquo;s
command-line client.</p>

<p>For example, here&rsquo;s how the test setup would look like in the ArangoShell:
<code>js
db._create("examples");
for (i = 0; i &lt; 100000; ++i) {
  db.examples.save({ attribute1: "test" + i, attribute2: "test" + i });
}
</code>
I find this much easier to use: it allows to do everything in one place,
removing the need to write another script that prepares a data file or an
SQL file first.</p>

<p>As a bonus, using a programming language is much more flexible and powerful.
If I needed to generate slightly more complex test data, I can just do it,
adjust the JavaScript code and re-run it.</p>

<p>Even more annoying to me is that I needed to provide a schema for the
table first. I could have got away with declaring all text fields as
<code>VARCHAR(255)</code> or <code>TEXT</code> so I can at least ignore string
length restrictions. But I still need to type in the table schema
once, even if it feels completely useless for this particular use case.</p>

<p>It would get even more annoying if during my test I noticed I needed more
or other columns. Then I would need to adjust the table schema using <code>ALTER TABLE</code>
or adjust the <code>CREATE TABLE</code> statement and run it again, keeping me
away from the original task.</p>

<p>Maybe using schema-free databases for too long has spoilt me, but I much
more prefer starting quickly and without a schema. I know the data that
I am going to load will have a structure and will be somewhat self-describing,
so the database can still figure out what the individual parts of a record are.</p>

<p><em>On a side-note: should you be a fan of using query languages, the same
test setup can also be achieved by running the following AQL query from
the ArangoShell</em>:
```js
db._query(&ldquo;FOR i IN 1..100000 LET value = CONCAT(&lsquo;test&rsquo;, i) &rdquo; +</p>

<pre><code>      "INSERT { attribute1: value, attribute2: value } INTO examples");
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why JSON?]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/14/why-json/"/>
    <updated>2014-08-14T22:27:27+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/14/why-json</id>
    <content type="html"><![CDATA[<h1>To JSON or not?</h1>

<p>We&rsquo;re often asked why ArangoDB uses <a href="http://json.org">JSON</a> as its
data-interchange format for transferring documents from clients to
the database server and back. This is often accompanied by the
question if we could use <code>&lt;insert fancy format here&gt;</code> instead.</p>

<p>In the following article, I&rsquo;ll try to outline the reasons for why
we picked JSON as the interchange format, and why we still use it.</p>

<p>I&rsquo;ll start with a discussion of the pros and cons of JSON, look at
some alternative formats and present my personal opinion on why
using a different format may not provide too much benefit for us,
at least at the moment.</p>

<!-- more -->


<p>This post does not intend to say that any of these formats are better
or worse in general. I think there are applications for all of them.</p>

<p>However, I wanted to look at the different formats with our specific
use case, i.e. a RESTful database, in mind.</p>

<h1>What I don&rsquo;t like about JSON</h1>

<p>JSON is often criticized for its inefficiency and lack of <strong>real</strong>
data types. I&rsquo;ll often criticize it myself.</p>

<p>Following are my personal top 3 pain points.</p>

<h2>Parsing and memory allocation</h2>

<p>I have to admit that parsing JSON is painful from the efficiency
perspective. When the JSON parser encounters a <code>{</code> token,
it will know this is the start of an object, but it has no idea how
many object members will follow and need to be stored with the
object. The same is true for lists (starting with <code>[</code>).</p>

<p>String values are no different: when the parser encounters a <code>"</code>,
the length of the string is still unknown. To determine the length
of the string, one must read until the end of the string, taking
into account escape sequences for special characters, e.g. <code>\/</code>,
<code>\n</code>, <code>\t</code>, <code>\\</code>, but also Unicode escape sequences.</p>

<p>For example, the escaped 36-byte string <code>In K\u00f6ln, it's 15 \u00b0 outside</code>
will be parsed into the 28-byte UTF-8 string <code>In Köln, it's 15 ° outside</code>.</p>

<p>With the overall size of objects, lists or strings unknown at the
start of a token, it&rsquo;s hard to reserve the <strong>right</strong> amount of memory.
Instead, memory either needs to be allocated on the fly as JSON
tokens are parsed, or (potentially too big) chunk(s) of memory
needs to be put aside at the start of parsing. The parser can
then use this already allocated memory to store whatever is found
afterwards.</p>

<h2>Verbosity</h2>

<p>JSON data can also become very fluffy. I already mentioned that
serializing strings to JSON might incur some overhead due to escape
sequences.</p>

<p>But there&rsquo;s more things like this: each boolean value requires 4
(<code>true</code>) or 5 (<code>false</code>) bytes respectively. Repeating object member
names need to be stored repeatedly, as JSON does not provide string
interning or similar mechanisms.</p>

<h2>Data types</h2>

<p>Apart from that, the JSON type system is limited. There is only one
type to represent numbers. Different types for representing numbers
of different value ranges are (intentionally) missing. For example,
one might miss 64 bit integer data types or arbitrary precision
numbers. A date type (for calendar dates and times) is often missed, too.</p>

<p>And yes, binary data cannot be represented in JSON without converting
them into a JSON string first. This may require base64-encoding or
something similar.</p>

<p>In general, the available data types in JSON are very limited, and the
format by itself is not extensible. Extending JSON with own type information
will either create ill-formed JSON (read: <em>non-portable</em>) or would
introduce special meaning members that other programs and tools won&rsquo;t
understand (read: <em>non-portable</em>).</p>

<h1>Why still use JSON?</h1>

<p>So what are the reasons to still stick with JSON?
From my point of view, there are still a few good reasons to do so:</p>

<h2>Simplicity</h2>

<p>The <a href="http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf">JSON specification</a>
fits on five pages (including images). It is simple and intuitive.</p>

<p>Additionally, JSON-encoded data is instantly comprehensible. There is
simply no need to look up the meanings of binary magic values in format
specifications. It is also very easy to spot errors in ill-formed JSON.</p>

<p>In my eyes, looking at JSON data during a debugging session is much
easier than looking at binary data (and I do look at binary data sometimes).</p>

<h2>Flexibility</h2>

<p>JSON requires no schema to be defined for data. This is good, as it allows to
get something done earlier. Schemas also tend to change over time, and this
can become a problem with other formats that have schemas. With schema-less JSON,
a schema change becomes a no-brainer &ndash; just change the data inside the JSON
and you&rsquo;re done. No need to maintain a separate schema.</p>

<p>The schema-relaxed approach of JSON also plays quite well with languages that
are loosely typed or allow runtime modifications of data structures. Most
scripting languages are in this category.</p>

<h2>Language support</h2>

<p>JSON is supported in almost every environment. Support for JSON is
sometimes built into languages directly (JavaScript) or the languages come
with built-in serialization and deserialization functions (e.g. PHP).
Just go and use it.</p>

<p>For any other language without built-in support for JSON, it won&rsquo;t be hard to find
a robust implementation for JSON serialization/deserialization.</p>

<p>In the ArangoDB server, we use a lot of JavaScript code ourselves. Users
can also extend the server functionality with JavaScript. Guess what happens
when a JSON request is sent to the server and its payload is handed to a
JavaScript-based action handler in the server? Yes, we&rsquo;ll take the request
body and create JavaScript objects from it. This is as simple as it can be,
because we have native JSON support in JavaScript, our server-side programming
language.</p>

<p>We also encourage users to use ArangoDB as a back end for their JavaScript-based
front ends. Especially when running in a browser, using JSON as the interchange
format inside AJAX* requests makes sense. You don&rsquo;t want to load serialization/deserialization
libraries that handle binary format into front ends for various reasons.</p>

<p>Many tools, including browsers, also support inspecting JSON data or can
import or export JSON-encoded data.</p>

<p>*Pop quiz: does anyone remember what was the meaning of the &ldquo;X&rdquo; in AJAX??</p>

<h1>Alternatives</h1>

<p>As I have tried to outline above, I think JSON has both strengths and
weaknesses. Is there an alternative format that is superior? I am listing
a few candidate formats below, and try to assess them quickly.</p>

<p>One thing that they all have in common is that they are not as much supported
by programming languages and tools as JSON is at the moment. For most of the
alternative formats, you would have to install some library in the environment
of your choice. XML is already available in many environments by default, with
the notable exception of JavaScript.</p>

<p>Even if a format is well supported by most programming languages, there are
other tools that should handle the format, too.</p>

<p>If there aren&rsquo;t any tools that allow converting existing data into the format,
then this is a severe limitation. Browsers, for example, are important tools.
Most of the alternative formats cannot be inspected easily with a browser,
which makes debugging data transfers from browser-based applications hard.</p>

<p>Additionally, one should consider how much example datasets are available.
I think at the moment it&rsquo;s much more likely that you&rsquo;ll find a JSON-encoded
dump of Wikipedia somewhere on the Internet than in one of the alternative
formats.</p>

<h2>Proprietary format</h2>

<p>An alternative to using JSON would be to create and our own binary format.
We could use a protocol tailored to our needs, and make it very very
efficient. The disadvantages of using a proprietary format are
that it is nowhere supported, so writing clients for ArangoDB in
another language becomes much harder for ourselves and for third-party
contributors. Effectively, we would need to write an adapter for
our binary protocol for each environment we want to have ArangoDB
used in.</p>

<p>This sounds like it would take a lot of time and keep us from doing
other things.</p>

<h2>XML</h2>

<p>It&rsquo;s human-readable, understandable, has a good standard type system
and is extensible. But if you thought that JSON is already inefficient
and verbose, try using XML and have fun. A colleague of mine even
claimed that XML is not human-readable due to its chattyness.</p>

<p>XML also hasn&rsquo;t been adopted much in the JavaScript community, and we
need to find a format that plays nicely with JavaScript.</p>

<h2>Smile</h2>

<p>There is also the <a href="http://wiki.fasterxml.com/SmileFormat">Smile</a> format.
Its goals are to provide an efficient alternative to JSON. It looks
good, but it does not seem to be used much outside of <a href="http://wiki.fasterxml.com/JacksonHome">Jackson</a>.
As mentioned earlier, we need a format that is supported in a variety of
environments.</p>

<h2>BSON</h2>

<p>Then there is <a href="http://bsonspec.org/">BSON</a>, made popular by MongoDB.
We had a look at it. It is not as space-efficient as it could be, but
it makes memory allocation very easy and allows for fast packing and
unpacking. It is not so good when values inside the structure need to
be updated. There are BSON-libraries for several languages</p>

<p>Still, it is a binary format. Using it for communication in the ArangoDB
cases includes using it from arbitrary JavaScript programs (including
applications run in a browser), using it in AJAX calls etc. This sounds
a bit like debugging hell.</p>

<h2>Msgpack</h2>

<p><a href="http://msgpack.org/">Msgpack</a> so far looks like the most-promising
alternative. It seems to become available in more and more programming
language environments. The format also seems to be relatively efficient.</p>

<p>A major drawback is that as a binary format, it will still be hard to debug.
Tool support is also not that great yet. Using Msgpack with a browser also
sounds like fun. I&rsquo;d like if tools like Firebug could display Msgpack
packet internals.</p>

<h2>Protocol buffers</h2>

<p>Two years ago, we also experimented with <a href="https://code.google.com/p/protobuf/">Protocol buffers</a>.
Protocol buffers require to set up a schema for the data first, and
then provide efficient means to serialize data from the wire into
programming-language objects.</p>

<p>The problem is that there are no fixed schemas in a document database
like ArangoDB. Users can structure their documents as they like. Each
document can have a completely different structure.</p>

<p>We ended up defining a schema for something JSON-like inside Protocol
buffers, and it did not make much sense in our use case.</p>

<h1>Conclusion</h1>

<p>There are alternative formats out there that address some of the issues
that JSON has from my point of view. However, none of the other formats
is yet that widely supported and easy to use as JSON.</p>

<p>This may change over time.</p>

<p>For our use case, it looks like Msgpack could fit quite well, but
probably only as a second, alternative interface for highest-efficiency
data transfers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Schema Handling in ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2014/06/03/schema-handling-in-arangodb/"/>
    <updated>2014-06-03T22:57:58+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/06/03/schema-handling-in-arangodb</id>
    <content type="html"><![CDATA[<h1>Schemas vs. schema-free</h1>

<p>In a relational database, all rows in a table have the same structure.
The structure is saved once for the table, and the invidiual rows only
contain the row&rsquo;s values. This is an efficient approach if all records
have the exact same structure, i.e. the same attributes (same names and
same data types).</p>

<!-- more -->


<p><code>plain Example records
firstName (varchar)  |  lastName (varchar)  |  status (varchar)
---------------------+----------------------+------------------
"fred"               |  "foxx"              |  "active"
"john"               |  "doe"               |  "inactive"
</code></p>

<p>This is not a good fit if the data structure changes. In this case, an
<code>ALTER TABLE</code> command would need to be issued in the relational database,
converting all existing rows into the new structure. This is an expensive
operation because it normally requires rewriting all existing rows.</p>

<p>The situation becomes really difficult when there is no definite structure
for a table &ndash; if rows shall have a dynamic or variable structure, then
it can be quite hard to define a sensible relational table schema!</p>

<p>This is where NoSQL databases enter the game &ndash; mostly they don&rsquo;t require
defining a schema for a &ldquo;table&rdquo; at all. Instead, each individual record
will not only contain its data values, but also its own schema. This means
much higher flexibility as every record can its completely own data
structure.</p>

<p>This flexibility has a disadvantage though: storing schemas in individual
records requires more storage space than storing the schema only once for
the complete table. This is especially true if most (or even all) records
in the table do have the same structure. A lot of storage space can be
wasted while storing the same structure information again and again and again&hellip;</p>

<h1>Schemas in ArangoDB</h1>

<p>ArangoDB tries to be different in this respect: on the one hand it is a
schema-free database and thus allows <em>flexible storage</em>. All documents in a
collection (the ArangoDB lingo for <em>record</em> and <em>table</em>) can have the same
or totally different structures. We leave this choice up to the user.</p>

<p>On the other hand, ArangoDB will exploit the similarities in document
structures to <em>save storage space</em>. It will detect identical document
schemas, and only save each unique schema once. This process is called
<strong>shaping</strong> in ArangoDB.</p>

<h2>Shaping</h2>

<p>We optimized ArangoDB for this use case because we found that in reality, the
documents in a collection will either have absolutely the same schema, or
there will only be a few different schemas in use.</p>

<p>From the user perspective there are no schemas in ArangoDB: there is no way
to create or alter the schema of a collection at all. Instead, ArangoDB
will use the attribute names and data types contained in the JSON data of
each document. All of this happens automatically.</p>

<p>For each new document in a collection, ArangoDB will first figure out the
schema. It will then check if it has already processed a document with the
same schema. If yes, then there is no need to save the schema information
again. Instead, the new document will only contain a pointer to an already
existing schema. This does not require much storage space.</p>

<p>If ArangoDB figures out that it has not yet processed a document with the
same schema, it will store the document schema once, and store a pointer
to the schema in the new document. This is a slightly more expensive
operation, but it pays out when there are multiple documents in a
collection with the same structure.</p>

<p>When ArangoDB looks at document schemas, it takes into account the attribute
names and the attribute value data types contained in a document. All
attribute names and data types in a document make the so-called <em>shape</em>.</p>

<p>Each shape is only stored once for each collection. Any attribute name used
in a collection is also stored only once, and then reused from any shape that
contains the attribute name.</p>

<h2>Examples</h2>

<p>The following documents do have different values but still their schemas are
identical:</p>

<p><code>json
{ "name" : { "first" : "fred", "last" : "foxx" }, "status" : "active" }
{ "name" : { "first" : "john", "last" : "doe" }, "status" : "inactive" }
</code></p>

<p>Both documents contain attributes named <code>name</code> and <code>status</code>. <code>name</code> is an
array with two sub-attributes <code>first</code> and <code>last</code>, which are both strings.
<code>status</code> also has string values in both documents.</p>

<p>ArangoDB will save this schema only once in a so-called <em>shape</em>. The documents
will store their own data values plus a pointer to this (same) shape.</p>

<p>The next two documents have different, yet unknown schemas. ArangoDB will
therefore store these two schemas in two new shapes:</p>

<p><code>json
{ "firstName" : "jack", "lastName" : "black", "status" : "inactive" }
{ "name" : "username", "status" : "unknown" }
</code></p>

<p>We would end up with three diferent <em>shapes</em> for the four documents. This
might not sound impressive, but if more documents are saved with one of the
existing shapes, then storing each shape just once might really pay out.</p>

<h2>A note on attribute names</h2>

<p>Even though the latter two example documents had unique schemas, we saw in
the examples that attribute names were already repeating. For example, all
documents shown so far had an attribute named <code>status</code>, and some also
had a <code>name</code> attribute.</p>

<p>ArangoDB figures out when attribute names repeat, and it will not store the
same attribute name more than once in a collection. Given that many
documents in a collection use a fixed set of repeating attribute names,
this approach can lead to considerable storage space reductions.</p>

<p>As an aside, reusing attribute name information allows using descriptive
(read: long) attribute names in ArangoDB with very low storage overhead.</p>

<p>For example, in ArangoDB it will not cost much extra space to use long
attribute names like these in lots of documents:
<code>json
{ "firstNameOfTheUser" : "jack", "lastNameOfTheUser" : "black" }
</code></p>

<p>Each unique attribute name is only stored once per collection. In ArangoDB
there is thus no need to <em>artifically</em> shorten the attribute names in data
like it sometimes is done in other schema-free databases to save storage
space:
<code>json
{ "fn" : "jack", "ln" : "black" }
</code>
This artifical crippling of the attribute names makes the meaning of the
attributes quite unclear and should be avoided. As mentioned, it is not
necessary to do this in ArangoDB as it will save attribute names separate
from attribute values, and repeating attribute names are not stored
repeatedly.</p>
]]></content>
  </entry>
  
</feed>
