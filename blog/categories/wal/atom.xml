<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: WAL | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/wal/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-07T16:29:56+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking ArangoDB's Write-ahead Log]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/07/benchmarking-arangodbs-write-ahead-log/"/>
    <updated>2014-08-07T01:27:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/07/benchmarking-arangodbs-write-ahead-log</id>
    <content type="html"><![CDATA[<h1>Motivation</h1>

<p>One of the major changes in ArangoDB 2.2 was the introduction of the
<em>write-ahead log</em> (abbreviated <em>WAL</em>).</p>

<p>The introduction of the WAL changed how documents are stored internally in
ArangoDB. A lot of things have been changed for it under the hood, and it has
been a lot of work to implement it.</p>

<p>During the implementation, we refactored some code parts and made them
considerably faster. From these changes we expected a positive effect on the
database performance. But due to the fact that shape information is now also
saved in the write-ahead log, there may also be some negative effect.</p>

<p>We developers were of course very interested in seeing the net effects, so
we ran some tests for a few use cases. We compared ArangoDB 2.1.2 (still without
the WAL) with ArangoDB 2.2.1 (with the WAL). The results are interesting.</p>

<!-- more -->


<h1>Test setup</h1>

<p>To get a broad overview of performance changes, we ran a few different test cases:</p>

<ul>
<li><strong>document</strong>: inserts a document</li>
<li><strong>crud</strong>: inserts a document, fetches it, updates it, and deletes it</li>
<li><strong>crud-append</strong>: inserts a document, fetches it, updates it, and fetches it again</li>
<li><strong>multi-collection</strong>: transactionally save two documents in two collections</li>
<li><strong>random-shapes</strong>: save documents with completely different structures/shapes</li>
</ul>


<p>All tests were run with the <code>arangob</code> benchmark tool, with various concurrency levels,
complexity settings and repeated several times. <code>arangob</code> was invoked like this:</p>

<p><code>
arangob                         \
  --test-case $case             \
  --request $requests           \
  --concurrency $concurrency    \
  --complexity $complexity
</code></p>

<p>Both ArangoDB servers and <code>arangob</code> were located on the same server. Only one
ArangoDB server was active during each test run, and the other was shut down so
it didn&rsquo;t compete for system resources.</p>

<h1>Test results</h1>

<p>Following are the results for the different test cases.</p>

<p>In the result tables, the columns have the following meanings:</p>

<ul>
<li><strong>Complexity</strong>: the number of attributes for the test documents, used as parameter
<code>--complexity</code> for <code>arangob</code></li>
<li><strong>Requests</strong>: the number of operations executed, used as parameter <code>--requests</code>
for <code>arangob</code></li>
<li><strong>Concurrency</strong>: the number of client threads started by <code>arangob</code>, used a parameter
<code>concurrency</code> for <code>arangob</code></li>
<li><strong>Time 2.1</strong>: test execution time (in seconds) for ArangoDB 2.1</li>
<li><strong>Time 2.2</strong>: test execution time (in seconds) for ArangoDB 2.2</li>
<li><strong>T2.1/T2.2</strong>: relative performance of ArangoDB 2.1 compared to ArangoDB 2.2. Values
less than one indicate that ArangoDB 2.1 was faster than ArangoDB 2.2. Values
greater than one indicate that ArangoDB 2.1 was slower than ArangoDB 2.2. A
value of one means that both versions had equal speed.</li>
</ul>


<p>Please note that the absolute execution times aren&rsquo;t too interesting in the
results shown here. We haven&rsquo;t used the most powerful server on earth for running
these tests. We were most interested in how ArangoDB 2.2 compared to
ArangoDB 2.1.</p>

<h2>Inserting documents</h2>

<p>The <code>document</code> test appends new documents (with an identical structure) to a
collection. Here are the results for inserting a million documents with 100
attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1       97.452       94.839         1.02    
   100       1000000               2       61.146       53.928         1.13   
   100       1000000               4       54.368       31.379         1.73  
</code></pre>

<p>```</p>

<p>Following are the results for 100,000 documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000               1       86.883       80.669         1.07 
  1000        100000               2       59.381       48.599         1.22 
  1000        100000               4       53.784       27.494         1.95 
</code></pre>

<p>```</p>

<p>The results for 5,000 documents with 10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1       43.991       40.577         1.08 
 10000          5000               2       31.403       26.117         1.20 
 10000          5000               4       28.713       20.044         1.43 
</code></pre>

<p>```</p>

<p>As we can see in the results above, ArangoDB 2.2 is faster than ArangoDB 2.1 for
all tested configurations. The difference is negligible if the test client is
single-threaded (one insert thread), but ArangoDB 2.2 is considerably faster than
ArangoDB 2.1 with more concurrent clients.</p>

<h2>CRUD operations (I)</h2>

<p>The <code>crud</code> test case inserts a document, fetches it, updates it, fetches it again
and finally deletes it. Executing 1 million crud operations on documents with 100
attributes each results in the following execution times:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1       66.856       67.072         0.99       
   100       1000000               2       47.907       47.043         1.01        
   100       1000000               4       42.089       42.047         1.00         
</code></pre>

<p>```</p>

<p>Running 100,000 crud operations on documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000               1      160.228      158.312         1.01 
  1000        100000               2      147.413      147.990         0.99  
  1000        100000               4      143.575      143.233         1.00   
</code></pre>

<p>```</p>

<p>And here are the test results for running 5,000 crud operations on documents with
10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1      645.859      643.924         1.00        
 10000          5000               2      640.414      640.767         0.99       
 10000          5000               4      637.438      637.132         1.00      
</code></pre>

<p>```</p>

<p>The results of all these tests show that performance for the tested workload hasn&rsquo;t
changed between ArangoDB 2.1 and 2.2. This is somewhat expected, as the WAL should
not affect the performance of read and delete operations much: reading a document
does not require writing to the WAL at all, and removing a document only requires
writing a very small remove marker to the WAL.</p>

<h2>CRUD operations (II)</h2>

<p>The <code>crud-append</code> test case inserts a document, fetches it, updates it, and fetches
it again. Executing 1 million crud operations on documents with 100 attributes each
results in the following execution times:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1       81.187       79.079         1.02 
   100       1000000               2       59.544       56.934         1.04  
   100       1000000               4       53.845       52.098         1.03   
</code></pre>

<p>```</p>

<p>Running 100,000 crud operations on documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000               1      200.057      196.722         1.01 
  1000        100000               2      182.436      181.633         1.00 
  1000        100000               4      181.233      180.631         1.00 
</code></pre>

<p>```</p>

<p>And here are the test results for running 5,000 crud operations on documents with
10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1      804.781      801.903         1.00 
 10000          5000               2      796.589      797.693         0.99  
 10000          5000               4      796.664      795.823         1.00  
</code></pre>

<p>```</p>

<p>Again, ArangoDB 2.1 and 2.2 are pretty much the same speed for the tested operations.</p>

<h2>Multi-collection write transactions</h2>

<p>The <code>multi-collection</code> test stores two documents in two different collections
transactionally. Here are the results for executing 100,000 transactions for
documents with 100 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100        100000               1     6936.895       30.736       225.69   
   100        100000               2     6946.524       25.577       271.59    
   100        100000               4     7720.682       24.334       317.27     
</code></pre>

<p>```</p>

<p>Executing 10,000 transactions on documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000         10000               1      949.974       21.368        44.45     
  1000         10000               2      953.672       19.322        49.35    
  1000         10000               4      941.645       20.486        45.96   
</code></pre>

<p>```</p>

<p>And finally, the results for executing 500 transactions on documents with 10,000
attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000           500               1       46.776       10.413         4.49 
 10000           500               2       45.102       10.173         4.43 
 10000           500               4       44.172        9.192         4.80  
</code></pre>

<p>```</p>

<p>The above results show that ArangoDB 2.2 is much faster than ArangoDB 2.1 for
executing transactions that write to two collections.</p>

<p>We expected that! Multi-collection (write) transactions in ArangoDB 2.2 require
far less calls to <code>msync</code> than in ArangoDB 2.1. ArangoDB 2.2 can sync operations
of multiple transactions together in one call to <code>msync</code>. ArangoDB 2.1 needed to
synchronize each transaction separately.</p>

<h2>(Fully) heterogenous documents</h2>

<p>The <code>random-shapes</code> test inserts documents that have different structures each.
For each inserted document a new shape will need to be stored. Inserting one million
documents with 100 attributes each results in the following figures:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1      664.006       87.181         7.61        
   100       1000000               2      433.570       60.613         7.15       
   100       1000000               4      313.666       45.327         6.92      
</code></pre>

<p>```
The results show that ArangoDB 2.2 is considerably faster than ArangoDB 2.1 for
these cases &ndash; even with the storage overhead of writing the shapes to the WAL.</p>

<p>Now we inserted 100,000 documents with 1,000 document attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000              1        61.709       66.954         0.92    
  1000        100000              2        39.294       46.458         0.84   
  1000        100000              4        36.866       40.910         0.90  
</code></pre>

<p>```
In these cases, the storage overhead of writing all shapes to the WAL seems to
start to matter, and ArangoDB 2.2 gets slower than ArangoDB 2.1 in this test
case.</p>

<p>The same was true when we inserted 5,000 documents with 10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1       25.337       30.412         0.83    
 10000          5000               2       20.021       22.128         0.90     
 10000          5000               4       14.800       18.872         0.78      
</code></pre>

<p>```</p>

<p>Note that the <code>random-shapes</code> test case is an extreme test case. We do not
consider it realistic that all documents in a collection have completely different
attribute names. Still we included it in our tests because we were sure that this
would be a case in which the overhead of the WAL would be clearly measurable.</p>

<p>Note that there is a way to make ArangoDB 2.2 faster than ArangoDB 2.1 even for
this test case: setting the option <code>--wal.suppress-shape-information</code> to <code>true</code>
will make ArangoDB not write shape information to the WAL, making document write
operations much faster in case all documents have heterogenous structures.</p>

<p>The option won&rsquo;t help much if a shapes repeat a lot. In this case, the WAL overhead
shouldn&rsquo;t matter too much already, or ArangoDB 2.2 should already be faster than 2.1
(as shown in the results above).</p>

<h1>Summary</h1>

<p>The tests revealed that the overhead of the WAL in ArangoDB 2.2 seems to be
negligible for most of the tested workloads. In many cases, ArangoDB 2.2 with
the WAL is even faster than ArangoDB 2.1 that did not have a WAL at all.</p>

<p>There is one notable exception: when documents have fully heterogenous structures,
the overhead of the WAL is measurable and significant. Though we consider this
to be a rather hypothetical case, we have added the configurtion option
<code>--wal.suppress-shape-information</code>. This can be used to turn off storing
shape information in the WAL. This is not safe when the server is to be used
as a replication master, but should work in cases when replication is not used.</p>

<p>The option should not have a big effect if documents are greatly or at least
somewhat homogenous. For these cases, ArangoDB 2.2 with its WAL should already
be as fast as 2.1 (or even faster).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How ArangoDB's Write-ahead Log Works]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/06/how-arangodbs-write-ahead-log-works/"/>
    <updated>2014-08-06T21:14:00+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/06/how-arangodbs-write-ahead-log-works</id>
    <content type="html"><![CDATA[<p>Since version 2.2, ArangoDB stores all data-modification operations in its
<em>write-ahead log</em> (abbreviated <em>WAL</em>). The introduction of the WAL massively
changed how data are stored in ArangoDB.</p>

<!-- more -->


<h1>What&rsquo;s in the WAL?</h1>

<p>The WAL contains data of all data-modification operations that were executed in
the ArangoDB server instance. Operations are written to the WAL in the order of
execution. The following types of operations are logged to the WAL:</p>

<ul>
<li>creating, updating, replacing or removing documents</li>
<li>creating, modifying or dropping collections and their indexes</li>
<li>creating or dropping databases</li>
</ul>


<p>The WAL is used for all databases of an ArangoDB server. Database ids are stored
in the WAL in order to tell data from different databases apart.</p>

<h1>Recovery using the WAL</h1>

<p>Should the ArangoDB server crash, it will replay its write-ahead
logs at restart. Replaying the logs will make the server recover the same
state of data as before the crash.</p>

<p>Any document-modification operations might belong to a transaction. Transaction
data are also stored in the write-ahead log, allowing the recovery of committed
transactions and preventing the recovery of aborted or unfinished transactions.</p>

<p>Let&rsquo;s assume the following operations are executed in an ArangoDB server in this
order&hellip;
<code>
Seq#  |  Operation type       |  Transaction#  |  Context
------+-----------------------+----------------+---------------------------------------
   1  |  start transaction    |           773  |  database "_system"
   2  |  insert document      |           773  |  collection "test", key "foo"
   3  |  start transaction    |           774  |  database "_system"
   4  |  insert document      |           774  |  collection "mycollection", key "bar"
   5  |  start transaction    |           775  |  database "_system"
   6  |  update document      |           775  |  collection "boom", key "test"
   7  |  abort transaction    |           774  |  -                
   8  |  remove document      |           773  |  collection "test", key "baz"
   9  |  commit transaction   |           773  |  -     
</code>
&hellip;and then the server goes down due to a power outage.</p>

<p>On server restart, the WAL contents will be replayed, so the server will redo the
above operations. It will find out that operations #2 and #8 belong to transaction #773.
Transaction #773 was already committed, so all of its operations must and will be
recovered.</p>

<p>Further it will find out that operation #4 belongs to transaction #774, which was
aborted by the user. Therefore, this operation will not be replayed but ignored.</p>

<p>Finally, it will find operation #6 belongs to transaction #775. For this transaction,
there is neither an abort nor a commit operation in the log. Because the transaction
was never committed, all of its operations are not replayed at restart and the server
will behave as if the transaction never happened.</p>

<h1>WAL and replication</h1>

<p>A side-effect of having a write-ahead log is that it can also be used for replication.
When a slave server fetches the latest changes from the master, the master can simply
read the operations from its WAL. Data in the WAL are self-contained, meaning the
master can efficiently compile the list of changes using only the WAL and without
performing lookups elsewhere.</p>

<p>The WAL is there and will be used anyway, enabling any ArangoDB server to be used as
a replication master without any configuration. Previous versions of ArangoDB (without
the WAL) required setting up an extra component for replication logging. This
requirement is now gone.</p>

<h1>Organization of the WAL</h1>

<p>The WAL is actually a collection of logfiles. Logfiles are named <code>logfile-xxxx.db</code>
(with xxxx being the logfile&rsquo;s id). Logfiles with lower ids are older than logfiles
with higher ids. By default, the logfiles reside in the <em>journals</em> sub-directory of
ArangoDB&rsquo;s database directory.</p>

<p>At any point in time, one of the logfiles will be the <em>active</em> logfile. ArangoDB will
write all data-modifications to the active logfile. Writing is append-only, meaning
ArangoDB will never overwrite existing logfile data. To ensure logfile integrity,
a CRC32 checksum is calculated for each logfile entry. This checksum is validated when
a logfile is replayed. When there is a checksum mismatch, this indicates a disk error
or an incompletely written operation &ndash; in both cases it won&rsquo;t be safe to recover and
replay the operation.</p>

<p>If an operation can&rsquo;t be written into the active logfile due to lack of space, the
active logfile will be closed and a new logfile will become the active logfile.</p>

<p>A background thread will open new logfiles before the current active one is fully
filled up. This is done to ensure that no waiting is required when there is a switch
of the active logfile.</p>

<p>By default, each logfile has a size of 32 MB, allowing lots of operations to be stored
in it. If you want to adjust the default size, the option <code>--wal.logfile-size</code> is for you.</p>

<h1>Logfile synchronization</h1>

<p>Writes to logfiles are synchronized to disk automatically in a configurable interval
(the option to look for is <code>--wal.sync-interval</code>). To get immediate synchronization
of operations, operations can be run with the <code>waitForSync</code> attribute set to <code>true</code>,
or on collections with the <code>waitForSync</code> attribute being set.</p>

<p>For example, the following operations will have been synchronized to disk when the
operations return:</p>

<p><code>``js
// operation will be synchronized because the</code>waitForSync` attribute
// is set on operation level
db.test.save({ foo: &ldquo;bar&rdquo; }, { waitForSync: true });</p>

<p>// operation will be synchronized because the <code>waitForSync</code> attribute
// is set on collection level
db.mycollection.properties({ waitForSync: true });
db.mycollection.save({ foo: &ldquo;bar&rdquo; });
```</p>

<p>When no immediate synchronization has been requested, ArangoDB will have a background
thread periodically call <code>msync</code> for not-yet synchronized logfile regions. Multiple
operations are synchronized together because they reside in adjacent memory regions.
That means automatic synchronization can get away with far less calls to <code>msync</code> than
there are operations.</p>

<h1>Storage overhead</h1>

<p>Documents stored in the WAL (as part of an insert, update or replace operation) are
stored in a format that contains the document values plus the document&rsquo;s shape.
This allows reading a document fully from a WAL entry without looking up shape
information elsewhere, making it faster and also more reliable.</p>

<p>Storing shape information in the WAL has a storage space overhead though. The overhead
should not matter much if a logfile contains a lot of documents with identical shapes.
ArangoDB will make sure each shape is only stored once per WAL logfile. This has
turned out to be a rather good solution: it reduces WAL storage space requirements
greatly, and still is reliable and fast, as shape lookups are local to the current
WAL logfile only.</p>

<p>The overhead of storing shape information in the WAL will matter most when documents
have completely different shapes. In this case, no shape information will ever be
re-used. While this may happen in benchmarks with synthetic data, we found that in
reality there are often lots of identically-structured documents and thus a lot of
potential for re-using shapes.</p>

<p>Note that storing shape information in the WAL can be turned off to reduce overhead.
ArangoDB provides the option <code>--wal.suppress-shape-information</code> for this purpose.
When set to <code>true</code>, no shape information will be written to the WAL. Note that by
default, the option is set to <code>false</code> and that the option shouldn&rsquo;t be changed if
the server is to be used as a replication master. If documents aren&rsquo;t too heterogenous,
setting the option to <code>true</code> won&rsquo;t help much. It will help a lot if all documents
that are stored have different shapes (which we consider unrealistic, but we still
provide the option to reduce overhead in this case).</p>

<h1>WAL cleanup</h1>

<p>WAL logfiles that are completely filled are subject to garbage collection. WAL
garbage collection is performed by a separate garbage collector thread. The thread
will copy over the still-relevant operations into the collection datafiles.
After that, indexes will be adjusted to point to the new storage locations.
Documents that have become obsolete due to later changes will not be copied from
the WAL into the collection datafiles at all.</p>

<p>Garbage-collected logfiles are deleted by ArangoDB automatically if there exist
more of these &ldquo;historic&rdquo; logfiles than configured. The number of historic logfiles
to keep before deletion is configured using the option <code>--wal.historic-logfiles</code>.</p>

<p>If no replication is to be used, there is no need to keep any historic logfiles.
They have no purpose but to provide a history of recent changes. The more history
there is on a master server, the longer is the period for which slave servers can
request changes for.
How much history is needed depends on how reliable the network connection between
a replication slave and the master is. If the network connection is known to fail
periodically, it may be wise to keep a few historic logfiles on the master, so the
slave can catch up from the point it stopped when the network connection is
re-established.</p>

<p>If network connections are reliable or no replication is to be used at all, the
number of historic logfiles can be set to a low value to save disk space.</p>

<h1>Side-effects of the WAL</h1>

<p>The WAL can be used for replication, removing the requirement to explicitly turn
on the separate logging of operations for replication purposes. This is a clear
improvement over previous versions of ArangoDB.</p>

<p>The introduction of the WAL also caused a few other minor changes:</p>

<p>While documents are stored in a WAL logfile, their sizes won&rsquo;t be included in
the output of the <code>figures</code> method of the collection. When a WAL logfile gets
garbage-collected, documents will physically be moved into the collection logfiles
and the figures will be updated.</p>

<p>Note that the output of the <code>count</code> method is not affected by whether a document
is stored in the WAL or in a collection logfile.</p>

<p>Another side-effect of storing operations in the WAL first is that no collection
logfiles will be created when the first document is inserted. So there will be
collections with documents but without any logfiles, at least temporarily until
the WAL garbage collection kicks in and will transfer data from the WAL to the
collection logfiles.</p>
]]></content>
  </entry>
  
</feed>
