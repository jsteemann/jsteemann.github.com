<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-20T22:06:55+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Great AQL Shootout: ArangoDB 2.5 vs 2.6]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/20/the-great-aql-shootout-arangodb-25-vs-26/"/>
    <updated>2015-05-20T18:04:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/20/the-great-aql-shootout-arangodb-25-vs-26</id>
    <content type="html"><![CDATA[<p>We are currently preparing ArangoDB 2.6 for release. A lot of work has been put into this release,
and I really hope we can ship a first 2.6 release soon.</p>

<p>To keep you hanging on in the meantime, I put together some performance tests results from 2.6.
The tests I ran compared AQL query execution times in 2.6 and 2.5.</p>

<p>The results look quite promising: <strong>2.6 outperformed 2.5 for all tested queries</strong>, mostly by
factors of 2 to 5. A few dedicated AQL features in the tests got boosted even more, resulting in
query execution time reductions of 90 % and more.
Finally, the tests also revealed a dedicated case for which 2.6 provides a several hundredfold speedup.</p>

<p>Also good news is that not a single of the test queries ran slower in 2.6 than in 2.5.</p>

<!-- more -->


<h1>What was tested?</h1>

<p>The tests execute several read-only AQL queries on datasets of different sizes and measure the
query execution times. The tests were conducted in both ArangoDB 2.5 (2.5.4, the current stable version)
and 2.6 (2.6.0-alpha2, the upcoming version), so the results of the two ArangoDB versions can be compared.</p>

<p>Though the tests do not cover every possible type of AQL operation, feature and function, they still do
cover a wide range of features, e.g. lookups, joins, COLLECT operations, sorting, subqueries,
and some AQL functions. Overall, the test suite contains 33 different cases.</p>

<p>All queries were run on datasets of three different sizes to validate that the results are relevant
for datasets of various sizes. The dataset sizes are 10,000 documents, 100,000 documents, and 1,000,000
documents. Each query was repeated a few times so outliers in execution time can be identified.</p>

<p>There is full disclosure of the test methodology and the test script below, so anyone interested
can repeat the tests locally and verify the results.</p>

<h1>Test results</h1>

<p>The combined test results from 2.5 and 2.6 can be found in this
<a href="/downloads/code/arango-25-26-shootout-results.pdf">PDF file</a>.
There is also an <em>ods</em> version of the same file <a href="/downloads/code/arango-25-26-shootout-results.ods">here</a>.
A description of the columns and test cases used in these files can be found further below.</p>

<p>For the detail-loving folks, here are the raw results for both versions in isolation:
<a href="/downloads/code/arango-25-26-shootout-results-25.txt">2.5</a>,
<a href="/downloads/code/arango-25-26-shootout-results-26.txt">2.6</a>.</p>

<p>The results show that ArangoDB 2.6 was consistently faster for <strong>all</strong> AQL queries included in the
tests.</p>

<p>The queries that improved most in 2.6 over 2.5 include:</p>

<ul>
<li><code>FILTER</code> conditions: simple <code>FILTER</code> conditions as used in the tests are 3 to 5 times faster</li>
<li>simple joins using the primary index (<code>_key</code> attribute), hash index or skiplist index are
2 to 3.5 times faster</li>
<li>sorting on a string attribute is 2.5 to 3 times faster</li>
<li>extracting the <code>_key</code> or other top-level attributes from documents is 4 to 5 times faster</li>
<li><code>COLLECT</code> statements: simple <code>COLLECT</code> statements like the ones in the tests are 7 to 15 times
faster</li>
<li>looking up documents using <code>IN</code> lists with a substantial amount of values contained in the <code>IN</code>
list is 250 to 700 times faster</li>
</ul>


<p>The one thing that did not change much when comparing 2.6 with 2.5 is iterating over a collection
and returning all its documents unmodified. The speedups observed for this type of query are between
18 and 25 %, which is the lowest speedup measured by the tests. Still 18 to 25 % seem okay
as a free take-away.</p>

<p>Speedups were observed for all three test dataset sizes alike. In some cases, the speedups
varied a bit with the dataset sizes, but it was still in the same ballpark for all three datasets.
The conclusion is thus that the speedups did not depend much on the dataset sizes.</p>

<h1>Reasons for speedups</h1>

<p>There are several reasons why the 2.6 performance is better than in previous versions. The main
reason is that we spent much time optimizing some of the crtical AQL code paths. Then we also
worked on optimizations for specific features, which are used by some of the tested queries.</p>

<p>If you&rsquo;re interested in the details, here they are:</p>

<ul>
<li><a href="/blog/2015/04/22/collecting-with-a-hash-table/">COLLECTing with a hash table</a></li>
<li><a href="/blog/2015/04/23/aql-functions-improvements/">AQL functions improvements</a></li>
<li><a href="/blog/2015/05/04/subquery-optimizations/">Subquery optimizations</a></li>
<li><a href="/blog/2015/05/04/return-value-optimization-for-aql/">Return value optimization for AQL</a></li>
<li><a href="/blog/2015/05/07/in-list-improvements/">IN-list improvements</a></li>
</ul>


<p>Additionally, UTF-8 string comparisons were boosted by the upgrade from ICU 52 to ICU 54. The
latter version contains a rewritten and much faster UTF-8-aware strcmp, which we heavily rely on.</p>

<h1>Test methodology</h1>

<p>Each query was run five times on each dataset, so execution time outliers can be identified. The
results contain the minimum, maximum and average execution times for each query.</p>

<p>Queries were run in isolation on an otherwise idle server. The queries were all run inside the
server, so there was no HTTP/network traffic involved for shipping query results (note: this
was also <a href="/blog/2015/04/01/improvements-for-the-cursor-api/">vastly improved in 2.6</a> but this is
not the subject of this post).</p>

<p>All tests were run on my local machine, which has 4 cores, 8 CPUs (though the number of CPUs will
not matter for any of the tests), 12 GB of physical memory, a Linux 3.16 kernel and an Ubuntu 15
OS. All datasets fit into the main memory, so tests were not I/O-bound.</p>

<p>The ArangoDB versions tested were 2.5.4 and 2.6.0-alpha2. Both versions were hand-compiled with
g++ 4.9.1 with options <code>CC='gcc' CXX='g++' CFLAGS='-O3 -Wall' CXXFLAGS='-O3 -Wall'</code>.</p>

<p>The complete test script, including the setup of the test data, is contained in
<a href="/downloads/code/arango-25-26-shootout-script.js">this file</a>. It can be run inside <em>arangod</em> by
typing the following in the server console:</p>

<p><code>js running the tests inside arangod
require("internal").load("/path/to/arango-25-26-shootout-script.js");
</code></p>

<p>Note that this needs an <em>arangod</em> started with option <code>--console</code>. Also note that running the
script will only test the current <em>arangod</em> instance, so the script needs to be run once in a
2.5 instance and once in 2.6.</p>

<p>Running the script will set up the test collections, run all queries on them (you will need some
patience for this) and finally print a table like the following:</p>

<p>```plain excerpt from test results</p>

<h2>test name                     | collection  |    runs |     min (s) |     max (s) |     avg (s)</h2>

<p>collect-number                | 10k         |       5 |      0.0760 |      0.1638 |      0.0943
collect-number                | 100k        |       5 |      0.8697 |      0.8966 |      0.8803
collect-number                | 1000k       |       5 |     10.4320 |     10.6597 |     10.5314
collect-string                | 10k         |       5 |      0.1211 |      0.1319 |      0.1250
collect-string                | 100k        |       5 |      1.5406 |      1.5974 |      1.5641
collect-string                | 1000k       |       5 |     19.0708 |     19.0966 |     19.0825
collect-count-number          | 10k         |       5 |      0.0763 |      0.0792 |      0.0778
```</p>

<p>These result columns have the following meanings:</p>

<ul>
<li><em>test name</em>: name of test</li>
<li><em>collection</em>: name of collection. <em>10k</em> is a collection with 10,000 documents, <em>100k</em> contains
 100,000 documents, and <em>1000k</em> contains 1,000,000 documents.</li>
<li><em>runs</em>: number of times the query was run</li>
<li><em>min (s)</em>: minimum query execution time (in seconds)</li>
<li><em>max (s)</em>: maximum query execution time (in seconds)</li>
<li><em>avg (s)</em>: average query execution time (in seconds)</li>
</ul>


<h1>Test data</h1>

<p>The test datasets for the three collections are filled with artifical data. Test documents are
created like this:</p>

<p><code>js test document creation
collection.insert({
  _key: "test" + i,
  value1: i,
  value2: "test" + i,
  value3: i,
  value4: "test" + i,
  value5: i,
  value6: "test" + i,
  value7: i % g,
  value8: "test" + (i % g)
});
</code></p>

<p>Each document has a <code>_key</code> attribute and 8 other attributes, <code>value1</code> to <code>value8</code>.</p>

<p><code>value1</code>, <code>value3</code>, <code>value5</code> and <code>value7</code> are numeric attributes, the other attributes contain
string values. The attributes <code>value1</code> to <code>value6</code> contain unique values. The attributes <code>value7</code>
and <code>value8</code> contain repeating values. They are used for <code>COLLECT</code> queries.</p>

<p><code>value1</code> and <code>value2</code> are each indexed with a hash index. <code>value3</code> and <code>value4</code> are each indexed with
a skiplist index. <code>value5</code> to <code>value8</code> are not indexed. This way queries can be run on the same values,
but with different indexes and even without indexes.</p>

<h1>Test cases</h1>

<p>The test cases cover the following queries:</p>

<ul>
<li><em>collect-number</em> and <em>collect-string</em>: run <code>COLLECT</code> on a repeating attribute, which is either
numeric or a string</li>
<li><em>collect-count-number</em> and <em>collect-count-string</em>: ditto, but also calculate the group lengths
using <code>WITH COUNT INTO</code></li>
<li><em>subquery</em>: run a single-document subquery for each document of the original collection</li>
<li><em>concat</em>: for each document in the collection, concat the document <code>_key</code> attribute with another
 document attribute using <code>CONCAT()</code></li>
<li><em>merge</em>: for each document in the collection, merge the document with another object using <code>MERGE()</code></li>
<li><em>keep</em>: for each document in the collection, remove all but a few named attributes from it using
<code>KEEP()</code></li>
<li><em>unset</em>: for each document in the collection, remove a few named attributes from it using <code>UNSET()</code></li>
<li><em>min-number</em> and <em>min-string</em>: return the minimum value of a specific attribute from all documents in
 the collection, which is either numeric or a string. This uses <code>MIN()</code></li>
<li><em>max-number</em> and <em>max-string</em>: ditto, but using <code>MAX()</code></li>
<li><em>sort-number</em> and <em>sort-string</em>: sort all documents in the collection by a non-indexed attribute,
 which is either numeric or a string</li>
<li><em>filter-number</em> and <em>filter-string</em>: filter all documents in the collection using a non-indexed attribute,
 which is either numeric or a string</li>
<li><em>extract-doc</em>: return all documents in the collection unmodified</li>
<li><em>extract-key</em>: return the <code>_key</code> attribute of all documents in the collection</li>
<li><em>extract-number</em> and <em>extract-string</em>: return an attribute from all documents in the collection,
 which is either numeric or a string</li>
<li><em>join-key</em>: for each document in the collection, perform a join on the <code>_key</code> attribute on the collection
 itself (i.e. <code>FOR c1 IN @@c FOR c2 IN @@c FILTER c1._key == c2._key RETURN c1</code>)</li>
<li><em>join-id</em>: ditto, but perform the join using the <code>_id</code> attribute</li>
<li><em>join-hash-number</em> and <em>join-hash-string</em>: ditto, but join using a hash index on a numeric or string
 attribute</li>
<li><em>join-skiplist-number</em> and <em>join-skiplist-string</em>: ditto, but join using a skiplist index on a numeric or
 string attribute</li>
<li><em>lookup-key</em>, <em>lookup-hash-number</em>, <em>lookup-hash-string</em>, <em>lookup-skiplist-number</em>, <em>lookup-skiplist-string</em>:
 compile an IN-list of 10,000 lookup values and search these 10,000 documents in the collection using
 either the primary index (<code>_key</code> attribute), a hash index or a skiplist index. The latter two are tested
 on numeric and string attributes.</li>
</ul>


<p>Further implementation details can be checked in the <a href="/downloads/code/arango-25-26-shootout-script.js">test script</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bulk Document Lookups]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/bulk-document-lookups/"/>
    <updated>2015-05-07T17:48:21+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/bulk-document-lookups</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.6 comes with a specialized API for bulk document lookups.</p>

<p>The new API allows fetching multiple documents from the server using a single
request, making bulk document retrieval more efficient than when using
one request per document to fetch.</p>

<!-- more -->


<p>The straight-forward implementation of a client application that needs to
fetch several documents from an ArangoDB server looks like this:</p>

<p>```js fetching multiple documents from the server one by one
// list of document keys
var keys = [ &ldquo;foo&rdquo;, &ldquo;bar&rdquo;, &ldquo;baz&rdquo;, &hellip;];</p>

<p>// sequentially fetch all documents from the server
var results = [ ];
for (var i = 0; i &lt; keys.length; ++i) {
  results.push(db.test.document(keys[i]));
}
// now all documents are contained in variable &lsquo;results&rsquo;
```</p>

<p>This works fine but causes excessive HTTP communication between the client
application and the server when many documents need to be fetched. In fact,
the above code will issue as many HTTP requests as there are documents to fetch.</p>

<p>From the performance point of view, it would be much better to reduce the
number of HTTP requests, and retrieve multiple documents from the server in
one go, using a single request.</p>

<p>This is where the new document lookup function comes into play. Provided the
documents keys are known, all the client application needs to do is to call the
collection&rsquo;s <code>lookupByKeys</code> method:</p>

<p>```js bulk method: fetching multiple documents at once
// list of document keys
var keys = [ &ldquo;foo&rdquo;, &ldquo;bar&rdquo;, &ldquo;baz&rdquo;, &hellip;];</p>

<p>var results = db.test.lookupByKeys(keys);
// now all documents are contained in variable &lsquo;results&rsquo;
```</p>

<p>Following is a comparison of the execution times for the two different methds.
All test runs were conducted in the same ArangoDB 2.6 instance. The tests were
run from the ArangoShell. The ArangoShell and the ArangoDB server were located on
the same physical host.</p>

<p>```plain comparing single document requests and bulk requests</p>

<h2>Number of keys     Single documents        Bulk</h2>

<pre><code>     1,000               0.24 s      0.04 s
    10,000               1.23 s      0.31 s
   100,000              10.89 s      2.13 s
</code></pre>

<p>```</p>

<p>As can be seen, the bulk method can provide a substantial speedup in case lots
of documents need to be fetched by their keys at once. The actual speedups might be
even higher when using a remote ArangoDB server instead of a localhost connection.</p>

<p>In 2.6 there is currently an ArangoShell implementation for bulk document lookups.
Other drivers will follow.</p>

<p>Additionally, the server-side REST API method for bulk document lookups can be
invoked directly via HTTP as follows:</p>

<p><code>plain invoking bulk document lookups via HTTP
curl                                                  \
  -X PUT                                              \
  http://127.0.0.1:8529/_api/simple/lookup-by-keys    \
  --data '{"collection":"test","keys":["foo","bar","baz"]}'
</code></p>

<p>Restrictions: the bulk document API works only with document keys, not document ids.
Additionally, it works on a single collection at a time and cannot be leveraged to fetch
documents from multiple collections. Still, a client application can group document keys
by collection beforehand and send one bulk request per involved collection. Finally,
trying to fetch a document using a non-existing key will not produce an error with the
bulk API. Using the one-by-one method, trying to fetch a non-existing document will throw
an exception.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IN-list Improvements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/in-list-improvements/"/>
    <updated>2015-05-07T16:46:30+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/in-list-improvements</id>
    <content type="html"><![CDATA[<p>We have worked on many AQL optimizations for ArangoDB 2.6.</p>

<p>As a side effect of one of these optimizations, some cases involving the handling
of large IN-lists have become much faster than before. Large IN-lists are normally
used when comparing attribute or index values against some big array of lookup values
or keys provided by the application.</p>

<!-- more -->


<p>Let&rsquo;s quickly create and populate a collection named <code>keys</code> so that we can use some
IN-list queries on it later on:</p>

<p>```js setting up example data
db._create(&ldquo;keys&rdquo;);</p>

<p>// insert 100k documents with some defined keys into the collection
for (var i = 0; i &lt; 100000; ++i) {
  db.keys.insert({ _key: &ldquo;test&rdquo; + i });
}
```</p>

<p>And here is a query to all find documents with one of the provided keys <code>test0</code> to <code>test999</code>.
The IN-list here contains 1,000 values:</p>

<p><code>js using an IN-list with 1,000 values
var keys = [ ];
var n = 1000;
for (var i = 0; i &lt; n; ++i) {
  keys.push("test" + i);
}
db._query("FOR doc IN keys FILTER doc._key IN @keys RETURN doc", { keys: keys }); });
</code></p>

<p>When invoked from the ArangoShell, this takes around 0.6 seconds to complete with ArangoDB 2.5.</p>

<p>Increasing the length of the IN-list from 1,000 to 5,000 values makes this run in around 15 seconds.
With an IN-list of 10,000 values, this already takes more than 60 seconds to complete in 2.5.</p>

<p>Obviously longer IN-lists weren&rsquo;t handled terribly efficiently in 2.5, and should be avoided there
if possible.</p>

<p>I am glad this has been fixed in 2.6. Following is a comparison of the above query for different
IN-list sizes, run on both ArangoDB 2.5 and 2.6.</p>

<p>```plain 2.5 and 2.6 with different IN-list sizes</p>

<h1>of IN-list values    Execution time (2.5)   Execution time (2.6)</h1>

<hr />

<pre><code>          1,000                  0.67 s                 0.03 s
          5,000                 15.34 s                 0.12 s
         10,000                 63.48 s                 0.20 s
         50,000                   n/a                   0.81 s
        100,000                   n/a                   1.60 s
</code></pre>

<p>```</p>

<p>Looks like 2.6 handles longer IN-lists way better than 2.5! The above figures suggest that execution
times now scale about linearly with the number of IN-list values. This also leads to reductions in query
execution times of 90 % and more percent.</p>

<p>Please note that longer IN-lists will still make a the query run longer than when
using shorter IN-lists. This is expected because longer IN-lists require more comparisons to
be made and will lead (in the above example) to more documents being returned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Subquery Optimizations]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/subquery-optimizations/"/>
    <updated>2015-05-04T13:26:00+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/subquery-optimizations</id>
    <content type="html"><![CDATA[<p>This is another post demonstrating some of the AQL query performance improvements
that can be expected in ArangoDB 2.6. Specifically, this post is about an optimization
for subqueries. AQL queries with multiple subqueries will likely benefit from it.</p>

<!-- more -->


<p>The following example queries use the same <code>users</code> collection with 100,000 documents
that was used in the <a href="/blog/2015/05/04/return-value-optimization-for-aql/">previous post about return value optimizations</a>.
Again, the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>.</p>

<p>We&rsquo;ll start with a query that uses a single subquery extracting all users from the
collection into a flat result array:</p>

<p><code>plain example query with single subquery
RETURN [
  (FOR u IN users RETURN u)
]
</code></p>

<p>This query is equally fast with ArangoDB 2.5 and 2.6, no changes here.</p>

<p>Let&rsquo;s ramp it up to using two subqueries, one for users with a <code>gender</code> attribute value
of <code>male</code>, and one for users with <code>gender</code> attribute value <code>female</code>. No indexes were used
for the extraction in 2.5 nor 2.6:</p>

<p><code>plain example query with two subqueries
RETURN [
  (FOR u IN users FILTER u.gender == 'male' RETURN u),
  (FOR u IN users FILTER u.gender == 'female' RETURN u)
]
</code></p>

<p>The query takes 16.6 seconds to execute in 2.5, but only 2.95 seconds with ArangoDB 2.6.
This 80 % reduction in execution time is due to ArangoDB 2.6 being a bit smarter about
subqueries than 2.5 is.</p>

<p>In the above query, the two subqueries are independent, so not only can they be executed in
any order, but they also do not rely on each other&rsquo;s results. ArangoDB 2.6 will detect that
and avoid copying variables and intermediate results into subqueries if they are actually not
needed there. 2.5 copied all variables into subqueries unconditionally, even if variables
were not needed there.</p>

<p>In 2.6, any AQL query with multiple subqueries will benefit from this optimization. The
performance improvements will be greater if subqueries late in the execution pipeline have a lot of
intermediate results created in front of them, but do not rely on these intermediate results.</p>

<p>Another nice example for a 2.6 speedup is extracting a single attribute per subquery, as is done
for the <code>name</code> attribute in the following query:</p>

<p><code>plain extracting a single attribute in two subqueries
RETURN [
  (FOR u IN users FILTER u.gender == 'male' RETURN u.name),
  (FOR u IN users FILTER u.gender == 'female' RETURN u.name)
]
</code></p>

<p>This takes 42 seconds to execute in 2.5, and only 0.86 seconds in 2.6. This is a more than
95 % reduction in execution time. It is caused by a mix of factors, one of them again being
the subquery optimization that avoids copying unneeded intermediate results.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Return Value Optimization for AQL]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql/"/>
    <updated>2015-05-04T10:32:43+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql</id>
    <content type="html"><![CDATA[<p>While in search for further AQL query optimizations last week, we found that intermediate AQL
query results were copied one time too often in some cases. Precisely, the data that a query&rsquo;s
<code>ReturnNode</code> will return to the caller was copied into the <code>ReturnNode</code>&rsquo;s own register. With
<code>ReturnNode</code>s never modifying their input data, this demanded for something that is called
<em>return-value optimization</em> in compilers.</p>

<p>2.6 will now optimize away these copies in many cases, and this post shows which performance
benefits can be expected due to the optimization.</p>

<!-- more -->


<p>The effect of the optimization can be demonstrated easily with a few simple AQL queries.
Let&rsquo;s start with a query that simply returns all 100,000 documents from a collection <code>users</code>:
<code>FOR u IN users RETURN u</code> (the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>).</p>

<p>This query&rsquo;s execution plan is already straight-forward and simple:</p>

<p><img src="/downloads/screenshots/return.png"></p>

<p>So where&rsquo;s the problem?</p>

<p>The <code>ReturnNode</code> in this query (and other queries too) will copy its input data into its own output
register, only to finally hand the results to the query&rsquo;s caller. This copying is most often unnecessary
as the <code>ReturnNode</code> will not modify its input. So the idea was to get rid of the copying action and
tell the query&rsquo;s calling code in which (now different) register to look for the results.</p>

<p>Optimizing away the copying inside the <code>ReturnNode</code> made the query run faster already.
The same query now returns the 100,000 documents in 0.24 to 0.26 seconds, compared to 0.27 to 0.30 s
before applying the optimization.</p>

<p>Returning just an attribute of each document shows about the same improvement rates. The execution
times of the query <code>FOR u IN users RETURN u._key</code> drop to between 0.13 and 0.14 seconds with the
optimization, from initially between 0.15 and 0.17 seconds.</p>

<p>Another example query, <code>FOR i IN 1..1000000 RETURN i</code>, now runs in 0.58 to 0.61 seconds with
the optimization, compared to between 0.77 and 0.81 seconds without it.</p>

<p>These absolute figures may not look overly impressive, but they indicate relative improvements of
between 10 and 25 %, which is quite nice. This is effectively saved CPU time that can now be used
for something more productive.</p>

<p>Of course the performance improvements may not be that high for every imaginable AQL query.
Though the optimization may be active in most AQL queries, its effect will only be measurable
for queries that return a significant number of documents/values. Otherwise the share of the
<code>ReturnNode</code>&rsquo;s work in the query&rsquo;s overall computations may be too low to have any effect.
Additionally, the more work a query spends in performing other operations (e.g. filtering,
sorting, collecting), the less relevant will be the overall effect of the optimized <code>ReturnNode</code>.
Finally, when query results need to be shipped from the server to the client over a network,
the relative effect of the optimization may diminish further.</p>

<p>So your mileage may vary. But the optimization will not do any harm, and together with some
other query optimizations already finished for 2.6 it will contribute to many AQL queries
running faster than before.</p>

<p>AQL queries will benefit from the optimization automatically in ArangoDB 2.6, without requiring
any adjustments to the query string, the server configuration etc. The optimizer will automatically
apply the optimization for the main-level <code>ReturnNode</code> of every AQL query.</p>

<p>On a side note: the optimization will not be shown in the list of applied optimizer rules for the
query. This is because the optimization is performed in some different place in the query
executor, after applying the optimizer rules.</p>
]]></content>
  </entry>
  
</feed>
