<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-07T17:33:22+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[IN-list Improvements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/in-list-improvements/"/>
    <updated>2015-05-07T16:46:30+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/in-list-improvements</id>
    <content type="html"><![CDATA[<p>We have worked on many AQL optimizations for ArangoDB 2.6.</p>

<p>As a side effect of one of these optimizations, some cases involving the handling
of large IN-lists have become much faster than before. Large IN-lists are normally
used when comparing attribute or index values against some big array of lookup values
or keys provided by the application.</p>

<!-- more -->


<p>Let&rsquo;s quickly create and populate a collection named <code>keys</code> so that we can use some
IN-list queries on it later on:</p>

<p>```js setting up example data
db._create(&ldquo;keys&rdquo;);</p>

<p>// insert 100k documents with some defined keys into the collection
for (var i = 0; i &lt; 100000; ++i) {
  db.keys.insert({ _key: &ldquo;test&rdquo; + i });
}
```</p>

<p>And here is a query to all find documents with one of the provided keys <code>test0</code> to <code>test999</code>.
The IN-list here contains 1,000 values:</p>

<p><code>js using an IN-list with 1,000 values
var keys = [ ];
var n = 1000;
for (var i = 0; i &lt; n; ++i) {
  keys.push("test" + i);
}
db._query("FOR doc IN keys FILTER doc._key IN @keys RETURN doc", { keys: keys }); });
</code></p>

<p>When invoked from the ArangoShell, this takes around 0.6 seconds to complete with ArangoDB 2.5.</p>

<p>Increasing the length of the IN-list from 1,000 to 5,000 values makes this run in around 15 seconds.
With an IN-list of 10,000 values, this already takes more than 60 seconds to complete in 2.5.</p>

<p>Obviously longer IN-lists weren&rsquo;t handled terribly efficiently in 2.5, and should be avoided there
if possible.</p>

<p>I am glad this has been fixed in 2.6. Following is a comparison of the above query for different
IN-list sizes, run on both ArangoDB 2.5 and 2.6.</p>

<p>```plain 2.5 and 2.6 with different IN-list sizes</p>

<h1>of IN-list values    Execution time (2.5)   Execution time (2.6)</h1>

<hr />

<pre><code>          1,000                  0.67 s                 0.03 s
          5,000                 15.34 s                 0.12 s
         10,000                 63.48 s                 0.20 s
         50,000                   n/a                   0.81 s
        100,000                   n/a                   1.60 s
</code></pre>

<p>```</p>

<p>Looks like 2.6 handles longer IN-lists way better than 2.5! The above figures suggest that execution
times now scale about linearly with the number of IN-list values.</p>

<p>Please note that longer IN-lists will still make a the query run longer than when
using shorter IN-lists. This is expected because longer IN-lists require more comparisons to
be made and will lead (in the above example) to more documents being returned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Subquery Optimizations]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/subquery-optimizations/"/>
    <updated>2015-05-04T13:26:00+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/subquery-optimizations</id>
    <content type="html"><![CDATA[<p>This is another post demonstrating some of the AQL query performance improvements
that can be expected in ArangoDB 2.6. Specifically, this post is about an optimization
for subqueries. AQL queries with multiple subqueries will likely benefit from it.</p>

<!-- more -->


<p>The following example queries use the same <code>users</code> collection with 100,000 documents
that was used in the <a href="/blog/2015/05/04/return-value-optimization-for-aql/">previous post about return value optimizations</a>.
Again, the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>.</p>

<p>We&rsquo;ll start with a query that uses a single subquery extracting all users from the
collection into a flat result array:</p>

<p><code>plain example query with single subquery
RETURN [
  (FOR u IN users RETURN u)
]
</code></p>

<p>This query is equally fast with ArangoDB 2.5 and 2.6, no changes here.</p>

<p>Let&rsquo;s ramp it up to using two subqueries, one for users with a <code>gender</code> attribute value
of <code>male</code>, and one for users with <code>gender</code> attribute value <code>female</code>. No indexes were used
for the extraction in 2.5 nor 2.6:</p>

<p><code>plain example query with two subqueries
RETURN [
  (FOR u IN users FILTER u.gender == 'male' RETURN u),
  (FOR u IN users FILTER u.gender == 'female' RETURN u)
]
</code></p>

<p>The query takes 16.6 seconds to execute in 2.5, but only 2.95 seconds with ArangoDB 2.6.
This 80 % reduction in execution time is due to ArangoDB 2.6 being a bit smarter about
subqueries than 2.5 is.</p>

<p>In the above query, the two subqueries are independent, so not only can they be executed in
any order, but they also do not rely on each other&rsquo;s results. ArangoDB 2.6 will detect that
and avoid copying variables and intermediate results into subqueries if they are actually not
needed there. 2.5 copied all variables into subqueries unconditionally, even if variables
were not needed there.</p>

<p>In 2.6, any AQL query with multiple subqueries will benefit from this optimization. The
performance improvements will be greater if subqueries late in the execution pipeline have a lot of
intermediate results created in front of them, but do not rely on these intermediate results.</p>

<p>Another nice example for a 2.6 speedup is extracting a single attribute per subquery, as is done
for the <code>name</code> attribute in the following query:</p>

<p><code>plain extracting a single attribute in two subqueries
RETURN [
  (FOR u IN users FILTER u.gender == 'male' RETURN u.name),
  (FOR u IN users FILTER u.gender == 'female' RETURN u.name)
]
</code></p>

<p>This takes 42 seconds to execute in 2.5, and only 0.86 seconds in 2.6. This is a more than
95 % reduction in execution time. It is caused by a mix of factors, one of them again being
the subquery optimization that avoids copying unneeded intermediate results.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Return Value Optimization for AQL]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql/"/>
    <updated>2015-05-04T10:32:43+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql</id>
    <content type="html"><![CDATA[<p>While in search for further AQL query optimizations last week, we found that intermediate AQL
query results were copied one time too often in some cases. Precisely, the data that a query&rsquo;s
<code>ReturnNode</code> will return to the caller was copied into the <code>ReturnNode</code>&rsquo;s own register. With
<code>ReturnNode</code>s never modifying their input data, this demanded for something that is called
<em>return-value optimization</em> in compilers.</p>

<p>2.6 will now optimize away these copies in many cases, and this post shows which performance
benefits can be expected due to the optimization.</p>

<!-- more -->


<p>The effect of the optimization can be demonstrated easily with a few simple AQL queries.
Let&rsquo;s start with a query that simply returns all 100,000 documents from a collection <code>users</code>:
<code>FOR u IN users RETURN u</code> (the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>).</p>

<p>This query&rsquo;s execution plan is already straight-forward and simple:</p>

<p><img src="/downloads/screenshots/return.png"></p>

<p>So where&rsquo;s the problem?</p>

<p>The <code>ReturnNode</code> in this query (and other queries too) will copy its input data into its own output
register, only to finally hand the results to the query&rsquo;s caller. This copying is most often unnecessary
as the <code>ReturnNode</code> will not modify its input. So the idea was to get rid of the copying action and
tell the query&rsquo;s calling code in which (now different) register to look for the results.</p>

<p>Optimizing away the copying inside the <code>ReturnNode</code> made the query run faster already.
The same query now returns the 100,000 documents in 0.24 to 0.26 seconds, compared to 0.27 to 0.30 s
before applying the optimization.</p>

<p>Returning just an attribute of each document shows about the same improvement rates. The execution
times of the query <code>FOR u IN users RETURN u._key</code> drop to between 0.13 and 0.14 seconds with the
optimization, from initially between 0.15 and 0.17 seconds.</p>

<p>Another example query, <code>FOR i IN 1..1000000 RETURN i</code>, now runs in 0.58 to 0.61 seconds with
the optimization, compared to between 0.77 and 0.81 seconds without it.</p>

<p>These absolute figures may not look overly impressive, but they indicate relative improvements of
between 10 and 25 %, which is quite nice. This is effectively saved CPU time that can now be used
for something more productive.</p>

<p>Of course the performance improvements may not be that high for every imaginable AQL query.
Though the optimization may be active in most AQL queries, its effect will only be measurable
for queries that return a significant number of documents/values. Otherwise the share of the
<code>ReturnNode</code>&rsquo;s work in the query&rsquo;s overall computations may be too low to have any effect.
Additionally, the more work a query spends in performing other operations (e.g. filtering,
sorting, collecting), the less relevant will be the overall effect of the optimized <code>ReturnNode</code>.
Finally, when query results need to be shipped from the server to the client over a network,
the relative effect of the optimization may diminish further.</p>

<p>So your mileage may vary. But the optimization will not do any harm, and together with some
other query optimizations already finished for 2.6 it will contribute to many AQL queries
running faster than before.</p>

<p>AQL queries will benefit from the optimization automatically in ArangoDB 2.6, without requiring
any adjustments to the query string, the server configuration etc. The optimizer will automatically
apply the optimization for the main-level <code>ReturnNode</code> of every AQL query.</p>

<p>On a side note: the optimization will not be shown in the list of applied optimizer rules for the
query. This is because the optimization is performed in some different place in the query
executor, after applying the optimizer rules.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exporting Data for Offline Processing]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/24/exporting-data-for-offline-processing/"/>
    <updated>2015-04-24T15:47:31+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/24/exporting-data-for-offline-processing</id>
    <content type="html"><![CDATA[<p>A few weeks ago I wrote about ArangoDB&rsquo;s
<a href="https://jsteemann.github.io/blog/2015/04/04/more-efficient-data-exports/">specialized export API</a>.</p>

<p>The export API is useful when the goal is to extract all documents from a given collection
and to process them outside of ArangoDB.</p>

<p>The export API can provide quick and memory-efficient snapshots of the data in the underlying
collection, making it suitable for extract all documents of the collection. It will be able
to provide data much faster than with an AQL query that will extract all documents.</p>

<p>In this post I&rsquo;ll show how to use the export API to extract data and process it with PHP.</p>

<!-- more -->


<p>A prerequiste for using the export API is using an ArangoDB server with version 2.6
or higher. As there hasn&rsquo;t been an official 2.6 release yet, this currently requires
building the <code>devel</code> branch of ArangoDB from source. When there is a regular 2.6
release, this should be used instead.</p>

<h2>Importing example data</h2>

<p>First we need some data in an ArangoDB collection that we can process externally.</p>

<p>For the following examples, I&rsquo;ll use a collection named <code>users</code> which I&rsquo;ll populate
with 100k <a href="/downloads/code/users-100000.json.tar.gz">example documents</a>. Here&rsquo;s how
to get this data into ArangoDB:</p>

<p>```bash commands for fetching and importing data</p>

<h1>download data file</h1>

<p>wget <a href="https://jsteemann.github.io/downloads/code/users-100000.json.tar.gz">https://jsteemann.github.io/downloads/code/users-100000.json.tar.gz</a></p>

<h1>uncompress it</h1>

<p>tar xvfz users-100000.json.tar.gz</p>

<h1>import into ArangoDB</h1>

<p>arangoimp &mdash;file users-100000.json &mdash;collection users &mdash;create-collection true
```</p>

<p>There should now be 100K documents present in a collection named <code>users</code>. You can
quickly verify that by peeking into the collection using the web interface.</p>

<h2>Setting up ArangoDB-PHP</h2>

<p>An easy way of trying the export API is to use it from PHP. We therefore clone the
devel branch of the <strong>arangodb-php</strong> Github repository into a local directory:</p>

<p><code>bash cloning arangodb-php
git clone -b devel "https://github.com/arangodb/arangodb-php.git"
</code></p>

<p>Note: when there is an official 2.6 release, the <code>2.6</code> branch of arangodb-php should
be used instead of the <code>devel</code> branch.</p>

<p>We now write a simple PHP script that establishes a connection to the ArangoDB
server running on localhost. We&rsquo;ll extend that file gradually. Here&rsquo;s a skeleton
file to start with. The code can be downloaded <a href="/downloads/code/export-skeleton.php">here</a>:</p>

<p>```php skeleton file for establishing a connection
&lt;?php</p>

<p>namespace triagens\ArangoDb;</p>

<p>// use the driver&rsquo;s autoloader to load classes
require &lsquo;arangodb-php/autoload.php&rsquo;;
Autoloader::init();</p>

<p>// set up connection options
$connectionOptions = array(
  // endpoint to connect to
  ConnectionOptions::OPTION_ENDPOINT     => &lsquo;tcp://localhost:8529&rsquo;,
  // can use Keep-Alive connection
  ConnectionOptions::OPTION_CONNECTION   => &lsquo;Keep-Alive&rsquo;,         <br/>
  // use basic authorization
  ConnectionOptions::OPTION_AUTH_TYPE    => &lsquo;Basic&rsquo;,               <br/>
  // user for basic authorization
  ConnectionOptions::OPTION_AUTH_USER    => &lsquo;root&rsquo;,                    <br/>
  // password for basic authorization
  ConnectionOptions::OPTION_AUTH_PASSWD  => &lsquo;&rsquo;,                    <br/>
  // timeout in seconds
  ConnectionOptions::OPTION_TIMEOUT      => 30,
  // database name
  ConnectionOptions::OPTION_DATABASE     => &lsquo;_system&rsquo;
);</p>

<p>try {
  // establish connection
  $connection = new Connection($connectionOptions);</p>

<p>  echo &lsquo;Connected!&rsquo; . PHP_EOL;</p>

<p>  // TODO: now do something useful with the connection!</p>

<p>} catch (ConnectException $e) {
  print $e . PHP_EOL;
} catch (ServerException $e) {
  print $e . PHP_EOL;
} catch (ClientException $e) {
  print $e . PHP_EOL;
}
```</p>

<p>Running that script should simply print <code>Connected!</code>. This means the PHP script
can connect to ArangoDB and we can go on.</p>

<h2>Extracting the data</h2>

<p>With a working database connection we can now start with the actual processing.
In place of the <code>TODO</code> in the skeleton file, we can actually run an export of
the data in collection <code>users</code>. The following simple function extracts all
documents from the collection and writes them to an output file <code>output.json</code>
in JSON format.</p>

<p>It will also print some statistics about the number of documents and the total
data size. The full script can be downloaded <a href="/downloads/code/export.php">here</a>:</p>

<p>```php exporting data into a file
function export($collection, Connection $connection) {
  $fp = fopen(&lsquo;output.json&rsquo;, &lsquo;w&rsquo;);</p>

<p>  if (! $fp) {</p>

<pre><code>throw new Exception('could not open output file!');
</code></pre>

<p>  }</p>

<p>  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true       // use simple PHP arrays
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // statistics
  $count   = 0;
  $batches = 0;
  $bytes   = 0;</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';
foreach ($docs as $doc) {
  $output .= json_encode($doc) . PHP_EOL;
} 

// write out chunk
fwrite($fp, $output);

// update statistics
$count += count($docs);
$bytes += strlen($output);
++$batches;
</code></pre>

<p>  }</p>

<p>  fclose($fp);</p>

<p>  echo sprintf(&lsquo;written %d documents in %d batches with %d total bytes&rsquo;,</p>

<pre><code>           $count,
           $batches, 
           $bytes) . PHP_EOL;
</code></pre>

<p>}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<p>Running this version of the script should print something similar to the following
and also produce a file named <code>output.json</code>. Each line in the file should be a JSON
object representing a document in the collection.</p>

<p><code>plain script output
written 100000 documents in 20 batches with 40890013 total bytes
</code></p>

<h2>Applying some transformations</h2>

<p>We now use PHP to transform data as we extract it. With an example script, we&rsquo;ll apply
the following transformations on the data:</p>

<ul>
<li>rewrite the contents of the <code>gender</code> attribute:

<ul>
<li><code>female</code> should become <code>f</code></li>
<li><code>male</code> should become <code>m</code></li>
</ul>
</li>
<li>rename attribute <code>birthday</code> to <code>dob</code></li>
<li>change date formats in <code>dob</code> and <code>memberSince</code> from YYYY-MM-DD to MM/DD/YYYY</li>
<li>concatenate the contents of the <code>name.first</code> and <code>name.last</code> subattributes</li>
<li>transform array in <code>contact.email</code> into a flat string</li>
<li>remove all other attributes</li>
</ul>


<p>Here&rsquo;s a transformation function that does this, and a slightly simplified export
function. This version of the script can also be downloaded <a href="/downloads/code/export-transform.php">here</a>:</p>

<p>```php transformation and export functions
function transformDate($value) {
  return preg_replace(&lsquo;/^(\d+)&ndash;(\d+)&ndash;(\d+)$/&rsquo;, &lsquo;\2/\3/\1&rsquo;, $value);
}</p>

<p>function transform(array $document) {
  static $genders = array(&lsquo;male&rsquo; => &rsquo;m', &lsquo;female&rsquo; => &lsquo;f&rsquo;);</p>

<p>  $transformed = array(</p>

<pre><code>'gender'      =&gt; $genders[$document['gender']],
'dob'         =&gt; transformDate($document['birthday']),
'memberSince' =&gt; transformDate($document['memberSince']),
'fullName'    =&gt; $document['name']['first'] . ' ' . $document['name']['last'],
'email'       =&gt; $document['contact']['email'][0]
</code></pre>

<p>  );</p>

<p>  return $transformed;
}</p>

<p>function export($collection, Connection $connection) {
  $fp = fopen(&lsquo;output-transformed.json&rsquo;, &lsquo;w&rsquo;);</p>

<p>  if (! $fp) {</p>

<pre><code>throw new Exception('could not open output file!');
</code></pre>

<p>  }</p>

<p>  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true       // use simple PHP arrays
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';
foreach ($docs as $doc) {
  $output .= json_encode(transform($doc)) . PHP_EOL;
} 

// write out chunk
fwrite($fp, $output);
</code></pre>

<p>  }</p>

<p>  fclose($fp);
}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<p>The adjusted version of the PHP script will now produce an output file named
<code>output-transformed.json</code>.</p>

<h2>Filtering attributes</h2>

<p>In the last example we discarded a few attributes of each document. Instead of
filtering out these attributes with PHP, we can configure the export to already
exclude these attributes server-side. This way we can save some traffic.</p>

<p>Here&rsquo;s an adjusted configuration that will exclude the unneeded attributes <code>_id</code>,
<code>_rev</code>, <code>_key</code> and <code>likes</code>:</p>

<p>```php configuration for attribute exclusion
// settings to use for the export
$settings = array(
  &lsquo;batchSize&rsquo; => 5000,  // export in chunks of 5K documents
  &lsquo;_flat&rsquo; => true,      // use simple PHP arrays
  &lsquo;restrict&rsquo; => array(</p>

<pre><code>'type' =&gt; 'exclude',
'fields' =&gt; array('_id', '_rev', '_key', 'likes')
</code></pre>

<p>  )
);
```</p>

<p>The full script that employs the adjusted configuration can be downloaded
<a href="/downloads/code/export-exclude.php">here</a>.</p>

<p>Instead of excluding specific attributes we can also do it the other way and only
include certain attributes in an export. The following script demonstrates this by
extracting only the <code>_key</code> and <code>name</code> attributes of each document. It then prints the
key/name pairs in CSV format.</p>

<p>The full script can be downloaded <a href="/downloads/code/export-csv.php">here</a>.</p>

<p>```php export function that prints key/name pairs in CSV format
function export($collection, Connection $connection) {
  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true,      // use simple PHP arrays
'restrict' =&gt; array(
  'type' =&gt; 'include',
  'fields' =&gt; array('_key', 'name')
)
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';

foreach ($docs as $doc) {
  $values = array(
    $doc['_key'], 
    $doc['name']['first'] . ' ' . $doc['name']['last']
  );

  $output .= '"' . implode('","', $values) . '"' . PHP_EOL;
}

// print out the data directly 
print $output;
</code></pre>

<p>  }
}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<h2>Using the API without PHP</h2>

<p>The export API REST interface is simple and it can be used with any client that can
speak HTTP. This includes <em>curl</em> obviously:</p>

<p>The following command fetches the initial 5K documents from the <code>users</code> collection
using <em>curl</em>:</p>

<p><code>bash using the export API with curl
curl                                                   \
  -X POST                                              \
  http://localhost:8529/_api/export?collection=users   \
  --data '{"batchSize":5000}'
</code></p>

<p>The HTTP response will contain a <code>result</code> attribute that contains the actual
documents. It will also contain an attribute <code>hasMore</code> that will indicate whether
there are more documents for the client to fetch. If it is set to <code>true</code>, the
HTTP response will also contain an attribute <code>id</code>. The client can use this id
for sending follow-up requests like this (assuming the returned id was <code>13979338067709</code>):</p>

<p><code>bash sending a follow-up request with curl
curl                                                   \
  -X PUT                                               \
  http://localhost:8529/_api/export/13979338067709  
</code></p>

<p>That&rsquo;s about it. Using the export API it should be fairly simple to ship bulk
ArangoDB data to client applications or data processing tools.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Functions Improvements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/23/aql-functions-improvements/"/>
    <updated>2015-04-23T10:24:53+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/23/aql-functions-improvements</id>
    <content type="html"><![CDATA[<p>Waiting for a <code>git pull</code> to complete over an 8 KiB/s internet connection is boring.
So I thought I&rsquo;d rather use the idle time and quickly write about some performance
improvements for certain AQL functions that were recently completed and that will
become available with ArangoDB 2.6.</p>

<p>The improvements affect the following AQL functions:</p>

<ul>
<li><code>UNSET()</code>: remove specified attributes from an object/document</li>
<li><code>KEEP()</code>: keep only specified attributes of an object/document</li>
<li><code>MERGE()</code>: merge the attributes of multiple objects/documents</li>
</ul>


<p>This blog post shows a few example queries that will benefit from <strong>50 to more than 60 %
reductions</strong> in query execution times due to the changes done to these functions.</p>

<!-- more -->


<h2>When to expect benefits</h2>

<p>Reductions in execution time can be expected for AQL queries invoking one of the above
AQL functions many times, and if the AQL function is used in a so-called <em>simple</em>
calculation. Whether or not a calculation is considered <em>simple</em> is shown in the detailed
JSON output of an <code>explain()</code>.</p>

<p>Queries will not benefit if they invoke the AQL functions only a few times or when the
function call is contained in an expression that is executed using the non-<em>simple</em> code path.</p>

<h2>Example queries</h2>

<p>Following are a few example queries for the three AQL functions, showing the reductions in
execution times. They all use the <em>simple</em> code path so the benefits can be reaped.</p>

<p>For comparing the execution times between 2.5 and 2.6, I have prepared a simple test setup.
Here is a test function that will create a collection named <code>test</code> and populate it
with a configurable amount documents. It will then run an AQL query that will update
each document in the collection, using one of the named AQL functions. The function
will return the execution time for the AQL query, excluding the collection setup time:</p>

<p>```js test function
var run = function (n, query) {
  var time = require(&ldquo;internal&rdquo;).time;
  var db = require(&ldquo;org/arangodb&rdquo;).db;</p>

<p>  // drop and re-create test collection
  db.<em>drop(&ldquo;test&rdquo;);
  var c = db.</em>create(&ldquo;test&rdquo;);</p>

<p>  // insert n documents
  for (var i = 0; i &lt; n; ++i) {</p>

<pre><code>c.insert({ value1: i, value2: i, value3: 'foobar' + i }); 
</code></pre>

<p>  }</p>

<p>  // flush write-ahead log and wait a few seconds before running query
  require(&ldquo;internal&rdquo;).wal.flush();
  require(&ldquo;internal&rdquo;).wait(5);</p>

<p>  // run query
  var s = time();
  db._query(query);
  return time() &ndash; s;
};
```</p>

<h3>UNSET()</h3>

<p>Let&rsquo;s start with the <code>UNSET()</code> AQL function. Its purpose is to remove or multiple
attributes from an object/document. Here is an example AQL query that removes
attribute <code>value2</code> from each document in the <code>test</code> collection:</p>

<p><code>js invocation of UNSET()
run(n, "FOR t IN test LET modified = UNSET(t, 'value2') REPLACE t WITH modified IN test");
</code></p>

<p>The execution times (in seconds) for different <code>n</code> values in ArangoDB 2.5 and 2.6 are:</p>

<p>```plain execution times of UNSET() query in 2.5 and 2.6</p>

<h2>value of n       time 2.5       time 2.6       speedup</h2>

<p>   100,000         3.28 s         1.13 s          65 %
   500,000        16.93 s         5.38 s          68 %
 1,000,000        32.60 s        11.40 s          65 %
```</p>

<h3>KEEP()</h3>

<p>The purpose of <code>KEEP()</code> is to remove all attributes from an object/document but the
specified ones. Here&rsquo;s an example query that uses <code>KEEP()</code> to remove all attributes
from the documents in the <code>test</code> collectionn but attribute <code>value2</code>:</p>

<p><code>js invocation of KEEP()
run(n, "FOR t IN test LET modified = KEEP(t, 'value2') REPLACE t WITH modified IN test");
</code></p>

<p>The execution times (in seconds) for different <code>n</code> values in ArangoDB 2.5 and 2.6 are:</p>

<p>```plain execution times of KEEP() query in 2.5 and 2.6</p>

<h2>value of n       time 2.5       time 2.6       speedup</h2>

<p>   100,000         1.98 s         0.87 s          56 %
   500,000         9.34 s         4.09 s          56 %
 1,000,000        18.86 s         8.23 s          56 %
```</p>

<h3>MERGE()</h3>

<p>Finally, the <code>MERGE()</code> function can be used to merge multiple objects/documents in a
single one. The following query will add an attribute <code>value4</code> to all documents in
collection <code>test</code>:</p>

<p><code>js invocation of MERGE()
run(n, "FOR t IN test LET modified = MERGE(t, { value4 : 1 }) REPLACE t WITH modified IN test");
</code></p>

<p>The execution times (in seconds) for different <code>n</code> values in ArangoDB 2.5 and 2.6 are:</p>

<p>```plain execution times of MERGE() query in 2.5 and 2.6</p>

<h2>value of n       time 2.5       time 2.6       speedup</h2>

<p>   100,000         3.93 s         1.22 s          68 %
   500,000        19.17 s         5.91 s          69 %
 1,000,000        38.27 s        12.33 s          67 %
```</p>
]]></content>
  </entry>
  
</feed>
