<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2016-06-16T19:39:17+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Compiling an Optimized Version of ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2016/06/09/compiling-an-optimized-version-of-arangodb/"/>
    <updated>2016-06-09T20:31:31+02:00</updated>
    <id>http://jsteemann.github.io/blog/2016/06/09/compiling-an-optimized-version-of-arangodb</id>
    <content type="html"><![CDATA[<p>Why should you care about compiling ArangoDB on your own when there are
official release packages that are ready to use?</p>

<!--more -->


<p>There are three main reasons for compiling on your own:</p>

<ul>
<li><p>as a developer you want to <em>make changes to the ArangoDB C++ source code</em>.
Then your only option obviously is to compile on your own. Please consult
the <a href="/blog/2016/06/09/compiling-a-debug-version-of-arangodb/">compiling a debug version of ArangoDB</a>
page for more information.</p></li>
<li><p>you are trying to <em>get meaningful stack traces from core dumps</em> produced
by ArangoDB and need an ArangoDB binary that comes with enough debug
information (debug symbols, probably also assertions turned on). In this
case, please also consult the <a href="/blog/2016/06/09/compiling-a-debug-version-of-arangodb/">compiling a debug version of ArangoDB</a>
blog post for how to get this done.</p></li>
<li><p>you want to use an ArangoDB binary that is <em>optimized for your specific
target architecture</em>.</p></li>
</ul>


<p>The latter reason is relevant because the official release packages that
are provided by ArangoDB cannot make too many assumptions about the environment
in which they will be used. In the general release packages there is not so
much room for platform-specific optimizations as there would be if you are
compiling just for the local machine.</p>

<p>For example, all relevant CPU offer <a href="https://en.wikipedia.org/wiki/SIMD">SIMD</a>
instructions that a compiler can exploit when generating code. But different
generations of CPUs offer different levels of SIMD instructions. Not every CPU
in use today provides SSE4, not to talk about AVX.</p>

<p>To make our release packages compatible with most environments, we have had
to make some conservative assumptions about the CPU abilities, which effectively
disables many optimizations that would have been possible when creating a
build that only needs to run on a specific architecture.</p>

<p>To fully exploit the capabilities of a specific target environment, it&rsquo;s required
to build executables for that specific architecture. Most compilers offer an option
<code>-march</code> for that. You normally want to set this to <code>native</code> when compiling an
optimized version. There are also lots of compiler options for enabling or disabling
specific CPU features such as <code>-msse</code>, <code>-msse2</code>, <code>-msse3</code>, <code>-mssse3</code>,
<code>-msse4.1</code>, <code>-msse4.2</code>, <code>-msse4</code>, <code>-mavx</code>, <code>-mavx2</code>, to name just a few.</p>

<p>The good news is that there is no need to deal with such compiler-specific optimization
options in order to get an optimized build. The cmake-based ArangoDB 3.0 builds will
automatically test the local environment&rsquo;s capabilities and set the compiler options
based on which CPU abilities were detected.</p>

<p>For example, a mere <code>(cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Release ..)</code> will run the
CPU ability detection and configure the build to use the features supported by the
local architecture:</p>

<p><code>bash configuring a release build
(mkdir -p build; cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Release ..)
</code></p>

<p>For example, on my laptop this prints:
<code>plain cmake output
-- The CXX compiler identification is GNU 5.3.1
-- The C compiler identification is GNU 5.3.1
...
-- target changed from "" to "auto"
-- Detected CPU: haswell
-- Performing Test check_cxx_compiler_flag__march_core_avx2
-- Performing Test check_cxx_compiler_flag__march_core_avx2 - Success
-- Performing Test check_cxx_compiler_flag__msse2
-- Performing Test check_cxx_compiler_flag__msse2 - Success
-- Performing Test check_cxx_compiler_flag__msse3
-- Performing Test check_cxx_compiler_flag__msse3 - Success
-- Looking for pmmintrin.h
-- Looking for pmmintrin.h - found
-- Performing Test check_cxx_compiler_flag__mssse3
-- Performing Test check_cxx_compiler_flag__mssse3 - Success
-- Looking for tmmintrin.h
-- Looking for tmmintrin.h - found
-- Performing Test check_cxx_compiler_flag__msse4_1
-- Performing Test check_cxx_compiler_flag__msse4_1 - Success
-- Looking for smmintrin.h
-- Looking for smmintrin.h - found
-- Performing Test check_cxx_compiler_flag__msse4_2
-- Performing Test check_cxx_compiler_flag__msse4_2 - Success
-- Performing Test check_cxx_compiler_flag__mavx
-- Performing Test check_cxx_compiler_flag__mavx - Success
-- Looking for immintrin.h
-- Looking for immintrin.h - found
-- Performing Test check_cxx_compiler_flag__msse2avx
-- Performing Test check_cxx_compiler_flag__msse2avx - Success
-- Performing Test check_cxx_compiler_flag__mavx2
-- Performing Test check_cxx_compiler_flag__mavx2 - Success
-- Performing Test check_cxx_compiler_flag__mno_sse4a
-- Performing Test check_cxx_compiler_flag__mno_sse4a - Success
-- Performing Test check_cxx_compiler_flag__mno_xop
-- Performing Test check_cxx_compiler_flag__mno_xop - Success
-- Performing Test check_cxx_compiler_flag__mno_fma4
-- Performing Test check_cxx_compiler_flag__mno_fma4 - Success
...
</code>
The detected options will end up in the CMakeCache.txt file in the build
directory. The Makefile generated by cmake will automatically make use of
these options when invoking the C++ compiler.</p>

<p>The compiler options are not shown by default, but they can be made visible
by compiling with the option <code>VERBOSE=1</code>, e.g.</p>

<p><code>bash configuring and compiling a build
(mkdir -p build; cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Release ..)
(cd build &amp;&amp; make -j4 VERBOSE=1)
</code></p>

<p>Note that this will be <em>very verbose</em>, so you only want to set the <code>VERBOSE=1</code>
option to check that the compiler options were picked correctly.</p>

<p>On my local laptop, the architecture-specific compiler options that were automatically
detected and used were</p>

<p><code>bash compiler architecture options used
-march=core-avx2 -msse2 -msse3 -mssse3 -msse4.1 -msse4.2 -mavx -msse2avx -mavx2 -mno-sse4a -mno-xop -mno-fma4
</code></p>

<p>The build has detected <code>core-avx2</code>, which in my case is good &ndash; and a lot more
specific than the official packages which for example cannot assume the presence
of either SSE4 or AVX instructions.</p>

<p>And now that we can rely on the presence of specific CPU instructions, some code
parts such as JSON parsing can make use of SSE4.2 instructions, or the compiler
can use some optimized SIMD variants for operations such as memcpy, strlen etc.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fastest String-to-uint64 Conversion Method?]]></title>
    <link href="http://jsteemann.github.io/blog/2016/06/02/fastest-string-to-uint64-conversion-method/"/>
    <updated>2016-06-02T20:15:27+02:00</updated>
    <id>http://jsteemann.github.io/blog/2016/06/02/fastest-string-to-uint64-conversion-method</id>
    <content type="html"><![CDATA[<p>While doing some optimization work for the upcoming ArangoDB 3.0 release,
we had to figure out what was the &ldquo;ideal&rdquo; way of converting a string
representation of a number into a C++ uint64_t (64 bit unsigned integer type).
This kind of operation is performed a lot during the lifetime of an ArangoDB
server process, so it seemed worthwhile making it as fast as possible.</p>

<!-- more -->


<h2>std::stoull and std::strtoull</h2>

<p>The natural solution for this kind of conversion is the standard library&rsquo;s
<a href="http://en.cppreference.com/w/cpp/string/basic_string/stoul">std::stoull</a>
function. On the one hand, that solution is potentially optimized and definitely
robust. It performs validation of the input characters and is also battle-tested
by millions of users.</p>

<p>On the other hand, std::stoull has a few &ldquo;features&rdquo; that some would consider
&ldquo;anti&rdquo;-features, and that sound like they could negatively influence performance:</p>

<ul>
<li>it may behave locale-dependent</li>
<li>it can parse and will let pass negative input values, which is often not desired
for a result of unsigned type</li>
<li>it can perform base conversion</li>
</ul>


<p>In C++11 there is also the alternative
<a href="http://en.cppreference.com/w/cpp/string/byte/strtoul">std::strtoull</a>.
It should behave about the same as std::stoull except in case of error.
std::stoull will throw and std::strtoull will not.</p>

<p>That&rsquo;s what we get from the standard library for long unsigned integer types.</p>

<h2>Alternative implementations (w/o error checking)</h2>

<p>Another alternative is to roll a string to number conversion function ourselves.
If we hard-code it to base 10 and do not care about error checking, a naive
implementation may look like this:</p>

<p>```cpp
inline uint64_t naive(std::string const&amp; value) {
  uint64_t result = 0;
  char const<em> p = value.c_str();
  char const</em> q = p + value.size();
  while (p &lt; q) {</p>

<pre><code>result *= 10;
result += *(p++) - '0';
</code></pre>

<p>  }
  return result;
}
```</p>

<p>Obviously the above will produce wrong results for invalid
input data, but for &ldquo;trusted&rdquo; (known to be valid) input it may
be just fine.</p>

<p>Here&rsquo;s just another implementation for the problem at hand. This one
doesn&rsquo;t perform the times 10 operation, but splits it into two bitshifting
operations:</p>

<p>```cpp
inline uint64_t bitshift(std::string const&amp; value) {
  uint64_t result = 0;</p>

<p>  char const<em> p = value.c_str();
  char const</em> q = p + value.size();
  while (p &lt; q) {</p>

<pre><code>result = (result &lt;&lt; 1) + (result &lt;&lt; 3) + *(p++) - '0';
</code></pre>

<p>  }
  return result;
}
```</p>

<p>Again no error checking is present in the above function, but it
should be ok for trusted inputs.</p>

<p>By manually unrolling the while loop and converting it into a switch
statement, we can also come up with a conversion function that has minimal
branching:</p>

<p>```cpp
inline uint64_t unrolled(std::string const&amp; value) {
  uint64_t result = 0;</p>

<p>  size_t const length = value.size();
  switch (length) {</p>

<pre><code>case 20:    result += (value[length - 20] - '0') * 10000000000000000000ULL;
case 19:    result += (value[length - 19] - '0') * 1000000000000000000ULL;
case 18:    result += (value[length - 18] - '0') * 100000000000000000ULL;
case 17:    result += (value[length - 17] - '0') * 10000000000000000ULL;
case 16:    result += (value[length - 16] - '0') * 1000000000000000ULL;
case 15:    result += (value[length - 15] - '0') * 100000000000000ULL;
case 14:    result += (value[length - 14] - '0') * 10000000000000ULL;
case 13:    result += (value[length - 13] - '0') * 1000000000000ULL;
case 12:    result += (value[length - 12] - '0') * 100000000000ULL;
case 11:    result += (value[length - 11] - '0') * 10000000000ULL;
case 10:    result += (value[length - 10] - '0') * 1000000000ULL;
case  9:    result += (value[length -  9] - '0') * 100000000ULL;
case  8:    result += (value[length -  8] - '0') * 10000000ULL;
case  7:    result += (value[length -  7] - '0') * 1000000ULL;
case  6:    result += (value[length -  6] - '0') * 100000ULL;
case  5:    result += (value[length -  5] - '0') * 10000ULL;
case  4:    result += (value[length -  4] - '0') * 1000ULL;
case  3:    result += (value[length -  3] - '0') * 100ULL;
case  2:    result += (value[length -  2] - '0') * 10ULL;
case  1:    result += (value[length -  1] - '0');
</code></pre>

<p>  }
  return result;
}
```</p>

<h2>Performance testing</h2>

<p>To check out how all these alternatives perform, I put them into a small
<a href="/downloads/code/stoull-test.cpp">test driver program</a>. To compile it with
g++ and run it, I used this command:</p>

<p><code>bash
g++ -Wall -Wextra -march=native -std=c++11 -O3 stdoull-test.cpp &amp;&amp; ./a.out
</code></p>

<p>The test program will convert input strings of different lengths to uint64_t
using the above (and some other) implementations, going up from 10,000 iterations
per string up to 100,000,000. It also prints the wall-clock time spent in each
run. The most interesting output it prints is in the &ldquo;ms&rdquo; column. The &ldquo;result&rdquo;
column can be fully ignored. It&rsquo;s only there so the compiler won&rsquo;t optimize away
the calls to the conversion functions.</p>

<p>The timings from my local laptop (Intel Core i7-4710HQ CPU @ 2.50GHz, Ubuntu 16.04,
g++ 5.3.1 &ndash; your mileage may vary!) for the 100,000,000 conversions are:</p>

<p>```plain execution times for various string-to-uint64 implementations
test &lsquo;std::stoull&rsquo;       string &lsquo;1&rsquo;                           1209 ms
test &lsquo;std::stoull&rsquo;       string &lsquo;99&rsquo;                          1382 ms
test &lsquo;std::stoull&rsquo;       string &lsquo;1234&rsquo;                        1725 ms
test &lsquo;std::stoull&rsquo;       string &lsquo;1234567&rsquo;                     2257 ms
test &lsquo;std::stoull&rsquo;       string &lsquo;1234567891&rsquo;                  2764 ms
test &lsquo;std::stoull&rsquo;       string &lsquo;12345678901234&rsquo;              3899 ms
test &lsquo;std::stoull&rsquo;       string &lsquo;12345678901234678901&rsquo;        7391 ms</p>

<p>test &lsquo;std::strtoull&rsquo;     string &lsquo;1&rsquo;                           1104 ms
test &lsquo;std::strtoull&rsquo;     string &lsquo;99&rsquo;                          1300 ms
test &lsquo;std::strtoull&rsquo;     string &lsquo;1234&rsquo;                        1628 ms
test &lsquo;std::strtoull&rsquo;     string &lsquo;1234567&rsquo;                     2428 ms
test &lsquo;std::strtoull&rsquo;     string &lsquo;1234567891&rsquo;                  2662 ms
test &lsquo;std::strtoull&rsquo;     string &lsquo;12345678901234&rsquo;              3705 ms
test &lsquo;std::strtoull&rsquo;     string &lsquo;12345678901234678901&rsquo;        6631 ms</p>

<p>test &lsquo;naive&rsquo;             string &lsquo;1&rsquo;                            202 ms
test &lsquo;naive&rsquo;             string &lsquo;99&rsquo;                           314 ms
test &lsquo;naive&rsquo;             string &lsquo;1234&rsquo;                         475 ms
test &lsquo;naive&rsquo;             string &lsquo;1234567&rsquo;                      732 ms
test &lsquo;naive&rsquo;             string &lsquo;1234567891&rsquo;                   987 ms
test &lsquo;naive&rsquo;             string &lsquo;12345678901234&rsquo;              1343 ms
test &lsquo;naive&rsquo;             string &lsquo;12345678901234678901&rsquo;        1862 ms</p>

<p>test &lsquo;bitshift&rsquo;          string &lsquo;1&rsquo;                            188 ms
test &lsquo;bitshift&rsquo;          string &lsquo;99&rsquo;                           245 ms
test &lsquo;bitshift&rsquo;          string &lsquo;1234&rsquo;                         397 ms
test &lsquo;bitshift&rsquo;          string &lsquo;1234567&rsquo;                      462 ms
test &lsquo;bitshift&rsquo;          string &lsquo;1234567891&rsquo;                   666 ms
test &lsquo;bitshift&rsquo;          string &lsquo;12345678901234&rsquo;               888 ms
test &lsquo;bitshift&rsquo;          string &lsquo;12345678901234678901&rsquo;        1277 ms</p>

<p>test &lsquo;unrolled&rsquo;          string &lsquo;1&rsquo;                            289 ms
test &lsquo;unrolled&rsquo;          string &lsquo;99&rsquo;                           289 ms
test &lsquo;unrolled&rsquo;          string &lsquo;1234&rsquo;                         351 ms
test &lsquo;unrolled&rsquo;          string &lsquo;1234567&rsquo;                      408 ms
test &lsquo;unrolled&rsquo;          string &lsquo;1234567891&rsquo;                   547 ms
test &lsquo;unrolled&rsquo;          string &lsquo;12345678901234&rsquo;               778 ms
test &lsquo;unrolled&rsquo;          string &lsquo;12345678901234678901&rsquo;        1068 ms
```</p>

<p>It&rsquo;s no surprise the standard library functions with their generality and
error checking features are slower than the specialized functions that took
the liberty to ignore all that.</p>

<p>Which method is fastest now?</p>

<p>As can be seen above, the &ldquo;bitshift&rdquo; variant was fastest for short and medium
length inputs in my environment. At some point when input values get longer,
the &ldquo;bitshift&rdquo; methods gets overtaken by the &ldquo;unrolled&rdquo; variant. The &ldquo;naive&rdquo;
variant was slower than the two in most cases, but still performs surprisingly
well.</p>

<h2>Conclusion</h2>

<p>The take-away: even though string-to-number conversion routines are
present in the standard library, it can still be beneficial to hand-craft
specialized variants of them for maximum performance. This is especially true
when most of the generality that the standard library routines provide is not
required in your use case.</p>

<p>For example, if you know that the conversion functions will only operate on
&ldquo;trusted&rdquo; input (only numeric digit input characters, length of input string
won&rsquo;t exceed the maximum length of a uint64_t number etc.) then error checking
is not required.</p>

<p>Additionally, if you can limit yourself to numbers of a specific base and
don&rsquo;t negative base conversion nor the handling of negative values, a lot of
the generality and safety overhead of std::stoull may be avoided.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Optimizer Improvements for 2.8]]></title>
    <link href="http://jsteemann.github.io/blog/2015/12/22/aql-optimizer-improvements-for-28/"/>
    <updated>2015-12-22T22:39:30+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/12/22/aql-optimizer-improvements-for-28</id>
    <content type="html"><![CDATA[<p>With the 2.8 beta phase coming to an end it&rsquo;s time to shed some light
on the improvements in the 2.8 AQL optimizer. This blog post summarizes
a few of them, focusing on the query optimizer. There&rsquo;ll be a follow-up
post that will explain dedicated new AQL features soon.</p>

<!-- more -->


<h2>Array indexes</h2>

<p>2.8 allows creating hash and skiplist indexes on attributes which are arrays.
Creating such index works similar to creating a non-array index, with the
exception that the name of the array attribute needs to be followed by a <code>[*]</code>
in the index fields definition:</p>

<p><code>js creating an array index
db._create("posts");
db.posts.ensureIndex({ type: "hash", fields: [ "tags[*]" ] });
</code></p>

<p>Now if the <code>tags</code> attribute of a document in the <code>posts</code> collection is an array,
each array member will be inserted into the index:</p>

<p><code>js storing an array value
db.posts.insert({ tags: [ "arangodb", "database", "aql" ] });
db.posts.insert({ tags: [ "arangodb", "v8", "javascript" ] });
db.posts.insert({ tags: [ "javascript", "v8", "nodejs" ] });
</code></p>

<p>The index on <code>tags[*]</code> will now contain the values <code>arangodb</code>, <code>database</code>, <code>aql</code> and
<code>nosql</code> for the first document, <code>arangodb</code>, <code>v8</code> and <code>javascript</code> for the second, and
<code>javascript</code>, <code>v8</code> and <code>nodejs</code> for the third.</p>

<p>The following AQL will find any documents that have a value of <code>javascript</code> contained
in their <code>tags</code> value:</p>

<p><code>plain array index query
FOR doc IN posts
  FILTER 'javascript' IN doc.tags[*]
  RETURN doc
</code></p>

<p>This will use the array index on <code>tags[*]</code>.</p>

<p>The array index works by inserting all members from an array into the index
separately. Duplicates are removed automatically when populating the index.</p>

<p>An array index can also be created on a sub-attribute of array members. For
example, the following definition will make sure the <code>name</code> sub-attributes of
the <code>tags</code> array values will make it into the index:</p>

<p><code>js creating an array index on a sub-attribute
db.posts.ensureIndex({ type: "hash", fields: [ "tags[*].name" ] });
</code></p>

<p>That will allow storing data as follows:</p>

<p><code>js storing an array value
db.posts.insert({ tags: [ { name: "arangodb" }, { name: "database" }, { name: "aql" } ] });
db.posts.insert({ tags: [ { name: "arangodb" }, { name: "v8" }, { name: "javascript" } ] });
db.posts.insert({ tags: [ { name: "javascript" }, { name: "v8" }, { name: "nodejs" } ] });
</code></p>

<p>The (index-based) selection query for this data structure then becomes</p>

<p><code>plain array index query using sub-attributes
FOR doc IN posts
  FILTER 'javascript' IN doc.tags[*].name
  RETURN doc
</code></p>

<p>Contrary to MongoDB, there is no automatic conversion to array values when
inserting non-array values in ArangoDB. For example, the following plain strings
will not be inserted into an array index, simply because the value of the index
attribute is not an array:</p>

<p><code>js storing non array values without indexing
db.posts.insert({ tags: "arangodb" });
db.posts.insert({ tags: "javascript" });
db.posts.insert({ tags: "nodejs" });
</code></p>

<p>Note that in this case a non-array index can still be used.</p>

<h2>Use of multiple indexes per collection</h2>

<p>The query optimizer can now make use of multiple indexes if multiple filter
conditions are combined with logical ORs, and all of them are covered by
indexes of the same collection.</p>

<p>Provided there are separate indexes present on <code>name</code> and <code>status</code>, the
following query can make use of index scans in 2.8, as opposed to full
collection scans in 2.7:</p>

<p><code>plain using multiple indexes
FOR doc IN users
  FILTER doc.name == 'root' || doc.status == 'active'
  RETURN doc
</code></p>

<p><img src="/downloads/screenshots/multiple-indexes.png"></p>

<p>If multiple filter conditions match for the same document, the result will
automatically be deduplicated, so each document is still returned at most once.</p>

<h2>Sorted IN comparison</h2>

<p>Another improvement for the optimizer is to pre-sort comparison values for <code>IN</code> and
<code>NOT IN</code> so these operators can use a (much faster) binary search instead of a linear search.</p>

<p>The optimization will be applied automatically for <code>IN</code> / <code>NOT IN</code> comparison values used
in filters, which are used inside of a <code>FOR</code> loop, and depend on runtime values. For example,
the optimization will be applied for the following query:</p>

<p><code>
LET values = /* some runtime expression here */
FOR doc IN collection
  FILTER doc.value IN values
  RETURN doc
</code></p>

<p>The optimization will not be applied for <code>IN</code> comparison values that are value
literals and those that are used in index lookups. For these cases the comparison values
were already deduplicated and sorted.</p>

<p>&ldquo;sort-in-values&rdquo; will appear in the list of applied optimizer rules if the optimizer
could apply the optimization:</p>

<p><img src="/downloads/screenshots/sorted-in.png"></p>

<h2>Optimization for LENGTH(collection)</h2>

<p>There are multiple ways for counting the total number of documents in a collection from
inside an AQL query. One obvious way is to use <code>RETURN LENGTH(collection)</code>.</p>

<p>That variant however was inefficient as it fully materialized the documents before
counting them. In 2.8 calling <code>LENGTH()</code> for a collection will get automatically replaced
by a call to a special function that can efficiently determine the number of documents.
For larger collections, this can be several thousand times faster than the naive 2.7
solution.</p>

<h2>C++ implementation for many AQL functions</h2>

<p>Many existing AQL functions have been backed with a C++ implementation that removes
the need for some data conversion that would otherwise happen if the function were
implemented in V8/JavaScript only. More than 30+ functions have been changed, including
several that may produce bigger result sets (such as <code>EDGES()</code>, <code>FULLTEXT()</code>, <code>WITHIN()</code>,
<code>NEAR()</code>) and that will hugely benefit from this.</p>

<h2>Improved skip performance</h2>

<p>2.8 improves the performance of skipping over many documents in case no indexes and no
filters are used. This might sound like an edge case, but it is quite common when the
task is to fetch documents from a big collection in chunks and there is certainty that
there will be no parallel modifications.</p>

<p>For example, the following query runs about 3 to 5 times faster in 2.8, and this
improvements can easily sum up to notable speedups if the query is called repeatedly
with increasing offset values for <code>LIMIT</code>:</p>

<p><code>plain query with huge skip
FOR doc IN collection
  LIMIT 1000000, 10
  RETURN doc
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Function Speedups]]></title>
    <link href="http://jsteemann.github.io/blog/2015/11/20/aql-function-speedups/"/>
    <updated>2015-11-20T16:55:34+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/11/20/aql-function-speedups</id>
    <content type="html"><![CDATA[<p>While working on the upcoming ArangoDB 2.8, we have reimplemented some AQL
functions in C++ for improved performance. AQL queries using these functions
may benefit from using the new implementation of the function.</p>

<p>The following list shows the AQL functions for which a C++ implementation has
been added in 2.8. The other C++-based AQL function implementations added since
ArangoDB 2.5 are also still available. Here&rsquo;s the list of functions added in 2.8:</p>

<!-- more -->


<ul>
<li>document-related functions: DOCUMENT, EDGES, PARSE_IDENTIFIER</li>
<li>numerical functions: ABS, FLOOR, RAND, ROUND, SQRT</li>
<li>statistical functions: MEDIAN, PERCENTILE, STDDEV_POPULATION, STDDEV_SAMPLE, VARIANCE_POPULATION, VARIANCE_SAMPLE</li>
<li>geo functions: NEAR, WITHIN</li>
<li>array functions: APPEND, FIRST, FLATTEN, LAST, MINUS, NTH, POP, POSITION, PUSH, REMOVE_NTH, REMOVE_VALUE, REMOVE_VALUES, SHIFT, UNSHIFT</li>
<li>informational functions: COLLECTIONS, CURRENT_DATABASE, FIRST_DOCUMENT, FIRST_LIST, NOT_NULL</li>
<li>object-related functions: MERGE_RECURSIVE, ZIP</li>
</ul>


<p>Following are a few example queries that benefit from using the C++ variants of some
of the above functions:</p>

<h3>Fetching documents programmatically using the <code>DOCUMENT</code> function:</h3>

<ul>
<li>query: <code>FOR i IN 1..10000 RETURN DOCUMENT(test, CONCAT('test', i))</code></li>
<li>2.7: 0.3005 s</li>
<li>2.8: 0.1050 s</li>
</ul>


<h3>Fetching edges programmatically using the <code>EDGES</code> function:</h3>

<ul>
<li>query: <code>FOR i IN 1..100000 RETURN EDGES(edges, CONCAT('test/test', i), 'outbound')</code>:</li>
<li>2.7: 4.3590 s</li>
<li>2.8: 1.4469 s</li>
</ul>


<h3>Fetching many documents from a geo index, post-filtering most of them:</h3>

<ul>
<li>query: <code>FOR doc IN WITHIN(locations, 0, 0, 100000) FILTER doc.value2 == 'test1001' LIMIT 1 RETURN doc</code></li>
<li>2.7: 2.9876 s</li>
<li>2.8: 0.4087 s</li>
</ul>


<h3>Generating random numbers:</h3>

<ul>
<li>query: <code>FOR value IN 1..100000 RETURN RAND() * 50</code></li>
<li>2.7: 0.1743 s</li>
<li>2.8: 0.1364 s</li>
</ul>


<p>Please note that not in every case there will be a tremendous speedup. As usual,
it depends on how often a function is called inside a query and what other constructs
are used. Your mileage may vary.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Multiple Indexes Per Collection]]></title>
    <link href="http://jsteemann.github.io/blog/2015/11/20/using-multiple-indexes-per-collection/"/>
    <updated>2015-11-20T15:30:05+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/11/20/using-multiple-indexes-per-collection</id>
    <content type="html"><![CDATA[<p>The query optimizer in ArangoDB 2.8 has been improved in terms of how it can
make use of indexes. In previous versions of ArangoDB, the query optimizer could
use only one index per collection used in an AQL query. When using a logical OR
in a FILTER condition, the optimizer did not use any index for the collection in
order to ensure the result is still correct.</p>

<p>This is much better in 2.8. Now the query optimizer can use multiple indexes on
the same collection for FILTER conditions that are combined with a logical OR.</p>

<!-- more -->


<p>For all following queries, I have set up a collection named <code>test</code>, which has
two isolated hash indexes on the attributes <code>value1</code> and <code>value2</code>, and a skiplist
index on attribute <code>value3</code>.</p>

<p>Let&rsquo;s first try an AQL queries that uses a logical OR on two different attributes of the
collection:</p>

<p><code>plain example query
FOR doc IN test
  FILTER doc.value1 == 11 || doc.value2 == 19
  RETURN doc
</code></p>

<p>The execution plan for this query in 2.7 reveals that query will perform a full
collection scan and cannot use indexes because of the logical OR on two different
attributes:</p>

<p><code>``plain 2.7 query execution plan
Execution plan:
 Id   NodeType                  Est.   Comment
  1   SingletonNode                1   * ROOT
  2   EnumerateCollectionNode      0     - FOR doc IN test   /* full collection scan */
  3   CalculationNode              0       - LET #1 = doc.</code>value1<code>== 11 || doc.</code>value2` == 19
  4   FilterNode                   0       &ndash; FILTER #1
  5   ReturnNode                   0       &ndash; RETURN doc</p>

<p>Indexes used:
 none</p>

<p>Optimization rules applied:
 none
```</p>

<p>Running the same query in 2.8 / devel will produce a much better execution plan:</p>

<p><code>``plain 2.8 query execution plan
Execution plan:
 Id   NodeType          Est.   Comment
  1   SingletonNode        1   * ROOT
  6   IndexNode            2     - FOR doc IN test   /* hash index scan, hash index scan */
  3   CalculationNode      2       - LET #1 = doc.</code>value1<code>== 11 || doc.</code>value2` == 19<br/>
  4   FilterNode           2       &ndash; FILTER #1
  5   ReturnNode           2       &ndash; RETURN doc</p>

<p>Indexes used:
 By   Type   Collection   Unique   Sparse   Selectivity   Fields         Ranges
  6   hash   test         false    false       100.00 %   [ <code>value1</code> ]   doc.<code>value1</code> == 11
  6   hash   test         false    false       100.00 %   [ <code>value2</code> ]   doc.<code>value2</code> == 19</p>

<p>Optimization rules applied:
 Id   RuleName
  1   use-indexes
```</p>

<p>Multiple indexes will also be used if different index types are accessed, or for non-equality
filter conditions. For example, the following query will make use of the two hash indexes
and also the skiplist index:</p>

<p><code>plain example query
FOR doc IN test
  FILTER doc.value1 == 11 || doc.value2 == 19 || doc.value3 &gt; 42
  RETURN doc
</code></p>

<p>Here is its execution plan from 2.8:</p>

<p><code>``plain 2.8 query execution plan
Execution plan:
 Id   NodeType          Est.   Comment
  1   SingletonNode        1   * ROOT
  6   IndexNode            3     - FOR doc IN test   /* hash index scan, hash index scan, skiplist index scan */
  3   CalculationNode      3       - LET #1 = doc.</code>value1<code>== 11 || doc.</code>value2<code>== 19 || doc.</code>value3` > 42
  4   FilterNode           3       &ndash; FILTER #1
  5   ReturnNode           3       &ndash; RETURN doc</p>

<p>Indexes used:
 By   Type       Collection   Unique   Sparse   Selectivity   Fields         Ranges
  6   hash       test         false    false       100.00 %   [ <code>value1</code> ]   doc.<code>value1</code> == 11
  6   hash       test         false    false       100.00 %   [ <code>value2</code> ]   doc.<code>value2</code> == 19
  6   skiplist   test         false    false            n/a   [ <code>value3</code> ]   doc.<code>value3</code> > 42</p>

<p>Optimization rules applied:
 Id   RuleName
  1   use-indexes
```</p>

<p>For comparison, here is the non-optimized plan from 2.7 for the same query:</p>

<p><code>``plain 2.7 query execution plan
Execution plan:
 Id   NodeType                  Est.   Comment
  1   SingletonNode                1   * ROOT
  2   EnumerateCollectionNode      0     - FOR doc IN test   /* full collection scan */
  3   CalculationNode              0       - LET #1 = doc.</code>value1<code>== 11 || doc.</code>value2<code>== 19 || doc.</code>value3` > 42
  4   FilterNode                   0       &ndash; FILTER #1
  5   ReturnNode                   0       &ndash; RETURN doc</p>

<p>Indexes used:
 none</p>

<p>Optimization rules applied:
 none
```</p>

<p>Still the query optimizer will not be able to use any indexes on a collection when
there are multiple FILTER conditions combined with logical OR and at least one of them
is not satisfisable by an index of the collection. In this case it has no other choice
but to do a full collection scan.</p>

<p>For queries that combine multiple FILTER conditions with a logical AND, the optimizer
will still try to pick the most selective index for the query and use it for the collection.</p>
]]></content>
  </entry>
  
</feed>
