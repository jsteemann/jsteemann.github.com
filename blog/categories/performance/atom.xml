<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-09-10T14:56:22+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ArangoDB-PHP Driver Improvements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/09/10/arangodb-php-driver-improvements/"/>
    <updated>2015-09-10T14:05:40+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/09/10/arangodb-php-driver-improvements</id>
    <content type="html"><![CDATA[<p>While preparing the release of ArangoDB 2.7, some improvements were made for the
<a href="https://github.com/arangodb/arangodb-php">PHP driver for ArangoDB</a>.</p>

<p>The 2.7 version of the PHP driver now supports the AQL query results cache. The
cache can be turned on or off globally, or be set to demand mode. The demand mode will
allow controlling caching on a per-AQL-query basis.</p>

<p>Additionally, the HTTP transport layer in the PHP driver was improved. Some internal
string handling methods were optimized so that the transport part becomes cheaper. All
driver operations that communicate with the ArangoDB server will benefit from this.</p>

<p>For a demonstration of the improvements, here is a script that creates 100,000
documents in a local ArangoDB database via the PHP driver. As we&rsquo;re interested in assessing
the HTTP layer improvements, the script intentionally issues 100,000 HTTP requests
instead of using the specialized <code>import</code> method provided by the driver.</p>

<p>The script code can be found <a href="https://github.com/arangodb/arangodb-php/blob/devel/examples/http-test.php">here</a>.</p>

<p>The baseline for the improvments is the (non-optimized) 2.6 version of the PHP
driver. Here are the results for issuing 100,000 requests with the 2.6 driver
(script was run twice to see if there are variations in execution time):</p>

<p>```plain execution times with 2.6 driver
creating 100000 documents
creating documents took 55.144556999207 s</p>

<p>creating 100000 documents
creating documents took 54.476955890656 s
```</p>

<p>Running it with the 2.7 version of the PHP driver now shows the improvements.
Execution time for the same script goes down from 54 seconds to 42 seconds:
```plain execution times with 2.7 driver
creating 100000 documents
creating documents took 42.886090040207 s</p>

<p>creating 100000 documents
creating documents took 42.578990936279 s
```</p>

<p>The PHP version used here was:
```plain PHP version details
PHP 5.5.12-2ubuntu4.6 (cli) (built: Jul  2 2015 15:27:14)
Copyright &copy; 1997-2014 The PHP Group
Zend Engine v2.5.0, Copyright &copy; 1998-2014 Zend Technologies</p>

<pre><code>with Zend OPcache v7.0.4-dev, Copyright (c) 1999-2014, by Zend Technologies
</code></pre>

<p>```</p>

<p>Following are the results from a different machine, this time using PHP 5.6:
```plain execution times with 2.6 driver
creating 100000 documents
creating documents took 48.394731044769 s</p>

<p>creating 100000 documents
creating documents took 47.618598937988 s
```</p>

<p>```plain execution times with 2.7 driver
creating 100000 documents
creating documents took 40.535583972931 s</p>

<p>creating 100000 documents
creating documents took 40.041265010834 s
```</p>

<p>The PHP version details for this machine were:
```plain PHP version details
PHP 5.6.4-4ubuntu6.2 (cli) (built: Jul  2 2015 15:29:28)
Copyright &copy; 1997-2014 The PHP Group
Zend Engine v2.6.0, Copyright &copy; 1998-2014 Zend Technologies</p>

<pre><code>with Zend OPcache v7.0.4-dev, Copyright (c) 1999-2014, by Zend Technologies
</code></pre>

<p>```</p>

<p>The actual improvements depend on many factors, so your exact mileage may vary.
The improvements may not be noticable for applications that issue only a few
requests with the driver, but they will be significant when performing lots of
requests, as in the above examples.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Throughput Enhancements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/07/30/throughput-enhancements/"/>
    <updated>2015-07-30T12:54:12+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/07/30/throughput-enhancements</id>
    <content type="html"><![CDATA[<p>We&rsquo;ve recently been working on improving ArangoDB&rsquo;s throughput,
especially when using the ArangoDB&rsquo;s interface.</p>

<p>In this post, I will show some of the improvements already achieved,
though the work is not yet finished. Therefore, the results shown here
are still somewhat preliminary.</p>

<!-- more -->


<p>We wanted to measure improvements for ArangoDB&rsquo;s HTTP interface, and so we used
<a href="https://github.com/wg/wrk"><em>wrk</em></a> as an external HTTP load generator.</p>

<p>During the tests, <em>wrk</em> called some specific URLs on a local ArangoDB instance on
an otherwise idle machine. The test was run with ArangoDB 2.6 and <code>devel</code>. The
ArangoDB instances were started with their default configuration.</p>

<p><em>wrk</em> was invoked with varying amounts of client connections and threads, so the
tests cover serial and concurrent/parallel requests:</p>

<p><code>bash invoking wrk
wrk -c $CONNECTIONS -t $THREADS -d 10 $URL
</code></p>

<p>The number of connections (<code>$CONNECTIONS</code>) and theads (<code>$THREADS</code>) were both varied
from 1 to 8. <em>wrk</em> requires at least as many connections as threads.</p>

<p>The first URL tested was a route in a simple Foxx application that inserts the data
shipped in the HTTP request into a collection on the server. The internals of the
route should not matter here, as this post focuses on the throughput improvements.</p>

<p>Following are the results for calling the route with <em>wrk</em>, comparing the stable
ArangoDB version (2.6.3) with the current development version (head of <code>devel</code> branch
as of today). The table shows the number of documents that were inserted during the
10 seconds the <em>wrk</em> client ran:</p>

<p>```plain test results for 1 thread</p>

<h2>Threads      Connections        2.6       devel</h2>

<pre><code>  1                1      12569       20157 
  1                2      28094       36031   
  1                4      46310       66524 
  1                8      46798       80667
</code></pre>

<p>```</p>

<p>As can be seen above, <code>devel</code> was able to handle much more requests than 2.6 even
with a single connection (i.e. serial client requests). Throughput was about 60 %
higher for this case.</p>

<p>When increasing the number of client connections, the number of requests handled by
<code>devel</code> ws also higher than that of 2.6, with improvements between around 25 and 70 %.</p>

<p>When increasing the number of client load generation threads, the picture doesn&rsquo;t
change much. Here&rsquo;s the full table of results:</p>

<p>```plain full test results</p>

<h2>Threads      Connections        2.6       devel</h2>

<pre><code>  1                1      12569       20157 
  1                2      28094       36031   
  1                4      46310       66524 
  1                8      46798       80667

  2                2      28931       36326    
  2                4      47181       67654    
  2                8      47594       88617 

  4                4      46553       67585   
  4                8      47531       86935 

  8                8      46431       91953 
</code></pre>

<p>```</p>

<p>The next test consisted of inserting documents into a collection again, but using the
built-in HTTP API for creating documents instead of a user-defined Foxx application.
Throughput is expected to be higher than in the Foxx case because the built-in method
is hard-wired and only serves a single purpose, whereas the Foxx route is user-definable
and capable of doing fancy things, such as validating data, restricting access etc.</p>

<p>Here are the results for calling the hard-wired insertion route, again for 2.6 and <code>devel</code>:</p>

<p>```plain full test results</p>

<h2>Threads      Connections        2.6       devel</h2>

<pre><code>  1                1     102133      112843 
  1                2     185529      210795 
  1                4     335607      373070
  1                8     518354      576034

  2                2     181237      196482 
  2                4     345455      363255
  2                8     474558      550835

  4                4     318331      355328
  4                8     483388      516100

  8                8     482369      527395
</code></pre>

<p>```</p>

<p><code>devel</code> provides higher throughput than 2.6 for this route as well. Improvements fell
into the range of between 5 and 15 %. That&rsquo;s not as impressive as in the Foxx route
case above, but still a welcome improvement.</p>

<p>And of course we&rsquo;ll try to improve the throughput further.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding Up Array/object Literal Access]]></title>
    <link href="http://jsteemann.github.io/blog/2015/06/15/speeding-up-array-slash-object-literal-access/"/>
    <updated>2015-06-15T15:56:27+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/06/15/speeding-up-array-slash-object-literal-access</id>
    <content type="html"><![CDATA[<p>Last week some further optimization slipped into 2.6. The optimization can
provide significant speedups in AQL queries using huge array/object bind parameters
and passing them into V8-based functions.</p>

<!-- more -->


<p>It started with an ArangoDB user reporting a specific query to run unexpectedly slow.
The part of the query that caused the problem was simple and looked like this:</p>

<p><code>plain problematic query
FOR doc IN collection
  FILTER doc.attribute == @value
  RETURN TRANSLATE(doc.from, translations, 0)
</code></p>

<p>In the original query, <code>translations</code> was a big, constant object literal. Think of
something like the following, but with a lot more values:</p>

<p><code>json example translations value
{
  "p1" : 1,
  "p2" : 2,
  "p3" : 40,
  "p4" : 9,
  "p5" : 12
}
</code></p>

<p>The translations were used for replacing an attribute value in existing documents
with a lookup table computed outside the AQL query.</p>

<p>The number of values in the <code>translations</code> object was varying from query to query,
with no upper bound on the number of values. It was possible that the query was
running with 50,000 lookup values in the <code>translations</code> object.</p>

<p>When trying to reproduce the problem, we expected that the query would get at worst
<em>linearly</em> slower with an increasing number of lookup values. But in reality, the
following <em>non-linear</em> execution times were observed when increasing the number of
lookup values:</p>

<p>```plain execution times for varying input sizes, without optimization</p>

<h1>of values |  execution time</h1>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</p>

<pre><code>      1 |        0.6111 s
      2 |        0.6078 s  
      4 |        0.6021 s
      8 |        0.6160 s
     16 |        0.6925 s
     32 |        0.7107 s
     64 |        0.7677 s
    128 |        0.8576 s
    256 |        1.0544 s
    512 |        1.4579 s
   1024 |        8.8303 s
   2048 |       17.3674 s
   4096 |       35.3109 s
   8192 |       74.9161 s
  16384 |      145.0837 s
  32768 |      361.9870 s
  65536 |      880.4995 s
</code></pre>

<p>```</p>

<p>(note: all values stated above are wall-clock times for running the query with a
FILTER condition matching 50,000 documents &ndash; i.e. the <code>TRANSLATE()</code> expression was
executed 50,000 times per query)</p>

<p>With small objects passed in <code>translate</code>, the execution times only increased slowly
even when object sizes were doubled. The <code>TRANSLATE()</code> expression&rsquo;s share of the
overall query execution time was still low for small objects, even when doubling
their sizes. However, it got pretty bad for objects with 1,024 members already, and
from that point on, execution times more than doubled if object sizes got doubled.</p>

<p>The <code>TRANSLATE()</code> function itself has O(1) complexity, so we could rule it out as
the problem cause. However, <code>TRANSLATE()</code> is V8-based, and it turned out that there
was a problem when the number of values in the <code>translations</code> object increased from
1022 to 1023. At that particular threshold, execution time quadrupled.</p>

<p>At 1023 object members, V8 seems to change the internal object format, which probably
requires rearranging the object data internally. V8 has several <em>internal</em> types for
representing JavaScript objects, and converting between them is not free.</p>

<p>The obvious optimization opportunity for this case was to create the <code>translations</code>
object value just once as a V8 object, and reuse the same object when calling the
<code>TRANSLATE()</code> function repeatedly. This avoids repeated creation and destruction of
the V8 objects used in function calls, and as a side effect may also lead to less garbage
values being accumulated when functions are called repeatedly.</p>

<p>The optimization is possible here because the <code>translations</code> object is an object literal
and thus constant. It will also work for array literals and bind parameters (which
are also treated as literals once their values are known).</p>

<p>Here are the execution time for running the <code>TRANSLATE()</code> on 50,000 documents with the
modification:</p>

<p>```plain execution times, with optimization</p>

<h1>of values |  execution time</h1>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</p>

<pre><code>      1 |        0.6251 s
      2 |        0.6302 s  
      4 |        0.6138 s
      8 |        0.6141 s
     16 |        0.6685 s
     32 |        0.6232 s
     64 |        0.6204 s
    128 |        0.6326 s
    256 |        0.6460 s
    512 |        0.6275 s
   1024 |        0.6639 s
   2048 |        0.6345 s
   4096 |        0.6554 s
   8192 |        0.6789 s
  16384 |        0.7569 s
  32768 |        0.7636 s
  65536 |        1.0173 s
</code></pre>

<p>```</p>

<p>Looks like this is going to scale way better.</p>

<p>The optimization is disabled for big array/objects which are non-constant (e.g. a variable
or the result of an expression), or for parameters passed into user-defined AQL functions.
Enabling it for user-defined AQL functions is not safe because in theory these might
modify their arguments (and function arguments are passed by reference &ndash; passing them
by value would also defeat the purpose of the optimization).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Getting Unique Values]]></title>
    <link href="http://jsteemann.github.io/blog/2015/06/01/on-getting-unique-values/"/>
    <updated>2015-06-01T13:27:13+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/06/01/on-getting-unique-values</id>
    <content type="html"><![CDATA[<p>While paging through the issues in the <a href="https://github.com/arangodb/arangodb">ArangoDB issue tracker</a>
I came across <a href="https://github.com/arangodb/arangodb/issues/987">issue #987</a>, titled
<em>Trying to get distinct document attribute values from a large collection fails</em>.</p>

<p>The issue was opened around 10 months ago when ArangoDB 2.2 was around. We improved AQL performance
somewhat since then, so I was eager to see how the query would perform in ArangoDB 2.6, especially
when comparing it to 2.2.</p>

<!-- more -->


<p>For reproduction I quickly put together some example data to run the query on:
<code>js setting up example data
var db = require("org/arangodb").db;
var c = db._create("test");
for (var i = 0; i &lt; 4 * 1000 * 1000; ++i) {
  c.save({ _key: "test" + i, value: (i % 100) });
}
require("internal").wal.flush(true, true);
</code></p>

<p>This produces a collection named <code>test</code> with 4 million documents. Each document has a numeric <code>value</code>
attribute, which in total has 100 unique values. I remembered from a conversation with the guy that
opened the issue that the number of distinct values was 100 or even slightly lower. I didn&rsquo;t bother
to create an index on the <code>value</code> attribute, which might have sped up the query.</p>

<p>With data available, it was time to run the query and measure its execution time:
<code>js running the query
var time = require("internal").time;
var start = time();
db._query("FOR doc IN test COLLECT value = doc.value RETURN value");
time() - start;
</code></p>

<p>Running this in 2.2.7 took 3 minutes and 18 seconds before bursting with the following error message:</p>

<p>```plain 2.2.7 error message
#</p>

<h1>Fatal error in CALL_AND_RETRY_2</h1>

<h1>Allocation failed &ndash; process out of memory</h1>

<p>#
```</p>

<p>In the 2.2 branch AQL queries were fully transformed to JavaScript and executed using V8. Obviously
that didn&rsquo;t work well with large collections. That was one of the reasons why version 2.3 saw a major
rewrite of the AQL engine.</p>

<p>As a consequence, running the query in 2.3 (2.3.5) worked fine. Execution took around 28 seconds.
The same was true for 2.4 (2.4.8) and 2.5 (2.5.5).</p>

<p>Finally, running the query in 2.6.0 completed in just 3.2 seconds.</p>

<p>The reasons for the speedup are the optimizations done for <code>COLLECT</code> (see
<a href="/blog/2015/04/22/collecting-with-a-hash-table/">COLLECTing with a hash table</a>, the
<a href="/blog/2015/05/04/return-value-optimization-for-aql/">Return value optimization for AQL</a> and some
minor optimizations within AQL that didn&rsquo;t get a pretty working title.</p>

<p>Looks like in sum all the optimizations put into 2.6 really pay out.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Great AQL Shootout: ArangoDB 2.5 vs 2.6]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/20/the-great-aql-shootout-arangodb-25-vs-26/"/>
    <updated>2015-05-20T18:04:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/20/the-great-aql-shootout-arangodb-25-vs-26</id>
    <content type="html"><![CDATA[<p>We are currently preparing ArangoDB 2.6 for release. A lot of work has been put into this release,
and I really hope we can ship a first 2.6 release soon.</p>

<p>To keep you hanging on in the meantime, I put together some performance tests results from 2.6.
The tests I ran compared AQL query execution times in 2.6 and 2.5.</p>

<p>The results look quite promising: <strong>2.6 outperformed 2.5 for all tested queries</strong>, mostly by
factors of 2 to 5. A few dedicated AQL features in the tests got boosted even more, resulting in
query execution time reductions of 90 % and more.
Finally, the tests also revealed a dedicated case for which 2.6 provides a several hundredfold speedup.</p>

<p>Also good news is that not a single of the test queries ran slower in 2.6 than in 2.5.</p>

<!-- more -->


<h1>What was tested?</h1>

<p>The tests execute several read-only AQL queries on datasets of different sizes and measure the
query execution times. The tests were conducted in both ArangoDB 2.5 (2.5.4, the current stable version)
and 2.6 (2.6.0-alpha2, the upcoming version), so the results of the two ArangoDB versions can be compared.</p>

<p>Though the tests do not cover every possible type of AQL operation, feature and function, they still do
cover a wide range of features, e.g. lookups, joins, COLLECT operations, sorting, subqueries,
and some AQL functions. Overall, the test suite contains 33 different cases.</p>

<p>All queries were run on datasets of three different sizes to validate that the results are relevant
for datasets of various sizes. The dataset sizes are 10,000 documents, 100,000 documents, and 1,000,000
documents. Each query was repeated a few times so outliers in execution time can be identified.</p>

<p>There is full disclosure of the test methodology and the test script below, so anyone interested
can repeat the tests locally and verify the results.</p>

<h1>Test results</h1>

<p>The combined test results from 2.5 and 2.6 can be found in this
<a href="/downloads/code/arango-25-26-shootout-results.pdf">PDF file</a>.
There is also an <em>ods</em> version of the same file <a href="/downloads/code/arango-25-26-shootout-results.ods">here</a>.
A description of the columns and test cases used in these files can be found further below.</p>

<p>For the detail-loving folks, here are the raw results for both versions in isolation:
<a href="/downloads/code/arango-25-26-shootout-results-25.txt">2.5</a>,
<a href="/downloads/code/arango-25-26-shootout-results-26.txt">2.6</a>.</p>

<p>The results show that ArangoDB 2.6 was consistently faster for <strong>all</strong> AQL queries included in the
tests.</p>

<p>The queries that improved most in 2.6 over 2.5 include:</p>

<ul>
<li><code>FILTER</code> conditions: simple <code>FILTER</code> conditions as used in the tests are 3 to 5 times faster</li>
<li>simple joins using the primary index (<code>_key</code> attribute), hash index or skiplist index are
2 to 3.5 times faster</li>
<li>sorting on a string attribute is 2.5 to 3 times faster</li>
<li>extracting the <code>_key</code> or other top-level attributes from documents is 4 to 5 times faster</li>
<li><code>COLLECT</code> statements: simple <code>COLLECT</code> statements like the ones in the tests are 7 to 15 times
faster</li>
<li>looking up documents using <code>IN</code> lists with a substantial amount of values contained in the <code>IN</code>
list is 250 to 700 times faster</li>
</ul>


<p>The one thing that did not change much when comparing 2.6 with 2.5 is iterating over a collection
and returning all its documents unmodified. The speedups observed for this type of query are between
18 and 25 %, which is the lowest speedup measured by the tests. Still 18 to 25 % seem okay
as a free take-away.</p>

<p>Speedups were observed for all three test dataset sizes alike. In some cases, the speedups
varied a bit with the dataset sizes, but it was still in the same ballpark for all three datasets.
The conclusion is thus that the speedups did not depend much on the dataset sizes.</p>

<h1>Reasons for speedups</h1>

<p>There are several reasons why the 2.6 performance is better than in previous versions. The main
reason is that we spent much time optimizing some of the crtical AQL code paths. Then we also
worked on optimizations for specific features, which are used by some of the tested queries.</p>

<p>If you&rsquo;re interested in the details, here they are:</p>

<ul>
<li><a href="/blog/2015/04/22/collecting-with-a-hash-table/">COLLECTing with a hash table</a></li>
<li><a href="/blog/2015/04/23/aql-functions-improvements/">AQL functions improvements</a></li>
<li><a href="/blog/2015/05/04/subquery-optimizations/">Subquery optimizations</a></li>
<li><a href="/blog/2015/05/04/return-value-optimization-for-aql/">Return value optimization for AQL</a></li>
<li><a href="/blog/2015/05/07/in-list-improvements/">IN-list improvements</a></li>
</ul>


<p>Additionally, UTF-8 string comparisons were boosted by the upgrade from ICU 52 to ICU 54. The
latter version contains a rewritten and much faster UTF-8-aware strcmp, which we heavily rely on.</p>

<h1>Test methodology</h1>

<p>Each query was run five times on each dataset, so execution time outliers can be identified. The
results contain the minimum, maximum and average execution times for each query.</p>

<p>Queries were run in isolation on an otherwise idle server. The queries were all run inside the
server, so there was no HTTP/network traffic involved for shipping query results (note: this
was also <a href="/blog/2015/04/01/improvements-for-the-cursor-api/">vastly improved in 2.6</a> but this is
not the subject of this post).</p>

<p>All tests were run on my local machine, which has 4 cores, 8 CPUs (though the number of CPUs will
not matter for any of the tests), 12 GB of physical memory, a Linux 3.16 kernel and an Ubuntu 15
OS. All datasets fit into the main memory, so tests were not I/O-bound.</p>

<p>The ArangoDB versions tested were 2.5.4 and 2.6.0-alpha2. Both versions were hand-compiled with
g++ 4.9.1 with options <code>CC='gcc' CXX='g++' CFLAGS='-O3 -Wall' CXXFLAGS='-O3 -Wall'</code>.</p>

<p>The complete test script, including the setup of the test data, is contained in
<a href="/downloads/code/arango-25-26-shootout-script.js">this file</a>. It can be run inside <em>arangod</em> by
typing the following in the server console:</p>

<p><code>js running the tests inside arangod
require("internal").load("/path/to/arango-25-26-shootout-script.js");
</code></p>

<p>Note that this needs an <em>arangod</em> started with option <code>--console</code>. Also note that running the
script will only test the current <em>arangod</em> instance, so the script needs to be run once in a
2.5 instance and once in 2.6.</p>

<p>Running the script will set up the test collections, run all queries on them (you will need some
patience for this) and finally print a table like the following:</p>

<p>```plain excerpt from test results</p>

<h2>test name                     | collection  |    runs |     min (s) |     max (s) |     avg (s)</h2>

<p>collect-number                | 10k         |       5 |      0.0760 |      0.1638 |      0.0943
collect-number                | 100k        |       5 |      0.8697 |      0.8966 |      0.8803
collect-number                | 1000k       |       5 |     10.4320 |     10.6597 |     10.5314
collect-string                | 10k         |       5 |      0.1211 |      0.1319 |      0.1250
collect-string                | 100k        |       5 |      1.5406 |      1.5974 |      1.5641
collect-string                | 1000k       |       5 |     19.0708 |     19.0966 |     19.0825
collect-count-number          | 10k         |       5 |      0.0763 |      0.0792 |      0.0778
```</p>

<p>These result columns have the following meanings:</p>

<ul>
<li><em>test name</em>: name of test</li>
<li><em>collection</em>: name of collection. <em>10k</em> is a collection with 10,000 documents, <em>100k</em> contains
 100,000 documents, and <em>1000k</em> contains 1,000,000 documents.</li>
<li><em>runs</em>: number of times the query was run</li>
<li><em>min (s)</em>: minimum query execution time (in seconds)</li>
<li><em>max (s)</em>: maximum query execution time (in seconds)</li>
<li><em>avg (s)</em>: average query execution time (in seconds)</li>
</ul>


<h1>Test data</h1>

<p>The test datasets for the three collections are filled with artifical data. Test documents are
created like this:</p>

<p><code>js test document creation
collection.insert({
  _key: "test" + i,
  value1: i,
  value2: "test" + i,
  value3: i,
  value4: "test" + i,
  value5: i,
  value6: "test" + i,
  value7: i % g,
  value8: "test" + (i % g)
});
</code></p>

<p>Each document has a <code>_key</code> attribute and 8 other attributes, <code>value1</code> to <code>value8</code>.</p>

<p><code>value1</code>, <code>value3</code>, <code>value5</code> and <code>value7</code> are numeric attributes, the other attributes contain
string values. The attributes <code>value1</code> to <code>value6</code> contain unique values. The attributes <code>value7</code>
and <code>value8</code> contain repeating values. They are used for <code>COLLECT</code> queries.</p>

<p><code>value1</code> and <code>value2</code> are each indexed with a hash index. <code>value3</code> and <code>value4</code> are each indexed with
a skiplist index. <code>value5</code> to <code>value8</code> are not indexed. This way queries can be run on the same values,
but with different indexes and even without indexes.</p>

<h1>Test cases</h1>

<p>The test cases cover the following queries:</p>

<ul>
<li><em>collect-number</em> and <em>collect-string</em>: run <code>COLLECT</code> on a repeating attribute, which is either
numeric or a string</li>
<li><em>collect-count-number</em> and <em>collect-count-string</em>: ditto, but also calculate the group lengths
using <code>WITH COUNT INTO</code></li>
<li><em>subquery</em>: run a single-document subquery for each document of the original collection</li>
<li><em>concat</em>: for each document in the collection, concat the document <code>_key</code> attribute with another
 document attribute using <code>CONCAT()</code></li>
<li><em>merge</em>: for each document in the collection, merge the document with another object using <code>MERGE()</code></li>
<li><em>keep</em>: for each document in the collection, remove all but a few named attributes from it using
<code>KEEP()</code></li>
<li><em>unset</em>: for each document in the collection, remove a few named attributes from it using <code>UNSET()</code></li>
<li><em>min-number</em> and <em>min-string</em>: return the minimum value of a specific attribute from all documents in
 the collection, which is either numeric or a string. This uses <code>MIN()</code></li>
<li><em>max-number</em> and <em>max-string</em>: ditto, but using <code>MAX()</code></li>
<li><em>sort-number</em> and <em>sort-string</em>: sort all documents in the collection by a non-indexed attribute,
 which is either numeric or a string</li>
<li><em>filter-number</em> and <em>filter-string</em>: filter all documents in the collection using a non-indexed attribute,
 which is either numeric or a string</li>
<li><em>extract-doc</em>: return all documents in the collection unmodified</li>
<li><em>extract-key</em>: return the <code>_key</code> attribute of all documents in the collection</li>
<li><em>extract-number</em> and <em>extract-string</em>: return an attribute from all documents in the collection,
 which is either numeric or a string</li>
<li><em>join-key</em>: for each document in the collection, perform a join on the <code>_key</code> attribute on the collection
 itself (i.e. <code>FOR c1 IN @@c FOR c2 IN @@c FILTER c1._key == c2._key RETURN c1</code>)</li>
<li><em>join-id</em>: ditto, but perform the join using the <code>_id</code> attribute</li>
<li><em>join-hash-number</em> and <em>join-hash-string</em>: ditto, but join using a hash index on a numeric or string
 attribute</li>
<li><em>join-skiplist-number</em> and <em>join-skiplist-string</em>: ditto, but join using a skiplist index on a numeric or
 string attribute</li>
<li><em>lookup-key</em>, <em>lookup-hash-number</em>, <em>lookup-hash-string</em>, <em>lookup-skiplist-number</em>, <em>lookup-skiplist-string</em>:
 compile an IN-list of 10,000 lookup values and search these 10,000 documents in the collection using
 either the primary index (<code>_key</code> attribute), a hash index or a skiplist index. The latter two are tested
 on numeric and string attributes.</li>
</ul>


<p>Further implementation details can be checked in the <a href="/downloads/code/arango-25-26-shootout-script.js">test script</a>.</p>
]]></content>
  </entry>
  
</feed>
