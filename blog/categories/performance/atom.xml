<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2014-10-17T02:37:17+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Understanding Where Operations Are Executed]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/30/understanding-where-operations-are-executed/"/>
    <updated>2014-08-30T22:38:42+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/30/understanding-where-operations-are-executed</id>
    <content type="html"><![CDATA[<p>I recently had to deal with some data processing operation that took
about 20 minutes to complete. When looking into this, I found that the
easiest and most beneficial change to the whole setup was to make the
operation a <em>server-side</em> operation instead of executing it <em>client-side</em>.</p>

<p>This change reduced the operation&rsquo;s total execution time to a few seconds.</p>

<!-- more -->


<p>I can&rsquo;t show the original processing task here, so I&rsquo;ll start with a
contrived example. Imagine the following <em>for</em> loop inserting 100K documents
into a collection named <code>test</code>:
<code>js inserting 100k documents
for (i = 0; i &lt; 100000; ++i) {
  db.test.save({ value: i });
}
</code>
Now we only need a client application to execute the operation. As I don&rsquo;t
have a presentable client application right now, I will use the ArangoShell as
my client application.</p>

<h2>What&rsquo;s in a for loop?</h2>

<p>Running the above <em>for</em> loop inside the ArangoShell will lead to the loop being
executed inside the <em>arangosh</em> process.</p>

<p>In order to save a document in the collection, arangosh (our client) must make a
call to the ArangoDB server. This means issuing an HTTP POST request
to the server&rsquo;s REST API at <code>/_api/document/?collection=test</code>.
The server process will receive this request, insert the document, and
respond with an HTTP status code 201 or 202 to our client.
The client will then continue the loop until all documents have been inserted.</p>

<p>Now it&rsquo;s easy to see that the simple 3-line loop will issue 100,000 HTTP requests
in total. This means lots of data being pushed through the network stack(s).
It is pretty easy to imagine that this will come at a cost.</p>

<p>If we instead execute the above loop directly inside the ArangoDB server, we
can get rid of all the network overhead. The server has no need to send HTTP
calls to itself. It can simply execute the 100K inserts and is then done.
We therefore assume the loop to run somewhat faster when executed server-side.</p>

<p>A quick test on a crap laptop produced the following execution times for running
the loops:</p>

<ul>
<li>server-side execution (arangod): 1.34 seconds</li>
<li>client-side execution (arangosh): 17.32 seconds</li>
</ul>


<p><strong>Ouch</strong>. It looks like the client-server request-response overhead matters.</p>

<p>The following sections deal with how to get rid of some or even all the
client-server ping pong.</p>

<h2>Graph traversals</h2>

<p>The above <em>for</em> loop example was contrived, but imagine running
a client-side graph traversal instead. In fact, the original problem mentioned
in the introduction has been a graph traversal.</p>

<p>The problem of a graph traversal is that is often iterative and highly
dynamic. Decisions are made during the traversal as nodes are encountered,
leading to dynamic inclusion or exclusion etc. This means that it makes sense to
process nodes and edges only when needed, at the point when they are visited.</p>

<p>Even if the client can employ some sort of caching for already visited
nodes, the client still needs to ask the server about each visited
node&rsquo;s connections at least once. Otherwise it could not follow them.</p>

<p>This normally means lots of requests and responses. Compare this to the
<em>single</em> request-response alternative in which a client kicks off a server-side
traversal, and finally receives the overal result once it is assembled.</p>

<p><strong>Conclusion</strong>: traversals on anything but very small graphs should be run server-side.
A server-side action (see below) is a good way to do this. Please note that
running a server-side traversal does not mean giving up flexibility and
control flow functionality. Server-side traversals remain highly configurable
through custom JavaScript functions that allow implementation of user-defined
business logic.</p>

<h2>AQL queries</h2>

<p>We won&rsquo;t have your application send a series of 100,000 individual
insert statements to the relational database of our choice. We already
know from the past that this is going to be rather slow, so we have
learned to avoid this. In the relational context, we rather use SQL queries
that create or modify many rows in one go, e.g. an <code>INSERT INTO ... SELECT ...</code>,
bulk inserts etc.</p>

<p>ArangoDB is no different. In general, you should try to avoid issuing lots
of individual queries to the database from a client application. Instead and if
the queries look alike, try converting multiple individual operations into a
single AQL query. This will already save a lot of network overhead.</p>

<p>AQL provides multi-document operations to insert, update, and remove data. An
overview is given <a href="http://docs.arangodb.org/Aql/DataModification.html">here</a>.</p>

<p>The above 100K inserts from the contrived example can easily be transformed
into this single AQL query:
<code>
FOR i IN 1..100000 INSERT { value: i } INTO test
</code></p>

<h2>Bulk imports</h2>

<p>For importing larger amounts of documents from files, there is the specialized
<a href="http://docs.arangodb.org/Arangoimp/README.html">arangoimp</a> import tool. It can
load data from JSON and CSV files into ArangoDB. The tool is shipped with
ArangoDB.</p>

<p>ArangoDB also provides a REST API for <a href="http://docs.arangodb.org/HttpBulkImports/README.html">bulk imports</a>
of documents.</p>

<h2>Joins</h2>

<p>A special note about <em>joins</em>: the fact that several NoSQL databases do not
provide join functionality has driven some people to emulate join functionality
on the client-side, in their applications.</p>

<p>This can be a recipe for disaster: client-side join implementation might lead
to horrendous amounts of queries that might need to be sent to the database for
fetching all the records. More than that, if data are queried individually,
the overall result may lack consistency. By the way, the same is true for
fetching referenced or linked documents.</p>

<p>ArangoDB provides join functionality via AQL queries. Additionally, AQL queries
can be used to fetch other documents with the original documents. Note that
ArangoDB has no way of defining references or links between documents, but
still AQL allows combining arbitrary documents in one query.</p>

<p>In almost all cases it make more sense to use an AQL query that performs
joins or reference-fetching server-side and close to the data than having to
deal with that on the application-side of things.</p>

<p>AQL joins are described <a href="http://docs.arangodb.org/AqlExamples/Join.html">here</a>.</p>

<h2>Server-side actions</h2>

<p>With <em>stored procedures</em>, relational databases provide another way for an
application to trigger the execution of a large amount of queries. Stored
procedures are executed server-side, too, so they allow avoiding a lot of
request-response ping pong between the application and the database, at least
for defined tasks. Additionally, stored procedures provide control flow
functionality, which can also be handy when operations depend on each other.</p>

<p>Coming back to ArangoDB: complex data-processing tasks that need to execute
multiple operations or need control flow functionality might benefit if
converted from multiple application-side operations into a single server-side
action.</p>

<p>Server-side actions run inside the ArangoDB server, closer to the data, and
can be much faster than a series of client-side operations.
A server-side action is called with just one HTTP request from the application,
so it may lead to saving lots of request-response cycles and reduction in
network overhead. Apart from that, server-side actions in ArangoDB can employ
transactions and provide the necessary control over isolation and atomicity
when executing a series of operations.</p>

<p>Business logic and control flow functionality can be integrated
easily because server-side actions in ArangoDB are JavaScript functions,
with all of the language&rsquo;s programming features being available.</p>

<p>But there&rsquo;s even more to it: a single server-side operation can be written
to put together its result in a format most convenient for the client
application. This can also lead to better encapsulation, because all an
application needs to know about a server-side action is its API or contract.
Any internals of the action can be hidden from the client application. Overall,
this supports a service-oriented approach.</p>

<p>To learn more about how to write server-side actions, please have a look
at ArangoDB&rsquo;s <a href="http://docs.arangodb.org/Foxx/README.html">Foxx</a>. It is all
about making server-side actions available via REST APIs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding Up Server-side Operations]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/20/speeding-up-server-side-operations/"/>
    <updated>2014-08-20T22:02:09+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/20/speeding-up-server-side-operations</id>
    <content type="html"><![CDATA[<p>Sometimes it is easier to make server-side operations run a bit faster.
In the following post I&rsquo;ll show a few low-level optimizations that can
be applied to user-defined JavaScript code that is executed inside the
ArangoDB server.</p>

<!-- more -->


<h1>Scope</h1>

<p>Some data-access operations can be sped up by using the appropriate indexes,
but that&rsquo;s not what I am going to show here.
Instead, I want to demo a few easy optimizations that don&rsquo;t require any
changes to the data. Only JavaScript code needs to be minimally adjusted
to use them.</p>

<p>Note that I am not talking about code that is executed in the ArangoShell
here. I will only be looking at code that is executed inside the arangod
server instance. The natural places for using custom JavaScript code in
the ArangoDB server are for example:</p>

<ul>
<li><a href="http://docs.arangodb.org/Foxx/README.html">Foxx</a> controllers</li>
<li><a href="http://docs.arangodb.org/Transactions/TransactionInvocation.html">transactions</a></li>
<li><a href="http://docs.arangodb.org/ModuleTasks/README.html">tasks</a></li>
<li><a href="http://docs.arangodb.org/Traversals/README.html">traversals</a></li>
</ul>


<p>Of course it does not make much sense to optimize operations that are not
called very often. The code changes I show will only be useful for server-side
operations that are called very often, for example, from within loops or
from batch processing actions.</p>

<p>Before starting to change any code, please make sure that the code is executed
often and that it accounts for a significant part of the total execution time.</p>

<h2>Baseline</h2>

<p>Imagine the following custom JavaScript code running somewhere inside ArangoDB:
<code>js baseline
for (var i = 0; i &lt; 100000; ++i) {
  db.test.save({ value: 1 });
}
</code>
This code inserts 100,000 documents into a collection <code>test</code>. Each document has
one attribute only. These numbers are arbitrary, but good enough for a demo.</p>

<p>What can we do to improve the runtime of the above code?</p>

<h2>Non-optimizations</h2>

<p>The <em>for</em> statement itself is not worth optimizing. It won&rsquo;t matter much if we used
pre-increment or post-increment for the loop induction variable <code>i</code> or if we
turned the <em>for</em> loop into a <em>while</em> loop. Any changes here might only save us a
few nanoseconds in total, but are likely to make the code more unreadable.</p>

<p>Let&rsquo;s not do that!</p>

<h2>Avoiding accessors</h2>

<p>Clearly, we should be looking at the <code>save</code> operation.</p>

<p><code>db.test.save()</code> looks like a function call, and we learned that function are
expensive. In this case, we cannot avoid the function call to <code>save()</code>, but we
can avoid another <em>hidden function call</em>. Yes, <code>db.test</code> actually calls a function,
though it does not look like it does.</p>

<p>The <code>db</code> object has auto-magic member attributes. The <code>db</code> object will have a
member attribute for existing collection. The member will automatically vanish when
a collection gets dropped, and the member will rename itself when collections are
renamed.</p>

<p>This magic is made possible by late-binding attributes and using accessor functions
for attribute accesses on the <code>db</code> object: whenever the attributes of the <code>db</code> object
are queried, an accessor function (<em>property query</em>) is called internally to compile
them. Accessing a specific attribute of the <code>db</code> object will also call an accessor
function (<em>property get</em>). This is exactly what happens in our case when we access
<code>db.test</code>.</p>

<p>If this was too complicated, it may become more obvious if we modified the original
code to this:
<code>js using attribute lookup
for (var i = 0; i &lt; 100000; ++i) {
  db['test'].save({ value: 1 });
}
</code>
Now it should be obvious that accessing <code>test</code> requires an attribute lookup on the
<code>db</code> object, and behind the scenes the same will happen if we had written <code>db.test</code>
instead.</p>

<p>Let&rsquo;s avoid the repeated call to the accessor function inside the loop! This can
easily be achieved by assigning <code>db.test</code> to a variable once and forever outside
of the loop. This technique is called loop-invariant code motion, and it can be
applied in a lot of other situations, too:
<code>js loop-invariant code motion
var collection = db.test;
for (var i = 0; i &lt; 100000; ++i) {
  collection.save({ value: 1 });
}
</code>
(on a side note: you cannot assign <code>db.test.save</code> to a variable and call it as a
function)</p>

<h2>Enjoying the silence</h2>

<p>The <code>save</code> operation is chatty. Every time it is called, it will return some meta
data from the just-inserted document, e.g.:
<code>json save result
{
  "_id" : "test/11443981931568",
  "_rev" : "11443981931568",
  "_key" : "11443981931568"
}
</code>
In our case, we&rsquo;re not interested in these returned values, and we don&rsquo;t
capture them in a variable.
The <code>save</code> function doesn&rsquo;t know this and will happily assemble its
result array. The array consists of three string values (six when also counting
attribute names). Setting up the result definitely requires costly
memory allocations and string copying.</p>

<p>We can avoid all this by passing an <em>options</em> parameter into <code>save</code>, and setting
its <code>silent</code> attribute to <code>true</code>:
<code>js silence option
for (var i = 0; i &lt; 100000; ++i) {
  db.test.save({ value: 1 }, { silent: true });
}
</code>
Now <code>save()</code> will only return a boolean value, which is much quicker.</p>

<h2>Transactions</h2>

<p>Yet another alternative is use to wrap the operations in the loop into a
transaction. Transaction themselves won&rsquo;t buy us much feature-wise, so why use
them? The reason is simple: if we do not use a transaction ourselves, each
<code>save</code> operation will implicitly be executed in a transaction of its own.
For a loop with 100,000 operations, that will be 100K transactions!</p>

<p>So when we put all the operations into a single, now explicit transaction,
we can save the overhead of 99,999 transaction begin and commit operations.
Here&rsquo;s how to do it:
```js transaction
db._executeTransaction({
  collections: {</p>

<pre><code>write: "test"
</code></pre>

<p>  },
  action: function () {</p>

<pre><code>for (var i = 0; i &lt; 100000; ++i) { 
  db.test.save({ value: 1 });
}
</code></pre>

<p>  }
});
```</p>

<h1>Results</h1>

<p>How far have we got with these minimal code adjustments?</p>

<p>I have put together a <a href="/downloads/code/speeding-up-server.js">script</a>
that can be run in arangod. The script will run each version of the loop
10 times and time the execution. The minimum, maximum and
average execution times are printed (in seconds, less is better). Note that
the absolute times do not matter much here. Please have a look at the percentage
column, which shows the execution time of each variant in comparison to the
baseline.</p>

<p>Here&rsquo;s an excerpt of the script&rsquo;s output:</p>

<p>```plain results</p>

<h2>test name      |   total (s) |     min (s) |     max (s) |    avg2 (s) |       %</h2>

<p>baseline       |     13.0940 |      1.2907 |      1.3357 |      1.3084 |  100.00
loop-invariant |     10.6888 |      1.0506 |      1.1042 |      1.0667 |   81.53
silence        |     11.7186 |      1.1512 |      1.2247 |      1.1678 |   89.25
transaction    |     10.1521 |      0.9987 |      1.0346 |      1.0149 |   77.56
combined       |      7.8545 |      0.7768 |      0.7977 |      0.7850 |   59.99</p>

<p>```
As can be seen, moving the loop-invariant accessor function call outside of the
loop provided an almost 20% speedup (from 1.30 to 1.06 s). Using the silence
option did also provide some, but not the same speedup. Using transactions reduced
the execution time, and by putting all this together, a reduction of about 40 %
was achieved.</p>

<p>Your mileage may vary. Please feel free to adjust the test script and run your
own tests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking ArangoDB's Write-ahead Log]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/07/benchmarking-arangodbs-write-ahead-log/"/>
    <updated>2014-08-07T01:27:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/07/benchmarking-arangodbs-write-ahead-log</id>
    <content type="html"><![CDATA[<h1>Motivation</h1>

<p>One of the major changes in ArangoDB 2.2 was the introduction of the
<em>write-ahead log</em> (abbreviated <em>WAL</em>).</p>

<p>The introduction of the WAL changed how documents are stored internally in
ArangoDB. A lot of things have been changed for it under the hood, and it has
been a lot of work to implement it.</p>

<p>During the implementation, we refactored some code parts and made them
considerably faster. From these changes we expected a positive effect on the
database performance. But due to the fact that shape information is now also
saved in the write-ahead log, there may also be some negative effect.</p>

<p>We developers were of course very interested in seeing the net effects, so
we ran some tests for a few use cases. We compared ArangoDB 2.1.2 (still without
the WAL) with ArangoDB 2.2.1 (with the WAL). The results are interesting.</p>

<!-- more -->


<h1>Test setup</h1>

<p>To get a broad overview of performance changes, we ran a few different test cases:</p>

<ul>
<li><strong>document</strong>: inserts a document</li>
<li><strong>crud</strong>: inserts a document, fetches it, updates it, and deletes it</li>
<li><strong>crud-append</strong>: inserts a document, fetches it, updates it, and fetches it again</li>
<li><strong>multi-collection</strong>: transactionally save two documents in two collections</li>
<li><strong>random-shapes</strong>: save documents with completely different structures/shapes</li>
</ul>


<p>All tests were run with the <code>arangob</code> benchmark tool, with various concurrency levels,
complexity settings and repeated several times. <code>arangob</code> was invoked like this:</p>

<p><code>
arangob                         \
  --test-case $case             \
  --request $requests           \
  --concurrency $concurrency    \
  --complexity $complexity
</code></p>

<p>Both ArangoDB servers and <code>arangob</code> were located on the same server. Only one
ArangoDB server was active during each test run, and the other was shut down so
it didn&rsquo;t compete for system resources.</p>

<h1>Test results</h1>

<p>Following are the results for the different test cases.</p>

<p>In the result tables, the columns have the following meanings:</p>

<ul>
<li><strong>Complexity</strong>: the number of attributes for the test documents, used as parameter
<code>--complexity</code> for <code>arangob</code></li>
<li><strong>Requests</strong>: the number of operations executed, used as parameter <code>--requests</code>
for <code>arangob</code></li>
<li><strong>Concurrency</strong>: the number of client threads started by <code>arangob</code>, used a parameter
<code>concurrency</code> for <code>arangob</code></li>
<li><strong>Time 2.1</strong>: test execution time (in seconds) for ArangoDB 2.1</li>
<li><strong>Time 2.2</strong>: test execution time (in seconds) for ArangoDB 2.2</li>
<li><strong>T2.1/T2.2</strong>: relative performance of ArangoDB 2.1 compared to ArangoDB 2.2. Values
less than one indicate that ArangoDB 2.1 was faster than ArangoDB 2.2. Values
greater than one indicate that ArangoDB 2.1 was slower than ArangoDB 2.2. A
value of one means that both versions had equal speed.</li>
</ul>


<p>Please note that the absolute execution times aren&rsquo;t too interesting in the
results shown here. We haven&rsquo;t used the most powerful server on earth for running
these tests. We were most interested in how ArangoDB 2.2 compared to
ArangoDB 2.1.</p>

<h2>Inserting documents</h2>

<p>The <code>document</code> test appends new documents (with an identical structure) to a
collection. Here are the results for inserting a million documents with 100
attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1       97.452       94.839         1.02    
   100       1000000               2       61.146       53.928         1.13   
   100       1000000               4       54.368       31.379         1.73  
</code></pre>

<p>```</p>

<p>Following are the results for 100,000 documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000               1       86.883       80.669         1.07 
  1000        100000               2       59.381       48.599         1.22 
  1000        100000               4       53.784       27.494         1.95 
</code></pre>

<p>```</p>

<p>The results for 5,000 documents with 10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1       43.991       40.577         1.08 
 10000          5000               2       31.403       26.117         1.20 
 10000          5000               4       28.713       20.044         1.43 
</code></pre>

<p>```</p>

<p>As we can see in the results above, ArangoDB 2.2 is faster than ArangoDB 2.1 for
all tested configurations. The difference is negligible if the test client is
single-threaded (one insert thread), but ArangoDB 2.2 is considerably faster than
ArangoDB 2.1 with more concurrent clients.</p>

<h2>CRUD operations (I)</h2>

<p>The <code>crud</code> test case inserts a document, fetches it, updates it, fetches it again
and finally deletes it. Executing 1 million crud operations on documents with 100
attributes each results in the following execution times:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1       66.856       67.072         0.99       
   100       1000000               2       47.907       47.043         1.01        
   100       1000000               4       42.089       42.047         1.00         
</code></pre>

<p>```</p>

<p>Running 100,000 crud operations on documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000               1      160.228      158.312         1.01 
  1000        100000               2      147.413      147.990         0.99  
  1000        100000               4      143.575      143.233         1.00   
</code></pre>

<p>```</p>

<p>And here are the test results for running 5,000 crud operations on documents with
10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1      645.859      643.924         1.00        
 10000          5000               2      640.414      640.767         0.99       
 10000          5000               4      637.438      637.132         1.00      
</code></pre>

<p>```</p>

<p>The results of all these tests show that performance for the tested workload hasn&rsquo;t
changed between ArangoDB 2.1 and 2.2. This is somewhat expected, as the WAL should
not affect the performance of read and delete operations much: reading a document
does not require writing to the WAL at all, and removing a document only requires
writing a very small remove marker to the WAL.</p>

<h2>CRUD operations (II)</h2>

<p>The <code>crud-append</code> test case inserts a document, fetches it, updates it, and fetches
it again. Executing 1 million crud operations on documents with 100 attributes each
results in the following execution times:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1       81.187       79.079         1.02 
   100       1000000               2       59.544       56.934         1.04  
   100       1000000               4       53.845       52.098         1.03   
</code></pre>

<p>```</p>

<p>Running 100,000 crud operations on documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000               1      200.057      196.722         1.01 
  1000        100000               2      182.436      181.633         1.00 
  1000        100000               4      181.233      180.631         1.00 
</code></pre>

<p>```</p>

<p>And here are the test results for running 5,000 crud operations on documents with
10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1      804.781      801.903         1.00 
 10000          5000               2      796.589      797.693         0.99  
 10000          5000               4      796.664      795.823         1.00  
</code></pre>

<p>```</p>

<p>Again, ArangoDB 2.1 and 2.2 are pretty much the same speed for the tested operations.</p>

<h2>Multi-collection write transactions</h2>

<p>The <code>multi-collection</code> test stores two documents in two different collections
transactionally. Here are the results for executing 100,000 transactions for
documents with 100 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100        100000               1     6936.895       30.736       225.69   
   100        100000               2     6946.524       25.577       271.59    
   100        100000               4     7720.682       24.334       317.27     
</code></pre>

<p>```</p>

<p>Executing 10,000 transactions on documents with 1,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000         10000               1      949.974       21.368        44.45     
  1000         10000               2      953.672       19.322        49.35    
  1000         10000               4      941.645       20.486        45.96   
</code></pre>

<p>```</p>

<p>And finally, the results for executing 500 transactions on documents with 10,000
attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000           500               1       46.776       10.413         4.49 
 10000           500               2       45.102       10.173         4.43 
 10000           500               4       44.172        9.192         4.80  
</code></pre>

<p>```</p>

<p>The above results show that ArangoDB 2.2 is much faster than ArangoDB 2.1 for
executing transactions that write to two collections.</p>

<p>We expected that! Multi-collection (write) transactions in ArangoDB 2.2 require
far less calls to <code>msync</code> than in ArangoDB 2.1. ArangoDB 2.2 can sync operations
of multiple transactions together in one call to <code>msync</code>. ArangoDB 2.1 needed to
synchronize each transaction separately.</p>

<h2>(Fully) heterogenous documents</h2>

<p>The <code>random-shapes</code> test inserts documents that have different structures each.
For each inserted document a new shape will need to be stored. Inserting one million
documents with 100 attributes each results in the following figures:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>   100       1000000               1      664.006       87.181         7.61        
   100       1000000               2      433.570       60.613         7.15       
   100       1000000               4      313.666       45.327         6.92      
</code></pre>

<p>```
The results show that ArangoDB 2.2 is considerably faster than ArangoDB 2.1 for
these cases &ndash; even with the storage overhead of writing the shapes to the WAL.</p>

<p>Now we inserted 100,000 documents with 1,000 document attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code>  1000        100000              1        61.709       66.954         0.92    
  1000        100000              2        39.294       46.458         0.84   
  1000        100000              4        36.866       40.910         0.90  
</code></pre>

<p>```
In these cases, the storage overhead of writing all shapes to the WAL seems to
start to matter, and ArangoDB 2.2 gets slower than ArangoDB 2.1 in this test
case.</p>

<p>The same was true when we inserted 5,000 documents with 10,000 attributes each:
```</p>

<h2>Complexity      Requests     Concurrency     Time 2.1     Time 2.2    T2.1/T2.2</h2>

<pre><code> 10000          5000               1       25.337       30.412         0.83    
 10000          5000               2       20.021       22.128         0.90     
 10000          5000               4       14.800       18.872         0.78      
</code></pre>

<p>```</p>

<p>Note that the <code>random-shapes</code> test case is an extreme test case. We do not
consider it realistic that all documents in a collection have completely different
attribute names. Still we included it in our tests because we were sure that this
would be a case in which the overhead of the WAL would be clearly measurable.</p>

<p>Note that there is a way to make ArangoDB 2.2 faster than ArangoDB 2.1 even for
this test case: setting the option <code>--wal.suppress-shape-information</code> to <code>true</code>
will make ArangoDB not write shape information to the WAL, making document write
operations much faster in case all documents have heterogenous structures.</p>

<p>The option won&rsquo;t help much if a shapes repeat a lot. In this case, the WAL overhead
shouldn&rsquo;t matter too much already, or ArangoDB 2.2 should already be faster than 2.1
(as shown in the results above).</p>

<h1>Summary</h1>

<p>The tests revealed that the overhead of the WAL in ArangoDB 2.2 seems to be
negligible for most of the tested workloads. In many cases, ArangoDB 2.2 with
the WAL is even faster than ArangoDB 2.1 that did not have a WAL at all.</p>

<p>There is one notable exception: when documents have fully heterogenous structures,
the overhead of the WAL is measurable and significant. Though we consider this
to be a rather hypothetical case, we have added the configurtion option
<code>--wal.suppress-shape-information</code>. This can be used to turn off storing
shape information in the WAL. This is not safe when the server is to be used
as a replication master, but should work in cases when replication is not used.</p>

<p>The option should not have a big effect if documents are greatly or at least
somewhat homogenous. For these cases, ArangoDB 2.2 with its WAL should already
be as fast as 2.1 (or even faster).</p>
]]></content>
  </entry>
  
</feed>
