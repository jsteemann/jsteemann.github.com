<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PHP | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/php/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-07T18:32:15+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Exporting Data for Offline Processing]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/24/exporting-data-for-offline-processing/"/>
    <updated>2015-04-24T15:47:31+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/24/exporting-data-for-offline-processing</id>
    <content type="html"><![CDATA[<p>A few weeks ago I wrote about ArangoDB&rsquo;s
<a href="https://jsteemann.github.io/blog/2015/04/04/more-efficient-data-exports/">specialized export API</a>.</p>

<p>The export API is useful when the goal is to extract all documents from a given collection
and to process them outside of ArangoDB.</p>

<p>The export API can provide quick and memory-efficient snapshots of the data in the underlying
collection, making it suitable for extract all documents of the collection. It will be able
to provide data much faster than with an AQL query that will extract all documents.</p>

<p>In this post I&rsquo;ll show how to use the export API to extract data and process it with PHP.</p>

<!-- more -->


<p>A prerequiste for using the export API is using an ArangoDB server with version 2.6
or higher. As there hasn&rsquo;t been an official 2.6 release yet, this currently requires
building the <code>devel</code> branch of ArangoDB from source. When there is a regular 2.6
release, this should be used instead.</p>

<h2>Importing example data</h2>

<p>First we need some data in an ArangoDB collection that we can process externally.</p>

<p>For the following examples, I&rsquo;ll use a collection named <code>users</code> which I&rsquo;ll populate
with 100k <a href="/downloads/code/users-100000.json.tar.gz">example documents</a>. Here&rsquo;s how
to get this data into ArangoDB:</p>

<p>```bash commands for fetching and importing data</p>

<h1>download data file</h1>

<p>wget <a href="https://jsteemann.github.io/downloads/code/users-100000.json.tar.gz">https://jsteemann.github.io/downloads/code/users-100000.json.tar.gz</a></p>

<h1>uncompress it</h1>

<p>tar xvfz users-100000.json.tar.gz</p>

<h1>import into ArangoDB</h1>

<p>arangoimp &mdash;file users-100000.json &mdash;collection users &mdash;create-collection true
```</p>

<p>There should now be 100K documents present in a collection named <code>users</code>. You can
quickly verify that by peeking into the collection using the web interface.</p>

<h2>Setting up ArangoDB-PHP</h2>

<p>An easy way of trying the export API is to use it from PHP. We therefore clone the
devel branch of the <strong>arangodb-php</strong> Github repository into a local directory:</p>

<p><code>bash cloning arangodb-php
git clone -b devel "https://github.com/arangodb/arangodb-php.git"
</code></p>

<p>Note: when there is an official 2.6 release, the <code>2.6</code> branch of arangodb-php should
be used instead of the <code>devel</code> branch.</p>

<p>We now write a simple PHP script that establishes a connection to the ArangoDB
server running on localhost. We&rsquo;ll extend that file gradually. Here&rsquo;s a skeleton
file to start with. The code can be downloaded <a href="/downloads/code/export-skeleton.php">here</a>:</p>

<p>```php skeleton file for establishing a connection
&lt;?php</p>

<p>namespace triagens\ArangoDb;</p>

<p>// use the driver&rsquo;s autoloader to load classes
require &lsquo;arangodb-php/autoload.php&rsquo;;
Autoloader::init();</p>

<p>// set up connection options
$connectionOptions = array(
  // endpoint to connect to
  ConnectionOptions::OPTION_ENDPOINT     => &lsquo;tcp://localhost:8529&rsquo;,
  // can use Keep-Alive connection
  ConnectionOptions::OPTION_CONNECTION   => &lsquo;Keep-Alive&rsquo;,         <br/>
  // use basic authorization
  ConnectionOptions::OPTION_AUTH_TYPE    => &lsquo;Basic&rsquo;,               <br/>
  // user for basic authorization
  ConnectionOptions::OPTION_AUTH_USER    => &lsquo;root&rsquo;,                    <br/>
  // password for basic authorization
  ConnectionOptions::OPTION_AUTH_PASSWD  => &lsquo;&rsquo;,                    <br/>
  // timeout in seconds
  ConnectionOptions::OPTION_TIMEOUT      => 30,
  // database name
  ConnectionOptions::OPTION_DATABASE     => &lsquo;_system&rsquo;
);</p>

<p>try {
  // establish connection
  $connection = new Connection($connectionOptions);</p>

<p>  echo &lsquo;Connected!&rsquo; . PHP_EOL;</p>

<p>  // TODO: now do something useful with the connection!</p>

<p>} catch (ConnectException $e) {
  print $e . PHP_EOL;
} catch (ServerException $e) {
  print $e . PHP_EOL;
} catch (ClientException $e) {
  print $e . PHP_EOL;
}
```</p>

<p>Running that script should simply print <code>Connected!</code>. This means the PHP script
can connect to ArangoDB and we can go on.</p>

<h2>Extracting the data</h2>

<p>With a working database connection we can now start with the actual processing.
In place of the <code>TODO</code> in the skeleton file, we can actually run an export of
the data in collection <code>users</code>. The following simple function extracts all
documents from the collection and writes them to an output file <code>output.json</code>
in JSON format.</p>

<p>It will also print some statistics about the number of documents and the total
data size. The full script can be downloaded <a href="/downloads/code/export.php">here</a>:</p>

<p>```php exporting data into a file
function export($collection, Connection $connection) {
  $fp = fopen(&lsquo;output.json&rsquo;, &lsquo;w&rsquo;);</p>

<p>  if (! $fp) {</p>

<pre><code>throw new Exception('could not open output file!');
</code></pre>

<p>  }</p>

<p>  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true       // use simple PHP arrays
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // statistics
  $count   = 0;
  $batches = 0;
  $bytes   = 0;</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';
foreach ($docs as $doc) {
  $output .= json_encode($doc) . PHP_EOL;
} 

// write out chunk
fwrite($fp, $output);

// update statistics
$count += count($docs);
$bytes += strlen($output);
++$batches;
</code></pre>

<p>  }</p>

<p>  fclose($fp);</p>

<p>  echo sprintf(&lsquo;written %d documents in %d batches with %d total bytes&rsquo;,</p>

<pre><code>           $count,
           $batches, 
           $bytes) . PHP_EOL;
</code></pre>

<p>}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<p>Running this version of the script should print something similar to the following
and also produce a file named <code>output.json</code>. Each line in the file should be a JSON
object representing a document in the collection.</p>

<p><code>plain script output
written 100000 documents in 20 batches with 40890013 total bytes
</code></p>

<h2>Applying some transformations</h2>

<p>We now use PHP to transform data as we extract it. With an example script, we&rsquo;ll apply
the following transformations on the data:</p>

<ul>
<li>rewrite the contents of the <code>gender</code> attribute:

<ul>
<li><code>female</code> should become <code>f</code></li>
<li><code>male</code> should become <code>m</code></li>
</ul>
</li>
<li>rename attribute <code>birthday</code> to <code>dob</code></li>
<li>change date formats in <code>dob</code> and <code>memberSince</code> from YYYY-MM-DD to MM/DD/YYYY</li>
<li>concatenate the contents of the <code>name.first</code> and <code>name.last</code> subattributes</li>
<li>transform array in <code>contact.email</code> into a flat string</li>
<li>remove all other attributes</li>
</ul>


<p>Here&rsquo;s a transformation function that does this, and a slightly simplified export
function. This version of the script can also be downloaded <a href="/downloads/code/export-transform.php">here</a>:</p>

<p>```php transformation and export functions
function transformDate($value) {
  return preg_replace(&lsquo;/^(\d+)&ndash;(\d+)&ndash;(\d+)$/&rsquo;, &lsquo;\2/\3/\1&rsquo;, $value);
}</p>

<p>function transform(array $document) {
  static $genders = array(&lsquo;male&rsquo; => &rsquo;m', &lsquo;female&rsquo; => &lsquo;f&rsquo;);</p>

<p>  $transformed = array(</p>

<pre><code>'gender'      =&gt; $genders[$document['gender']],
'dob'         =&gt; transformDate($document['birthday']),
'memberSince' =&gt; transformDate($document['memberSince']),
'fullName'    =&gt; $document['name']['first'] . ' ' . $document['name']['last'],
'email'       =&gt; $document['contact']['email'][0]
</code></pre>

<p>  );</p>

<p>  return $transformed;
}</p>

<p>function export($collection, Connection $connection) {
  $fp = fopen(&lsquo;output-transformed.json&rsquo;, &lsquo;w&rsquo;);</p>

<p>  if (! $fp) {</p>

<pre><code>throw new Exception('could not open output file!');
</code></pre>

<p>  }</p>

<p>  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true       // use simple PHP arrays
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';
foreach ($docs as $doc) {
  $output .= json_encode(transform($doc)) . PHP_EOL;
} 

// write out chunk
fwrite($fp, $output);
</code></pre>

<p>  }</p>

<p>  fclose($fp);
}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<p>The adjusted version of the PHP script will now produce an output file named
<code>output-transformed.json</code>.</p>

<h2>Filtering attributes</h2>

<p>In the last example we discarded a few attributes of each document. Instead of
filtering out these attributes with PHP, we can configure the export to already
exclude these attributes server-side. This way we can save some traffic.</p>

<p>Here&rsquo;s an adjusted configuration that will exclude the unneeded attributes <code>_id</code>,
<code>_rev</code>, <code>_key</code> and <code>likes</code>:</p>

<p>```php configuration for attribute exclusion
// settings to use for the export
$settings = array(
  &lsquo;batchSize&rsquo; => 5000,  // export in chunks of 5K documents
  &lsquo;_flat&rsquo; => true,      // use simple PHP arrays
  &lsquo;restrict&rsquo; => array(</p>

<pre><code>'type' =&gt; 'exclude',
'fields' =&gt; array('_id', '_rev', '_key', 'likes')
</code></pre>

<p>  )
);
```</p>

<p>The full script that employs the adjusted configuration can be downloaded
<a href="/downloads/code/export-exclude.php">here</a>.</p>

<p>Instead of excluding specific attributes we can also do it the other way and only
include certain attributes in an export. The following script demonstrates this by
extracting only the <code>_key</code> and <code>name</code> attributes of each document. It then prints the
key/name pairs in CSV format.</p>

<p>The full script can be downloaded <a href="/downloads/code/export-csv.php">here</a>.</p>

<p>```php export function that prints key/name pairs in CSV format
function export($collection, Connection $connection) {
  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true,      // use simple PHP arrays
'restrict' =&gt; array(
  'type' =&gt; 'include',
  'fields' =&gt; array('_key', 'name')
)
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';

foreach ($docs as $doc) {
  $values = array(
    $doc['_key'], 
    $doc['name']['first'] . ' ' . $doc['name']['last']
  );

  $output .= '"' . implode('","', $values) . '"' . PHP_EOL;
}

// print out the data directly 
print $output;
</code></pre>

<p>  }
}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<h2>Using the API without PHP</h2>

<p>The export API REST interface is simple and it can be used with any client that can
speak HTTP. This includes <em>curl</em> obviously:</p>

<p>The following command fetches the initial 5K documents from the <code>users</code> collection
using <em>curl</em>:</p>

<p><code>bash using the export API with curl
curl                                                   \
  -X POST                                              \
  http://localhost:8529/_api/export?collection=users   \
  --data '{"batchSize":5000}'
</code></p>

<p>The HTTP response will contain a <code>result</code> attribute that contains the actual
documents. It will also contain an attribute <code>hasMore</code> that will indicate whether
there are more documents for the client to fetch. If it is set to <code>true</code>, the
HTTP response will also contain an attribute <code>id</code>. The client can use this id
for sending follow-up requests like this (assuming the returned id was <code>13979338067709</code>):</p>

<p><code>bash sending a follow-up request with curl
curl                                                   \
  -X PUT                                               \
  http://localhost:8529/_api/export/13979338067709  
</code></p>

<p>That&rsquo;s about it. Using the export API it should be fairly simple to ship bulk
ArangoDB data to client applications or data processing tools.</p>
]]></content>
  </entry>
  
</feed>
