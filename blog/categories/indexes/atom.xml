<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Indexes | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/indexes/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-11-18T01:27:06+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Fulltext Index Enhancements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/fulltext-index-enhancements/"/>
    <updated>2015-05-07T15:08:18+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/fulltext-index-enhancements</id>
    <content type="html"><![CDATA[<p>This post is about improvements for the fulltext index in ArangoDB 2.6. The improvements
address the problem that non-string attributes were ignored when fulltext-indexing.</p>

<p>Effectively this prevented string values inside arrays or objects from being indexed. Though this
behavior was documented, it was limited the usefulness of the fulltext index much. Several
users requested the fulltext index to be able to index arrays and object attributes, too.</p>

<p>Finally this has been accomplished, so the fulltext index in 2.6 supports indexing arrays
and objects!</p>

<!-- more -->


<h2>Some history</h2>

<p>So far (up to including ArangoDB 2.5) the fulltext indexing in ArangoDB only supported indexing
<em>string</em> attributes. Documents in which the index attribute was an <em>array</em> of strings or an <em>object</em>
with string member values were excluded from indexing.</p>

<p>This turned out to be limiting, because more complex documents effectively couldn&rsquo;t be
fulltext-indexed.</p>

<p>Here&rsquo;s an example&hellip; Let&rsquo;s say we had a collection named <code>example</code> with a fulltext index
defined on attribute <code>value</code>, set up as follows:</p>

<p><code>js setting up a collection with a fulltext index
var c = db._create("example");
c.ensureFulltextIndex("value");
</code></p>

<p>Adding a document with a <code>value</code> attribute containing a string value adds all words contained
in the string attribute to the fulltext index:</p>

<p><code>js adding a document that is fulltext-indexed
c.insert({ value: "Fox is the English translation of the German word Fuchs" });
</code></p>

<p>Now the index can be queried using any of the words:</p>

<p><code>``js querying the fulltext index</code>
c.fulltext(&ldquo;value&rdquo;, &ldquo;fox&rdquo;).toArray();
[
  {</p>

<pre><code>... 
"value" : "Fox is the English translation of the German word Fuchs" 
</code></pre>

<p>  }
]
```</p>

<p>So far, so good. Now let&rsquo;s try it with more complex document structures.
We&rsquo;re now using arrays and objects inside the <code>value</code> attribute instead of simple
string values:</p>

<p>```js adding documents that are not fulltext-indexed  <br/>
c.insert({ value: { en: &ldquo;fox&rdquo;, de: &ldquo;Fuchs&rdquo;, fr: &ldquo;renard&rdquo;, ru: &ldquo;лиса&rdquo; } });
c.insert({ value: [ &ldquo;ArangoDB&rdquo;, &ldquo;document&rdquo;, &ldquo;database&rdquo;, &ldquo;Foxx&rdquo; ] });
c.insert({ value: [ { name: &ldquo;ArangoDB&rdquo;, type: &ldquo;database&rdquo; }, { name: &ldquo;Fox&rdquo;, type: &ldquo;animal&rdquo; } ] });</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;renard&rdquo;).toArray();
[ ]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;ArangoDB&rdquo;).toArray();
[ ]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;database&rdquo;).toArray();
[ ]
```</p>

<p>Bad luck!</p>

<p>None of the above documents made it into the fulltext index because the index attribute
did not contain string values. Though that was documented, it was not the desirable
behavior.</p>

<h2>2.6</h2>

<p>Retrying the same operations in ArangoDB 2.6 changes the picture.</p>

<p>All the above example documents are included in the fulltext index in 2.6. The fulltext index
in 2.6 can index <em>string</em> values, <em>object</em> values (it will index the object&rsquo;s members if they are strings)
and <em>array</em> values (it will index the array members if they are strings or objects). Indexing
is still limited to one sub-attribute level, so in deeply nested structures only the
top level ones will be indexed.</p>

<p>A few example queries on the index in 2.6 prove that now all the data from the more
complex documents can be queried:</p>

<p>```js querying the fulltext index in 2.6
c.fulltext(&ldquo;value&rdquo;, &ldquo;renard&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : { 
  "en" : "fox", 
  "de" : "Fuchs", 
  "fr" : "renard", 
  "ru" : "лиса" 
} 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;ArangoDB&rdquo;).toArray();
[
  {</p>

<pre><code>... 
"value" : [ 
  "ArangoDB", 
  "document", 
  "database", 
  "Foxx" 
] 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : [ 
  { 
    "name" : "ArangoDB", 
    "type" : "database" 
  }, 
  { 
    "name" : "Fox", 
    "type" : "animal" 
  } 
] 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;database&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : [ 
  "ArangoDB", 
  "document", 
  "database", 
  "Foxx" 
] 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : [ 
  { 
    "name" : "ArangoDB", 
    "type" : "database" 
  }, 
  { 
    "name" : "Fox", 
    "type" : "animal" 
  } 
] 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;лиса&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : { 
  "en" : "fox", 
  "de" : "Fuchs", 
  "fr" : "renard", 
  "ru" : "лиса" 
} 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;prefix:Fox&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : "Fox is the English translation of the German word Fuchs" 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : { 
  "en" : "fox", 
  "de" : "Fuchs", 
  "fr" : "renard", 
  "ru" : "лиса" 
} 
</code></pre>

<p>  },
  {</p>

<pre><code>... 
"value" : [ 
  "ArangoDB", 
  "document", 
  "database", 
  "Foxx" 
] 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : [ 
  { 
    "name" : "ArangoDB", 
    "type" : "database" 
  }, 
  { 
    "name" : "Fox", 
    "type" : "animal" 
  } 
] 
</code></pre>

<p>  }
]
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[COLLECTing With a Hash Table]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/22/collecting-with-a-hash-table/"/>
    <updated>2015-04-22T13:53:10+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/22/collecting-with-a-hash-table</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.6 will feature an alternative <strong>hash</strong> implementation of the AQL <code>COLLECT</code>
operation. The new implementation can speed up some AQL queries that can not exploit indexes
on the <code>COLLECT</code> group criteria.</p>

<p>This blog post provides a preview of the feature and shows some nice performance improvements.
It also explains the <code>COLLECT</code>-related optimizer parts and how the optimizer will decide whether
to use the new or the traditional implementation.</p>

<!-- more -->


<h2>Introduction to COLLECT</h2>

<p>A quick recap: in AQL, the <code>COLLECT</code> operation can be used for grouping and optionally counting values.</p>

<p>Here&rsquo;s an example, using flight data:</p>

<p><code>plain AQL COLLECT example
FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count
  RETURN { from: from, count: count }
</code></p>

<p>This query will iterate over all documents in collection <code>flights</code>, and count the
number of flights per different <code>_from</code> value (origin airport). The query result will
contain only unique <code>from</code> values plus a counter for each:</p>

<p><code>json query result, grouped by from
[
  { "from" : "airports/ABE", "count" : 6205 },
  { "from" : "airports/ABQ", "count" : 39346 },
  { "from" : "airports/ACV", "count" : 362 },
  ...
  { "from" : "airports/YAP", "count" : 285 },
  { "from" : "airports/YKM", "count" : 879 },
  { "from" : "airports/YUM", "count" : 2275 }
]
</code></p>

<p>As the <code>COLLECT</code> will group its result according to the specified group criteria (<code>flights._from</code>
in the above query), it needs a way of figuring out to which group any input value does belong.</p>

<p>Before ArangoDB 2.6, there was a single method for determining the group. Starting with ArangoDB
2.6, the query optimizer can choose between two different <code>COLLECT</code> methods, the <strong>sorted</strong> method
and the <strong>hash</strong> method.</p>

<h2>Sorted COLLECT method</h2>

<p>The traditional method for determining the group values is the <strong>sorted</strong> method. It has been
available in ArangoDB since the very start.</p>

<p>The sorted method of <code>COLLECT</code> requires its input to be sorted by the group criteria specified
in the <code>COLLECT</code> statement. Because there is no guarantee that the input data are already sorted
in the same way, the query optimizer will automatically insert a <code>SORT</code> statement into the query
in front of the <code>COLLECT</code>. In case there is a sorted index present on the group criteria attributes,
the optimizer may be able to optimize away the <code>SORT</code> again. If there is no sorted index present
on the group criteria attributes, the <code>SORT</code> will remain in the execution plan.</p>

<p>Here is the execution plan for the above query using the <strong>sorted</strong> method of <code>COLLECT</code>. We can see
the extra <code>SortNode</code> with id #7 being added by the optimizer in front of the <code>COLLECT</code>:</p>

<p><img src="/downloads/screenshots/collect-sorted.png"></p>

<p>The <strong>sorted</strong> method of <code>COLLECT</code> is efficient because it can write out a group result whenever
an input value will start a new group. Therefore it does not need to keep the whole <code>COLLECT</code>
result in memory. The downside of using the sorted method is that it requires its input to be
sorted, and that this requires adding an extra <code>SORT</code> for not properly sorted input.</p>

<h2>Hash COLLECT method</h2>

<p>Since ArangoDB 2.6, the query optimizer can also employ the <strong>hash</strong> method for <code>COLLECT</code>. The
hash method works by assigning the input values of the <code>COLLECT</code> to slots in a hash table. It
does not require its input to be sorted. Because the entries in the hash table do not have a
particular order, the query optimizer will add a post-<code>COLLECT</code> <code>SORT</code> statement. With this extra
sort of the <code>COLLECT</code> result, the optimizer ensures that the output of the sorted <code>COLLECT</code> will
be the same as the output of the hash <code>COLLECT</code>.</p>

<p>Here is the execution plan for the above query when using the <strong>hash</strong> method of <code>COLLECT</code>.
Here we can see the extra <code>SortNode</code> with id #7 being added post-<code>COLLECT</code>:</p>

<p><img src="/downloads/screenshots/collect-hash.png"></p>

<p>The <strong>hash</strong> method is beneficial because it does not require sorted input and thus no extra
<code>SORT</code> step in front. However, as the input is not sorted, it is never clear when a group is
actually finished. The hash method therefore needs to build the whole <code>COLLECT</code> result in memory
until the input is exhausted. Then it can safely write out all group results. Additionally,
the result of the hash <code>COLLECT</code> is unsorted. Therefore the optimizer will add a post-<code>COLLECT</code>
sort to ensure the result will be identical to a <strong>sorted</strong> <code>COLLECT</code>.</p>

<h2>Which method will be used when?</h2>

<p>The query optimizer will always take the initial query plan and specialize its <code>COLLECT</code> nodes to
using the <strong>sorted</strong> method. It will also add the pre-<code>COLLECT</code> <code>SORT</code> in the original plan.</p>

<p>In addition, for every <code>COLLECT</code> statement not using an <code>INTO</code> clause, the optimizer will create
a plan variant that uses the <strong>hash</strong> method. In that plan variant, the post-<code>COLLECT</code> <code>SORT</code>
will be added. Note that a <code>WITH COUNT INTO</code> is still ok here, but that using a regular <code>INTO</code>
clause will disable the usage of the <strong>hash</strong> method:</p>

<p><code>plain a query that cannot use the hash method
FOR flight IN flights
  COLLECT from = flight._from INTO allFlights
  RETURN { from: from, flights: allFlights }
</code></p>

<p>If more than one <code>COLLECT</code> method can be used for a query, the created plans will be shipped through
the regular optimization pipeline. In the end, the optimizer will pick the plan with the lowest
estimated total cost as it will do for all other queries.</p>

<p>The <strong>hash</strong> variant does not require an up-front sort of the <code>COLLECT</code> input, and will thus be
preferred over the <strong>sorted</strong> method if the optimizer estimates many input elements for the <code>COLLECT</code>
and cannot use an index to process them in already sorted order. In this case, the optimizer
will estimate that post-sorting the result of the <strong>hash</strong> <code>COLLECT</code> will be more efficient than
pre-sorting the input for the <strong>sorted</strong> <code>COLLECT</code>.</p>

<p>The main assumption behind this estimation is that the result of any <code>COLLECT</code> statement will
contain at most as many elements as there are input elements to it. Therefore, the output of
a <code>COLLECT</code> is likely to be smaller (in terms of rows) than its input, making post-sorting more
efficient than pre-sorting.</p>

<p>If there is a sorted index on the <code>COLLECT</code> group criteria that the optimizer can exploit, the
optimizer will pick the <strong>sorted</strong> method because thanks to the index it can optimize away the
pre-<code>COLLECT</code> sort, leaving no sorts left in the final execution plan.</p>

<p>To override the optimizer decision, <code>COLLECT</code> statements now have an <code>OPTIONS</code> modifier. This
modifier can be used to force the optimizer to use the <strong>sorted</strong> variant:</p>

<p><code>plain forcing the use of the sorted variant
FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count OPTIONS { method: "sorted" }
  RETURN { from: from, count: count }
</code></p>

<p>Note that specifying <strong>hash</strong> in <code>method</code> will not force the optimizer to use the <strong>hash</strong> method.
The reason is that the <strong>hash</strong> variant cannot be used for all queries (only <code>COLLECT</code> statements
without an <code>INTO</code> clause are eligible). If <code>OPTIONS</code> are omitted or any other method than <code>sorted</code>
is specified, the optimizer will ignore it and use its regular cost estimations.</p>

<h2>Understanding execution plans</h2>

<p>Which method is actually used in a query can found out by explaining it and looking at its
execution plan.</p>

<p>A <code>COLLECT</code> is internally handled by an object called <code>AggregateNode</code>, so we have to look for that.
In the above screenshots, the <code>AggregateNode</code>s are tagged with either <strong>hash</strong> or <strong>sorted</strong>. This can
also be checked programatically by looking at the <code>aggregationOptions.method</code> attributes in the
JSON result of an explain().</p>

<p>Here is some example code to extract this information, limited to the <code>AggregateNode</code>s of the
query already:</p>

<p><code>js extracting just the AggregateNodes from an explain
var query = `
  FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count
  RETURN { from: from, count: count }
`;
var stmt = db._createStatement(query);
var plan = stmt.explain().plan;
plan.nodes.filter(function(node) {
  return node.type === 'AggregateNode';
});
</code></p>

<p>For the above query, this will produce something like this:</p>

<p>```json JSON explain result for AggregateNode
[
  {</p>

<pre><code>"type" : "AggregateNode", 
...
"aggregationOptions" : { 
  "method" : "hash" 
}  
</code></pre>

<p>  }
]
```</p>

<p>Here we can see that the query is using the <strong>hash</strong> method.</p>

<h2>Optimizing away post-COLLECT sorts</h2>

<p>If a query uses the <strong>hash</strong> method for a <code>COLLECT</code> but the sort order of the <code>COLLECT</code> result
is irrelevant to the user, the user can provide a hint to the optimizer to remove the
post-<code>COLLECT</code> sort.</p>

<p>This can be achieved by simply appending a <code>SORT null</code> to the original <code>COLLECT</code> statement.
Here we can see that this removes the post-<code>COLLECT</code> sort:</p>

<p><img src="/downloads/screenshots/collect-nosort.png"></p>

<h2>Performance improvements</h2>

<p>The improvements achievable by using the <strong>hash</strong> method instead of the <strong>sorted</strong> method obviously
depend on whether there are appropriate indexes present for the group criteria. If an index can
be exploited, the <strong>sorted</strong> method may be just fine. However, there are cases when no indexes are
present, for example, when running arbitrary ad-hoc queries or when indexes are too expensive
(indexes need to be updated on insert/update/remove and also will use memory).</p>

<p>Following are a few comparisons of the <strong>sorted</strong> and the <strong>hash</strong> methods in case no indexes can be
used.</p>

<p>Here&rsquo;s the setup for the test data. This generates 1M documents with both unique and repeating
string and numeric values. For the non-unique values, we&rsquo;ll use 20 different categories:</p>

<p>```js setting up test data
var test = db._create(&ldquo;test&rdquo;);
for (var i = 0; i &lt; 1000000; ++i) {
  test.insert({</p>

<pre><code>uniqueNumber: i, 
uniqueString: String("test" + i), 
repeatingNumber: (i % 20), 
repeatingString: String("test" + (i % 20)) 
</code></pre>

<p>  });
}
```</p>

<p>Now let&rsquo;s run the following query on the data and measure its execution time:</p>

<p><code>plain test query
FOR v IN test
  COLLECT value = v.@attribute WITH COUNT INTO count
  RETURN { value: value, count: count }
</code></p>

<p>The worst case is when the <code>COLLECT</code> will produce as many output rows as there are input
rows. This will happen when using a unique attribute as the grouping criterion. We&rsquo;ll run
tests on both numeric and string values.</p>

<p>Here are the execution times for unique inputs. It can be seen that the <strong>hash</strong> method
here will be beneficial if the post-<code>COLLECT</code> sort can be optimized away. As demonstrated
above, this can be achieved by adding an extra <code>SORT null</code> after the <code>COLLECT</code> statement.
If the post-<code>COLLECT</code> sort is not optimized away, it will make the hash method a bit more
expensive than the <strong>sorted</strong> method:</p>

<p>```plain COLLECT performance with unique inputs</p>

<h2>collect method       @attribute                duration</h2>

<p>sorted               uniqueNumber               11.92 s
hash                 uniqueNumber               13.40 s
hash (sort null)     uniqueNumber               10.13 s
sorted               uniqueString               22.04 s
hash                 uniqueString               27.35 s
hash (sort null)     uniqueString               12.12 s
```</p>

<p>Now let&rsquo;s check the results when we group on an attribute that is non-unique. Following
are the results for numeric and string attributes with 20 different categories each:</p>

<p>```plain COLLECT performance with non-unique inputs</p>

<h2>collect method       @attribute                duration</h2>

<p>sorted               repeatingNumber             5.56 s
hash                 repeatingNumber             0.94 s
hash (sort null)     repeatingNumber             0.94 s
sorted               repeatingString            10.56 s
hash                 repeatingString             1.09 s
hash (sort null)     repeatingString             1.09 s
```</p>

<p>In these cases, the result of the <code>COLLECT</code> will be much smaller than its input (we&rsquo;ll
only get 20 result rows out instead of 1M). Therefore the post-<code>COLLECT</code> sort for the <strong>hash</strong>
method will not make any difference, but the pre-<code>COLLECT</code> sort for the <strong>sorted</strong> method
will still need to sort 1M input values. This is also the reason why the <strong>hash</strong> method
is significantly faster here.</p>

<p>As usual, your mileage may vary, so please run your own tests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating Highscore Lists]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/20/creating-highscore-lists/"/>
    <updated>2015-04-20T20:18:59+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/20/creating-highscore-lists</id>
    <content type="html"><![CDATA[<p>I just came across a question about how to create highscore lists or
leaderboards in ArangoDB, and how they would work when compared to
<a href="http://redis.io/topics/data-types-intro#sorted-sets">Redis sorted sets</a>.</p>

<p>This blog post tries to give an answer on the topic and also detailed
instructions and queries for setting up highscore lists with ArangoDB.</p>

<!-- more -->


<h2>A highscore list in Redis</h2>

<p>Highscore lists are normally used to quickly determine who&rsquo;s currently at
the top, so we obviously need some sorted data structure.</p>

<p>Redis has a specialized datatype named <em>sorted set</em> which can be used for
exactly this purpose. A sorted set in Redis is a value consisting of multiple
key/value pairs, and that is automatically sorted by values. The sorted
set is stored under a key so it can be accessed as a whole.</p>

<p>Here&rsquo;s how one would create a sorted set named <code>highscores</code> and populate
it with 5 key/value pairs in Redis (using <em>redis-cli</em>):</p>

<p>```plain creating a sorted set in Redis</p>

<blockquote><p>ZADD highscores frank 50 jan 20 willi 35 thomas 75 ingo 60
```</p></blockquote>

<p>Adding a new entry to a sorted set is done using <code>ZADD</code> too.
Inserting into a Redis sorted set has logarithmic complexity.</p>

<p>Updating a score in the sorted set is done using <code>ZINCRBY</code>. This command works
regardless of whether the to-be-updated key already exists in the sorted set.
If it exists, its score will be increased by the specified value, and if it does
not exist, it will be created with the specified value:</p>

<p>```plain updating a value in a sorted set</p>

<blockquote><p>ZINCRBY highscores 80 max
(integer) 1
```</p></blockquote>

<p>In this case the return value <code>1</code> indicates that a new key was added to the set
and that it didn&rsquo;t update an existing one.</p>

<p>Querying the entries with the lowest scores from a Redis sorted set is trivial.</p>

<p>The <code>ZRANGE</code> command will query the entries in the sorted set from lowest to
highest score. As the entries are already stored in sorted order, this is very
efficient.</p>

<p>The following command queries the bottom 3 keys from the sorted set:</p>

<p><code>plain querying the sorted set, from lowest to highest
ZRANGE highscores 0 2
1) "jan"
2) "willi"
3) "frank"
</code></p>

<p>For querying in reverse order, there is <code>ZREVRANGE</code>. Both commands can be
accompanied by the <code>WITHSCORES</code> flag to also return the associated values (i.e.
the scores). Here are the top 3 key/value pairs in the sorted set:</p>

<p>```plain querying the sorted set, from highest to lowest and with scores</p>

<blockquote><p>ZREVRANGE highscores 0 2 WITHSCORES
1) &ldquo;max&rdquo;
2) &ldquo;80&rdquo;
3) &ldquo;thomas&rdquo;
4) &ldquo;70&rdquo;
5) &ldquo;ingo&rdquo;
6) &ldquo;60&rdquo;
```</p></blockquote>

<p>For removing an entry from a sorted set there is <code>ZREM</code>:</p>

<p>```plain removing a key from a sorted set</p>

<blockquote><p>ZREM highscores jan
(integer) 1
```</p></blockquote>

<p>There are many more specialized Redis commands for working with sorted sets. The
<a href="http://redis.io/commands">Redis commands</a> prefixed with a <code>Z</code> are sorted set
commands.</p>

<h2>A highscore list in ArangoDB</h2>

<p>Now let&rsquo;s try to mimic that with ArangoDB.</p>

<p>In ArangoDB, there is no such thing as a sorted set and no direct equivalent.
Instead, data in ArangoDB are stored in collections. Collections are a
general-purpose storage mechanism and they are not limited to storing just
scores.</p>

<p>We also need a mechanism for keeping highscores sorted. By default, no
specific sort order is maintained for data in a collection. To have the
collection entries sorted by highscore values, we have to explicitly create
a (sorted) skiplist index on some attribute. We&rsquo;ll use an attribute named
<code>score</code> for this.</p>

<p>The following shell commands create the collection and the index on <code>score</code>:</p>

<p><code>js creating the highscores collection in ArangoDB
db._create("highscores");
db.highscores.ensureIndex({ type: "skiplist", fields: [ "score" ] });
</code></p>

<p>Once the collection is set up, we can switch to AQL for the following operations
(though we could achieve the same with Shell commands).</p>

<p>To insert the same initial data as in the Redis case, we can run the following
five AQL queries:</p>

<p><code>plain inserting initial scores
INSERT { _key: "frank", score: 50 } IN highscores
INSERT { _key: "jan", score: 20 } IN highscores
INSERT { _key: "willi", score: 35 } IN highscores
INSERT { _key: "thomas", score: 75 } IN highscores
INSERT { _key: "ingo", score: 60 } IN highscores
</code></p>

<p>Note that I have been using the <code>_key</code> attribute for saving the user id. Using the
<code>_key</code> attribute is normally beneficial because it is the collection&rsquo;s primary key.
It is always present and automatically unique, so exactly what we need for maintaining
a highscore list. Note that there are some restrictions for what can be stored inside
the <code>_key</code> attribute, but as long as values are only ASCII letters or digits, there
is nothing to worry about.</p>

<p>Inserting into the collection will also automatically populate the indexes.
Inserting into a skiplist should have about logarithmic complexity on average
(though this is not guaranteed &ndash; this is because the skiplist is a probabilistic
data structure and internally it will be flipping coins. In theory there is a chance
that it becomes badly balanced. But in practice it should be quite close to an
average logarithmic complexity).</p>

<p>As we have some initial documents, we can now query the lowest and highest scores.
This will also be efficient as the queries will use the sorted index on <code>score</code>:</p>

<p><code>plain querying the users with lowest scores
FOR h IN highscores
  SORT h.score ASC
  LIMIT 3
  RETURN { user: h._key, score: h.score }
</code></p>

<p><code>plain querying the users with highest scores
FOR h IN highscores
  SORT h.score DESC
  LIMIT 3
  RETURN { user: h._key, score: h.score }
</code></p>

<p>To store a highscore for a user without knowing in advance whether a value has already
been stored before for this user, one can use <code>UPSERT</code>. The <code>UPSERT</code> will either insert
a new highscore entry, or update an existing one if already present:</p>

<p><code>plain using UPSERT
UPSERT { _key: "max" }
  INSERT { _key: "max", score: 80 }
  UPDATE { score: OLD.score + 80 } IN highscores
  RETURN { user: NEW._key, score: NEW.score }
</code></p>

<p>If there is already an entry with a key <code>max</code>, its scores will be increased by 80.
If such entry does not exist, it will be created. In both cases, the new score will
be returned.</p>

<p>Note: the <code>UPSERT</code> command has been added in ArangoDB version 2.6.</p>

<p>Finally, removing an entry from a highscore list is a straight-forward remove operation:</p>

<p><code>plain removing an element
REMOVE { _key: "jan" } IN highscores
</code></p>

<h2>Extensions</h2>

<p>We&rsquo;ll now build on this simple example and create slightly more advanced highscore list
use cases. The following topics will be covered:</p>

<ul>
<li>multi-game highscore lists</li>
<li>joining data</li>
<li>maintaining a &ldquo;last updated&rdquo; date</li>
</ul>


<h3>Multi-game highscore lists</h3>

<p>We&rsquo;ll start with generalizing the single-game highscore list into a multi-game
highscore list.</p>

<p>In Redis, one would create multiple sorted sets for handling the highscore lists of
multiple games. Multiple Redis sorted sets are stored under different keys, so they
are isolated from each other.</p>

<p>Though Redis provides a few commands to aggregate data from multiple sorted sets
(<code>ZUNIONSTORE</code> and <code>ZINTERSTORE</code>) into a new sorted set, other cross-set operations are
not supported. This is not a problem if the client application does not have to
perform cross-set queries or cleanup tasks.</p>

<p>In ArangoDB, multi-game highscore lists can be implemented in two variants.<br/>
In order to decide which variant is better suited, we need to be clear about whether
all highscores should be stored in the same collection or if we prefer using multiple
collections (e.g. one per game).</p>

<p>Storing highscores for different games in separate collections has the advantage that
they&rsquo;re really isolated. It is easy to get rid of a specific highscore list by simply
dropping its collection. It is also easy to get right query-wise.</p>

<p>All that needs to be changed to turn the above examples into a multi-game highscore
list solution is to change the hard-coded collection name <code>highscores</code> and make it a
bind parameter, so the right collection name can be injected by the client application
easily.</p>

<p>On the downside, the multi-collection solution will make cross-game operations difficult.
Additionally, having one collection per game may get out of hand when there are many,
many highscore lists to maintain. In case there are many but small highscore lists to
maintain, it might be better to put them into a single collection and add a <code>game</code>
attribute to tell the individual lists apart in it.</p>

<p>Let&rsquo;s focus on this and put all highscores of all games into a single collection.</p>

<p>The first adjustment that needs to be made is that we cannot use <code>_key</code> for user ids
anymore. This is because user ids may repeat now (a user may be contained in more than
one list). So we will change the design and make the combination of <code>game</code> and <code>user</code>
a new unique key:</p>

<p><code>js creating a multi-game highscore collection
db._drop("highscores");
db._create("highscores");
db.highscores.ensureIndex({ type: "hash", unique: true, fields: [ "user", "game" ] });
db.highscores.ensureIndex({ type: "skiplist", fields: [ "game", "score" ] });
</code></p>

<p>We can use the unique hash index on <code>user</code> and <code>game</code> to ensure there is at most one entry
for per user per game. It also allows use to find out quickly whether we already have
an entry for that particular combination of game and user. Because we are not using
<code>_key</code> we could now also switch to numeric ids if we preferred that.</p>

<p>The other index on <code>game</code> and <code>score</code> is sorted. It can be used to quickly retrieve the
leaderboard for a given game. As it is primarily sorted by <code>game</code>, it can also be used
to enumerate all entries for a given game.</p>

<p>The following Shell command populates the multi-game highscores collection with 55,000
highscores:</p>

<p>```js populating the multi-game collection
for (var game = 0; game &lt; 10; ++game) {
  for (var user = 0; user &lt; (game + 1) * 1000; ++user) {</p>

<pre><code>db.highscores.save({ 
  game: game, 
  user: String(user),
  score: (game + user) % 997  /* arbitrary score */
}); 
</code></pre>

<p>  }
}
```</p>

<p>The game ids used above are between 0 and 9, though any other game ids would work, too.
User ids are stringified numbers.</p>

<p>We can now find out the leaderboard for game 2 with the following adjusted AQL query.
The query will use the (sorted) skiplist index:</p>

<p><code>plain querying the leaderboard of a specific game
FOR h IN highscores
  FILTER h.game == 2
  SORT h.score DESC
  LIMIT 3
  RETURN { user: h.user, score: h.score }
</code></p>

<p>Removing all scores for a specific game is also efficient due to the the same index:</p>

<p><code>plain removing all scores for game 5
FOR h IN highscores
  FILTER h.game == 5
  REMOVE h IN highscores
</code></p>

<p>On a side note: when storing all highscores in the same collection, we could also
run cross-game queries if we wanted to. All that needs to be done for this is adjusting
the <code>FILTER</code> conditions in the queries.</p>

<p>Inserting or updating a user score can be achieved using an <code>UPSERT</code>.
Here&rsquo;s a query to increase the score of user <code>"1571"</code> in game <code>2</code> by a value of 5:</p>

<p><code>plain updating a score for a specific user/game combination
UPSERT { game: 2, user: "1571" }
  INSERT { game: 2, user: "1571", score: 5 }
  UPDATE { score: OLD.score + 5 } IN highscores
  RETURN { user: NEW._key, score: NEW.score }
</code></p>

<p>The same index on <code>[ "user", "game" ]</code> is used in the following query that will
delete the highscore of a given user in a specific game:</p>

<p><code>plain removing a score for a specific user/game combination
FOR h IN highscores
  FILTER h.game == 6
  FILTER h.user == '3894'
  REMOVE h IN highscores
</code></p>

<h3>Joining data</h3>

<p>Querying the leaderboard for a specific game was easy. However, so far we have only
queried user ids and associated scores in games. In reality, we probably want to display
some more user information in a leaderboard, for example their screen names.</p>

<p>In Redis, no extra information can be stored in sorted sets. So extra user information
must be stored under separate keys. There is no concept of joins in Redis. The scores
contained in the sorted set need to be queried by the client application, and extra
user information have to be queried by the client application separately.</p>

<p>In ArangoDB, we could store the screen names in the highscores collection along with
the highscores so we can easily query them with the leaderboard query. This is also how it
would be done in MongoDB due to the absence of joins there.</p>

<p>While this would work, it will create lots of redundant data if the screen names are
also used and stored elsewhere.</p>

<p>So let&rsquo;s pick the option that stores highscores and screen names in separate places,
and brings them together only when needed in a leaderboard query.</p>

<p>Let&rsquo;s store screen names in a collection named <code>users</code>. The following Shell commands
will create the collection and set up 100K users with dummy screen names:</p>

<p>```js creating test users
db._create(&ldquo;users&rdquo;);
for (var i = 0; i &lt; 100000; ++i) {
  db.users.insert({</p>

<pre><code>_key: String(i), 
name: "test user #" + i 
</code></pre>

<p>  });
}
```</p>

<p>We can now query the highscores plus the screen name in one go:</p>

<p>```plain joining highscores with user data
FOR h IN highscores
  FILTER h.game == 2
  SORT h.score DESC
  LIMIT 3
  FOR u IN users</p>

<pre><code>FILTER h.user == u._key 
RETURN { user: h.user, name: u.name, score: h.score } 
</code></pre>

<p>```</p>

<h3>Maintaining a &ldquo;last updated&rdquo; date</h3>

<p>Finally, let&rsquo;s try to keep track of when a highscore was last updated. There are
a few use cases for this, for example displaying the date and time of when a highscore
was achieved or for revmoing older highscores.</p>

<p>In Redis, the sorted set values are just the numeric scores, so we cannot store
anything else (such as a date) inside the sorted sets. We would really need to store
the update date for each highscore entry outside the sorted set, either under a
separate key, or using a Redis hash. However, this is complex to manage and keep
consistent so we won&rsquo;t do it.</p>

<p>For implementing the automatic expiration, it would be good if we could use the
built-in automatic key expiration of Redis. Each key can optionally be given a time-to-live
or an expiration date, and it will automatically expire and vanish then without further
ado. This may be exactly what we need to remove older highscore entries, but we cannot
use it. The reason is that expiration only works for keys at the top level, but not
for individual keys inside a sorted set. So we cannot really implement this sanely.</p>

<p>Let&rsquo;s switch to ArangoDB now. Here we work with arbitrarily structured documents.
That means we can store any other attributes along with a highscore. We can store the
timestamp of when a highscore was last set or updated in an attribute named <code>date</code>:</p>

<p><code>plain storing the date of last update
LET now = DATE_NOW()
UPSERT { game: 2, user: "1571" }
  INSERT { game: 2, user: "1571", score: 10, date: now }
  UPDATE { score: OLD.score + 10, date: now } IN highscores
  RETURN { user: NEW._key, score: NEW.score }
</code></p>

<p>The <code>date</code> attribute can now be used for display purposes already.</p>

<p>We can also use the <code>date</code> attribute for identifying the oldest entries in the
highscore list in case we want the list to be periodically cleaned up.</p>

<p>Obviously we will be indexing <code>date</code> for this, but we need to decide whether we want to use
the same expiration periods for all games, or if we want to use game-specific expirations.</p>

<p>If the expiration date is the same for all games, then we can index just <code>date</code>:</p>

<p><code>js creating the index on date
db.highscores.ensureIndex({ type: "skiplist", fields: [ "date" ] });
</code></p>

<p>If we now want to remove entries older than roughly 2 days, regardless of the
associated game, the removal query looks like this:</p>

<p><code>plain deleting oldest entries
LET compare = DATE_NOW() - 2 * 86400 * 1000
FOR h IN highscores
  FILTER h.date &lt; compare
  LIMIT 1000
  REMOVE h IN highscores
</code></p>

<p>If we instead want to find (and remove) the oldest entries for individual games,
we need to create the index on <code>game</code> and <code>date</code>:</p>

<p><code>js creating the index on game and date
db.highscores.ensureIndex({ type: "skiplist", fields: [ "game", "date" ] });
</code></p>

<p>This index allows to efficiently get rid of the oldest entries per game:</p>

<p><code>plain remvoin oldest entries for a game
LET compare = DATE_NOW() - 2 * 86400 * 1000
FOR h IN highscores
  FILTER h.game == 2
  FILTER h.date &lt; compare
  LIMIT 1000
  REMOVE h IN highscores
</code></p>

<p>On a side note: the <code>REMOVE</code> was limited to the <em>oldest</em> 1000 entries. This
was done to make the query return fast. The removal query can be repeated while
there are still entries to remove.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Improvements in 2.5]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/26/aql-improvements-in-25/"/>
    <updated>2015-02-26T10:35:31+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/26/aql-improvements-in-25</id>
    <content type="html"><![CDATA[<p>Contained in 2.5 are some small but useful AQL language improvements plus several AQL optimizer improvements.</p>

<p>We are working on further AQL improvements for 2.5, but work is still ongoing.
This post summarizes the improvements that are already completed and will be shipped with the initial ArangoDB
2.5 release.</p>

<!-- more -->


<h1>Language improvements</h1>

<h2>Dynamic attribute names</h2>

<p>Often the need arises to dynamically name object attributes in return values.
In AQL this was not directly possible so far, though there were some workarounds available to achieve about
the same result. <a href="https://docs.arangodb.com/cookbook/UsingDynamicAttributeNames.html">This recipe</a> summarizes
the options that are available to pre-ArangoDB 2.5 users.</p>

<p>With ArangoDB 2.5, dynamic attribute names can be constructed much more easily and flexibly. Object
attribute names in ArangoDB 2.5 can be specified using static string literals, bind parameters,
and dynamic expressions.</p>

<p>Dynamic expressions are most interesting, and to disambiguate them from other regular string literal attribute
names, dynamic attribute names need to be enclosed in square brackets (<code>[</code> and <code>]</code>). I have written about
that before in <a href="http://jsteemann.github.io/blog/2015/02/03/using-dynamic-attribute-names-in-aql/">this blog</a>.</p>

<p>Here is an example query that uses the new syntax:</p>

<p><code>plain example query using dynamic attribute names
FOR i IN [ 17, 23, 42, 83 ]
  RETURN { [ CONCAT('value-of-', i, ' * ', i) ] : i * i }
</code></p>

<p>This will produce:</p>

<p>```json query result
[
  {</p>

<pre><code>"value-of-17 * 17" : 289 
</code></pre>

<p>  },
  {</p>

<pre><code>"value-of-23 * 23" : 529 
</code></pre>

<p>  },
  {</p>

<pre><code>"value-of-42 * 42" : 1764 
</code></pre>

<p>  },
  {</p>

<pre><code>"value-of-83 * 83" : 6889 
</code></pre>

<p>  }
]
```</p>

<h2>Functions added</h2>

<p>The following AQL functions have been added in 2.5:</p>

<ul>
<li><code>MD5(value)</code>: produces the MD5 hash of <code>value</code></li>
<li><code>SHA1(value)</code>: produces the SHA1 hash of <code>value</code></li>
<li><code>RANDOM_TOKEN(length)</code>: produces a pseudo-random string of the specified length.
 Such strings can be used for id or token generation. Tokens consist only of letters
 (lower and upper case) plus digits, so they are also URL-safe</li>
</ul>


<h1>Optimizer improvements</h1>

<h2>Optimizer rules</h2>

<p>The following AQL optimizer rules have been added in ArangoDB 2.5:</p>

<ul>
<li><p><code>propagate-constant-attributes</code></p>

<p>This rule will look inside <code>FILTER</code> conditions for constant value equality comparisons,
and insert the constant values in other places in <code>FILTER</code>s. For example, the rule will
insert <code>42</code> instead of <code>i.value</code> in the second <code>FILTER</code> of the following query:</p>

<pre><code>FOR i IN c1 
  FOR j IN c2 
    FILTER i.value == 42 
    FILTER j.value == i.value 
    RETURN 1
</code></pre></li>
<li><p><code>move-calculations-down</code></p>

<p>This rule moves calculations down in the execution plan as far as possible. The intention
is to move calculations beyond filters, in order to avoid calculations and computations
for documents that will be filtered away anyway.</p>

<p>If a query contains a lot of computations and a lot of documents will be skipped because
of filters, this rule might provide a big benefit.</p>

<p>A more detailed example is provided in
<a href="http://jsteemann.github.io/blog/2015/01/31/yaor-yet-another-optimizer-rule/">this post</a>.</p></li>
</ul>


<p>The already existing optimizer rule <code>use-index-for-sort</code> was also improved in the following way:</p>

<ul>
<li><p>the rule can now remove <code>SORT</code>s also in case a non-sorted index (i.e. a hash index) is used
for an equality lookup and all sort attributes are covered by the index.</p></li>
<li><p>the rule can also remove <code>SORT</code>s in case the sort critieria excludes the left-most index attributes,
but the left-most index attributes are used in a <code>FILTER</code> for equality-only lookups.</p>

<p>Here is an example that will use an existing skiplist index on [ <code>value1</code>, <code>value2</code> ] for sorting,
removing the extra <code>SORT</code>:</p>

<pre><code>FOR doc IN collection 
  FILTER doc.value1 == 1 
  SORT doc.value2 
  RETURN doc
</code></pre></li>
</ul>


<h2>Index usage</h2>

<p>The AQL optimizer now supports <a href="https://www.arangodb.com/2015/02/24/sparse-indexes-in-arangodb">sparse indexes</a>,
a feature added in 2.5.</p>

<p>It will use them automatically in queries when appropriate and when safe. Sparse indexes do exclude certain
documents purposely, so the optimizer always has to figure out whether it can use a sparse index to satisfy
a given <code>FILTER</code> condition.</p>

<p>The optimizer will also take into account index selectivity estimates when there are multiple index candidates.</p>

<h2>Estimates</h2>

<p>The optimizer estimates for the number of documents to be returned by a query or a subquery are more accurate
now for several types of queries. For example, if the optimizer can use a primary key, an edge index, or a hash
index in a given query part, it will use the index selectivity estimates for calculating the number of return
documents.</p>

<p>These estimates will be a lot more accurate than the previoulsy hard-coded filtering factors, and can lead to
better optimizer decisions and reporting (because estimates are returned in <code>explain</code> results, too).</p>

<h2>Memory savings</h2>

<p>Finally, the optimizer will now detect if the data-modification part in a data-modification query
can be executed in lockstep with the data-retrieval part of the same query. Previously, a data-modification
query always executed its data-retrieval part first, and then executed its data-modification part.
This could have resulted in big intermediate result sets which to retrieval part constructed in order
to pass them to the modification part of the query.</p>

<p>Here&rsquo;s an example query:</p>

<p><code>plain data-modification query
FOR doc IN test
  INSERT doc INTO backup
</code></p>

<p>In the above query, the <code>FOR</code> loop is the retrieval part, and the <code>INSERT</code> is the modification part.
The optimizer in 2.5 will check if the two parts of the query are independent, and if it turns out they are,
will execute them in lockstep instead of sequentially.</p>

<p>The execution in lockstep is not necessarily faster than sequential execution, but it can save lots of
memory if the data-retrieval part constructed big intermediate result sets.</p>

<h1>Miscellaneous changes</h1>

<p>The AQL query execution statistics now also provide an attribute <code>filtered</code>. Its value indicates how many
documents were filtered by <code>FilterNode</code>s in the AQL query. This can be used as an indicator for whether
indexes should be added, and for how effective indexes are used for filtering.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improved Non-unique Hash Indexes in 2.3]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes/"/>
    <updated>2014-11-07T20:51:12+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes</id>
    <content type="html"><![CDATA[<p>With ArangoDB 2.3 now getting into the <a href="https://www.arangodb.com/install-beta-version">beta stage</a>,
it&rsquo;s time to spread the word about new features and improvements.</p>

<p>Today&rsquo;s post will be about the changes made to non-unique hash
indexes.</p>

<!-- more -->


<p>Hash indexes allow looking up documents quickly if the indexed
attributes are all provided in a search query. They are not
suitable for range queries, but are the perfect choice if equality
comparisons are all that&rsquo;s needed.</p>

<p>Hash indexes have been available in ArangoDB ever since. There
have always been two variants of them:</p>

<ul>
<li>unique hash indexes</li>
<li>non-unique hash indexes</li>
</ul>


<p>There wasn&rsquo;t much to be done for unique hash indexes, and so there
haven&rsquo;t been any changes to them in 2.3. However, the non-unique
hash indexes were improved significantly in the new version.</p>

<p>The non-unique indexes already performed quite well if most of the
indexed values were unique and only few repetitions occurred. But their
performance suffered severly if the indexed attribute values repeated
a lot &ndash; that is, when the indexed value had a <strong>low cardinality</strong> and thus
the index had a <strong>low selectivity</strong>.</p>

<p>This was a problem because it slowed down inserting new documents into
a collection with such an index. And it also slowed down loading collections
with low cardinality hash indexes.</p>

<p>I am happy to state that in ArangoDB 2.3 this has been fixed, and the insert
performance of non-unique hash indexes has been improved significantly.
The index insertion time now scales quite well with the number
of indexed documents regardless of the cardinality of the indexed
attribute.</p>

<p>Following are a few measurements of non-unique hash index insertion
times from ArangoDB 2.3, for different cardinalities of the indexed
attribute.</p>

<p>The times reported are the net non-unique hash index
insertion times (the documents were present already, just the index
was created on them and index creation time was measured).</p>

<p>Let&rsquo;s start with a not too challenging case: indexing documents in
a collection with 100,000 different index values (<em>cardinality 100,000</em>):</p>

<p><code>text index insertion times for cardinality 100,000
number of documents:    128,000    =&gt;    time:   0.144 s
number of documents:    256,000    =&gt;    time:   0.231 s
number of documents:    512,000    =&gt;    time:   0.347 s
number of documents:  1,024,000    =&gt;    time:   0.694 s
number of documents:  2,048,000    =&gt;    time:   1.379 s
</code></p>

<p>The picture doesn&rsquo;t change much when reducing the cardinality
by a factor or 10 (i.e. <em>cardinality 10,000</em>):</p>

<p><code>text index insertion times for cardinality 10,000
number of documents:    128,000    =&gt;    time:   0.169 s
number of documents:    256,000    =&gt;    time:   0.194 s
number of documents:    512,000    =&gt;    time:   0.355 s
number of documents:  1,024,000    =&gt;    time:   0.668 s
number of documents:  2,048,000    =&gt;    time:   1.325 s
</code></p>

<p>Let&rsquo;s again divide cardinality by 10 (now <em>cardinality 1,000</em>):</p>

<p><code>text index insertion times for cardinality 1,000
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.152 s
number of documents:    512,000    =&gt;    time:   0.261 s
number of documents:  1,024,000    =&gt;    time:   0.524 s
number of documents:  2,048,000    =&gt;    time:   0.934 s
</code></p>

<p><em>Cardinality 100</em>:</p>

<p><code>text index insertion times for cardinality 100
number of documents:    128,000    =&gt;    time:   0.114 s
number of documents:    256,000    =&gt;    time:   0.148 s
number of documents:    512,000    =&gt;    time:   0.337 s
number of documents:  1,024,000    =&gt;    time:   0.452 s
number of documents:  2,048,000    =&gt;    time:   0.907 s
</code></p>

<p><em>Cardinality 10</em>:</p>

<p><code>text index insertion times for cardinality 10
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.327 s
number of documents:    512,000    =&gt;    time:   0.239 s
number of documents:  1,024,000    =&gt;    time:   0.442 s
number of documents:  2,048,000    =&gt;    time:   0.827 s
</code></p>

<p>Finally we get to <em>cardinality 1</em>, the definitive indicator
for the index being absolutely useless. Let&rsquo;s create it anyway,
for the sake of completeness of this post:</p>

<p><code>text index insertion times for cardinality 1
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    128,000    =&gt;    time:   0.095 s
number of documents:    256,000    =&gt;    time:   0.146 s
number of documents:    512,000    =&gt;    time:   0.246 s
number of documents:  1,024,000    =&gt;    time:   0.445 s
number of documents:  2,048,000    =&gt;    time:   0.925 s
</code></p>

<p>On a side note: all indexed values were numeric. In absolute terms,
indexing string values will be slower than indexing numbers, but insertion
should still scale nicely with the number of documents as long as everything
fits in RAM.</p>
]]></content>
  </entry>
  
</feed>
