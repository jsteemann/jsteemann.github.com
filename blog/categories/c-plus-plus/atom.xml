<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: C++ | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/c-plus-plus/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-11-20T16:20:12+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[C++ Constructors and Memory Leaks]]></title>
    <link href="http://jsteemann.github.io/blog/2015/11/18/on-exception-handling/"/>
    <updated>2015-11-18T01:10:51+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/11/18/on-exception-handling</id>
    <content type="html"><![CDATA[<h2>Preventing leaks in throwing constructors</h2>

<p>The easiest way to prevent memory leaks is to create all objects on
the stack and not using dynamic memory at all. However, often this is not
possible, for example because stack size is limited or objects need to
outlive the caller&rsquo;s scope.</p>

<p>Another way to prevent memory leaks and leaks of other resources is
obviously to employ the RAII pattern. How can it be used safely and
easily in practice, so memory leaks can be avoided?</p>

<p>This post will start with a few seemingly working but subtly ill-formed
techniques that a few common pitfalls. Later on it will provide a few
very simple solutions for getting it right.</p>

<!--more -->


<p>None of the solutions here are new or original.</p>

<p>I took some inspiration from the excellent <a href="http://gotw.ca/gotw/066.htm">constructor failures GotW post</a>.
That doesn&rsquo;t cover smart pointers and is not explicitly about preventing
preventing memory leak, so I put together this overview myself.</p>

<h2>Naive implementation</h2>

<p>Let&rsquo;s pretend we have a simple test program <code>main.cpp</code>, which
creates an object of class <em>MyClass</em> on the stack like this:</p>

<p>```cpp main.cpp</p>

<h1>include <iostream></h1>

<h1>include &ldquo;MyClass.h&rdquo;</h1>

<p>int main () {
  try {</p>

<pre><code>MyClass myClass;
std::cout &lt;&lt; "NO EXCEPTION" &lt;&lt; std::endl;
</code></pre>

<p>  }
  catch (&hellip;) {</p>

<pre><code>std::cout &lt;&lt; "CAUGHT EXCEPTION" &lt;&lt; std::endl;
</code></pre>

<p>  }
}
```</p>

<p>The above code creates the <em>myClass</em> instance on the stack, so
itself will not leak any memory. When the creating of the <em>myClass</em>
instance fails for whatever reason, the instance newer existed so
the memory for holding a <em>MyClass</em> object will be freed automatically.
If object creation succeeds and the object goes out of scope at the
end of the <em>try</em> block, then the object&rsquo;s destructor will be called
and resources can be freed, too.</p>

<p>Obviously this is already good, so let&rsquo;s keep it as it is and have a
look at the implementation of <em>MyClass</em> now. This class will manage
two heap objects of type <em>A</em>, which are created using the helper
function <em>createInstance</em>:</p>

<p>```cpp MyClass.h</p>

<h1>include <iostream></h1>

<h1>include &ldquo;A.h&rdquo;</h1>

<p>struct MyClass {
  A<em> a1;
  A</em> a2;</p>

<p>  MyClass ()</p>

<pre><code>: a1(createInstance()),
  a2(createInstance()) {

std::cout &lt;&lt; "CTOR MYCLASS" &lt;&lt; std::endl;
</code></pre>

<p>  }</p>

<p>  ~MyClass () {</p>

<pre><code>std::cout &lt;&lt; "DTOR MYCLASS" &lt;&lt; std::endl;
delete a1;
delete a2;
</code></pre>

<p>  }
};
```</p>

<p>For completeness, here is class <em>A</em>. It won&rsquo;t manage any resources
itself:</p>

<p>```cpp A.h</p>

<h1>include <iostream></h1>

<p>struct A {
  A () {</p>

<pre><code>std::cout &lt;&lt; "CTOR A" &lt;&lt; std::endl;
</code></pre>

<p>  }
  ~A () {</p>

<pre><code>std::cout &lt;&lt; "DTOR A" &lt;&lt; std::endl;
</code></pre>

<p>  }
};</p>

<p>// helper method for creating an instance of A
A* createInstance (bool shouldThrow = false) {
  if (shouldThrow) {</p>

<pre><code>throw "THROWING AN EXCEPTION";
</code></pre>

<p>  }
  return new A;
}
```</p>

<p>During this complete post, the code of <em>A.h</em> will remain unchanged.</p>

<p>Compiling and running the initial version of <code>main.cpp</code> will produce the
following output:</p>

<p><code>plain output of naive implementation
CTOR A
CTOR A
CTOR MYCLASS
NO EXCEPTION
DTOR MYCLASS
DTOR A
DTOR A
</code></p>

<p>Valgrind also reports no memory leaks. Are we done already?</p>

<h2>Introducing exceptions</h2>

<p>No, because everything still went well. Let&rsquo;s introduce exceptions into
the picture and check what happens then.</p>

<p>Let&rsquo;s first introduce an exception in the constructor of <em>MyClass</em>.
We&rsquo;ll make the <em>createInstance</em> function throw on second invocation (we do
this by passing a value of <em>true</em> to it):</p>

<p>```cpp constructor throwing an exception
MyClass ()
  : a1(createInstance()),</p>

<pre><code>a2(createInstance(true)) {
</code></pre>

<p>  std::cout &lt;&lt; &ldquo;CTOR MYCLASS&rdquo; &lt;&lt; std::endl;
}
```</p>

<p>Running the program will now emit the following:</p>

<p><code>plain output of naive implementation, with exception
CTOR A
CAUGHT EXCEPTION
</code></p>

<p>As we&rsquo;re throwing in the initializer list already, we don&rsquo;t even
reach the constructor body. This is no problem, but worse is that the
destructor for class <em>MyClass</em> is not being called at all.
Valgrind therefore reports the memory for first <em>A</em> instance as leaked.</p>

<p>By the way, the destructor for the <em>MyClass</em> instance is intentionally
not being called as the object hasn&rsquo;t been fully constructed and logically
never existed.</p>

<p>Will it help if we move the heap allocations from the initializer list
into the constructor body like this?</p>

<p><code>cpp using the constructor body instead of the initializer list
MyClass () {
  std::cout &lt;&lt; "CTOR MYCLASS" &lt;&lt; std::endl;
  a1 = createInstance();
  a2 = createInstance(true);
}
</code></p>

<p>Unfortunately not. Still no destructor invocations:</p>

<p><code>plain output of constructor body variant
CTOR MYCLASS
CTOR A
CAUGHT EXCEPTION
</code></p>

<p>Remember: an object&rsquo;s destructor won&rsquo;t be called if its constructor threw
and the exception wasn&rsquo;t caught. That also means releasing an object&rsquo;s
resources solely via the destructor as in implementation above will not be
sufficient if resources are allocated in the constructor and the constructor
can throw.</p>

<p>What can be done about that?</p>

<p>Obviously all resource allocations can be moved into the constructor body so
exceptions can be caught there:</p>

<p>```cpp catching exceptions in constructor of MyClass
MyClass () {
  std::cout &lt;&lt; &ldquo;CTOR MYCLASS&rdquo; &lt;&lt; std::endl;
  a1 = createInstance();</p>

<p>  try {</p>

<pre><code>a2 = createInstance(true);
</code></pre>

<p>  }
  catch (&hellip;) {</p>

<pre><code>// must clean up a1 to prevent a leak
delete a1;
// and re-throw the exception
throw;
</code></pre>

<p>  }
}
```</p>

<p>While the above will work, it&rsquo;s clumsy, verbose and error-prone. If
more objects need to be managed this will make us end up in deeply
nested try&hellip;catch blocks.</p>

<h2>try&hellip;catch for the initializer list</h2>

<p>But wait, wasn&rsquo;t there a try&hellip;catch feature especially for initializer
list code? Sounds like it could be useful. Maybe we can use this instead
so we can catch exceptions during initialization?</p>

<p>There is indeed something like that: exceptions thrown from the initializer
list  can be caught using the following special syntax:</p>

<p>```cpp catching exceptions thrown in the initializer list
MyClass ()
  try : a1(createInstance()),</p>

<pre><code>    a2(createInstance(true)) {

std::cout &lt;&lt; "CTOR MYCLASS" &lt;&lt; std::endl;
</code></pre>

<p>  }
  catch (&hellip;) {  // catch block for initializer list code</p>

<pre><code>std::cout &lt;&lt; "CATCH BLOCK MYCLASS" &lt;&lt; std::endl;
delete a1;
</code></pre>

<p>  }
```</p>

<p>Running the program with the above <em>MyClass</em> constructor will also
do what is expected: when creating the second <em>A</em> instance, the
initializer list code will throw, invoking its catch block. Again
code execution won&rsquo;t make it into the constructor body, and we don&rsquo;t
see the destructor code in action.</p>

<p>The output of the program is:</p>

<p><code>plain output of initializer list variant
CTOR A
CATCH BLOCK MYCLASS
DTOR A
CAUGHT EXCEPTION
</code></p>

<p>Valgrind does not report a leak, so are we done now?</p>

<p>No, as the above code has a severe problem. It worked only
because we knew the second invocation of <em>createInstance</em> would fail.</p>

<p>But in the general case, either the first call or the second call
can fail. If the first call fails, then the initializer hasn&rsquo;t
initialized any of the object&rsquo;s members, and it would be unsafe to
delete any object members in the initializer&rsquo;s catch block. If the
second <em>createInstance</em> call fails, then the initializer has created
<em>a1</em> but not <em>a2</em>. To prevent a leak in this case, we should delete <em>a1</em>,
but we better don&rsquo;t delete <em>a2</em> yet.</p>

<p>But how do we tell in the catch block at what stage the initializer
list had thrown? There is no natural way to do this correctly without
introducing more state. And without that, we have the choice between
undefined behavior when deleting the not-yet-initialized object
members, and memory leaks when ignoring them.</p>

<h2>Not using pointers at all</h2>

<p>Note that if we wouldn&rsquo;t have used pointers for our managed <em>A</em> objects,
then we could have used the fact that destructors for all initialized
object members <strong>are</strong> actually called when object construction fails.</p>

<p>However, simple pointers don&rsquo;t have a destructor, so the objects they
point to remain and the memory is lost.</p>

<p>So one obvious solution for preventing memory leaks is to not use pointers,
and get rid of all <code>new</code> and <code>delete</code> statements.</p>

<p>In some situations we can probably get away with making the managed objects
regular class members of the class that manages them:</p>

<p>```cpp not using pointers
struct MyClass {
  A a1; // no pointer anymore!
  A a2; // no pointer anymore!</p>

<p>  MyClass ()</p>

<pre><code>: a1(),
  a2() {

std::cout &lt;&lt; "CTOR MYCLASS" &lt;&lt; std::endl;
</code></pre>

<p>  }</p>

<p>  ~MyClass () {</p>

<pre><code>std::cout &lt;&lt; "DTOR MYCLASS" &lt;&lt; std::endl;
// no delete statements needed anymore!
</code></pre>

<p>  }
};
```</p>

<p>Now if any of the <em>A</em> constructors will throw an exception during
initialization, everything will be cleaned up properly. Now we can make
use of the destructor of <em>A</em>. If <em>A</em> instances are not pointers but
regular objects, the destructors for already created instances will
be called normally, and no destructors will be called for the not-yet-initialized
<em>A</em> instances. That&rsquo;s how it should be. We don&rsquo;t get this benefit with
regular pointers, which don&rsquo;t have a destructor.</p>

<p>As an aside, we got rid of the <code>delete</code> statements in the destructor
and may even get away with the default destructor.</p>

<p>Obviously this is an easy and safe solution, but it also has a few
downsides. Here are a few (incomplete list):</p>

<ul>
<li>when compiling <em>MyClass</em>, the compiler will now need to know the
definition for class <em>A</em>. You can&rsquo;t get away with a simple forward
declaration for class <em>A</em> anymore as in the case when the class
only contained pointers to <em>A</em>.
So this solution increases the source code dependencies and coupling.</li>
<li>instances of managed objects (e.g. <em>A</em>) will need to be created when
the managing object (e.g. <em>MyClass</em>) is created. There is no way to
postpone the object creation as in the case of when using pointers.</li>
<li>in general, the lifetime of the managed objects is tied to the lifetime
of the managing object. This may or may not be ok, depending on
requirements.</li>
</ul>


<h2>Using smart pointers (e.g. std::unique_ptr)</h2>

<p>In many cases the superior alternative to all the above is using one
of the available smart pointer classes for managing resources.</p>

<p>The promise of smart pointers is that resource management becomes easier,
safer and more flexible with them.</p>

<p>Really useful smart pointers (this excludes <code>std::auto_ptr</code>) are part
of standard C++ since C++11, and to my knowledge they can be used in
all C++11-compatible compilers and even in some older ones. Apart from
that, smart pointers are available in Boost for a long time already.</p>

<p>In the following snippets, I&rsquo;ll be using smart pointers of type
<code>std::unique_ptr</code> as it is the perfect fit for this particular problem.
I won&rsquo;t cover <code>shared_ptr</code>, <code>weak_ptr</code> or other types of smart pointers
here.</p>

<p>When using an <code>std::unique_ptr</code> for managing the resources of <em>MyClass</em>,
the <em>MyClass</em> code becomes:</p>

<p>```cpp using std::unique_ptr</p>

<h1>include <memory></h1>

<p>struct MyClass {
  std::unique_ptr<A> a1;
  std::unique_ptr<A> a2;</p>

<p>  MyClass () :</p>

<pre><code>a1(createInstance()),
a2(createInstance(true)) {

std::cout &lt;&lt; "CTOR MYCLASS" &lt;&lt; std::endl;
</code></pre>

<p>  }</p>

<p>  ~MyClass () {</p>

<pre><code>std::cout &lt;&lt; "DTOR MYCLASS" &lt;&lt; std::endl;
</code></pre>

<p>  }
};
```</p>

<p>With a <code>unique_ptr</code>, we can still create resources when needed,
either in the initializer list, the constructor or even later. The
resources can still be created dynamically using <code>new</code> (as is still done
by function <em>createInstance</em>). When we&rsquo;re not taking the resources
away from the <code>unique_ptr</code>s, then they will free their managed
objects automatically and safely. We don&rsquo;t need to bother with <code>delete</code>.</p>

<p>And we don&rsquo;t need to bother with nested try&hellip;catch blocks either. If
anything goes wrong during object creation, any already assigned
<code>unique_ptr</code>s will happily release the resources they manage in their
own destructors.</p>

<p>It does not matter if the above code throws an exception in the first
invocation of <em>createInstance</em>, in the second or not at all: in every
case any allocated resources are released properly, and still there
is no need for any explicit exception handling or cleanup code. This is
what a smart pointer will do for us, behind the scenes.</p>

<p>Simply compare the following two code snippets, which both create three
instances of <em>A</em> while making sure no memory will be leaked if the
initialization goes wrong:</p>

<p>```cpp solution using smart pointers
std::unique_ptr<A> a1(createInstance());
std::unique_ptr<A> a2(createInstance());
std::unique_ptr<A> a3(createInstance());</p>

<p>// now do something with a1, a2, a3
// managed objects will be released automatically when
// the unique_ptrs go out of scope
// note: they may go out of scope unintentionally if
// some code below will throw an exception&hellip;
```</p>

<p>```cpp solution using nested try&hellip;catch blocks
A<em> a1 = nullptr;
A</em> a2 = nullptr;
A* a3 = nullptr;</p>

<p>a1 = new A;
try {
  a2 = new A;
  try {</p>

<pre><code>a3 = new A;
</code></pre>

<p>  }
  catch (&hellip;) {</p>

<pre><code>delete a2;
throw;
</code></pre>

<p>  }
}
catch (&hellip;) {
  delete a1;
  throw;
}</p>

<p>// now do something with a1, a2, a3
// objects a1, a2, a3 will not be released automatically
// when a1, a2, a3 go out of scope. any user of a1, a2, a3
// below must make sure to release the objects when they
// go out of scope or when an exception is thrown&hellip;
```</p>

<p>Obviously the smart pointer-based solution is less verbose,
but it is also safer and hard to get wrong. It is especially
useful for initializing and managing dynamically allocated
object members, because as we&rsquo;ve seen most of the other
ways to do this are either subtly broken or much more complex.</p>

<p>Apart from that, we can take the managed object from out of a
<code>unique_ptr</code> and take over responsibility for managing its
lifetime.</p>

<p>Further on the plus side, a class definition that contains
<code>unique_ptr</code>s can be compiled with only forward declarations
for the managed types. However, when the <code>unique_ptr</code> is a
regular object member, at least the class destructor
implementation will need to know the size of the managed type
so it can call <code>delete</code> properly.</p>

<p>The downside of using smart pointers is that they may impose
minimal overhead when compared to the pure pointer-based
solution. However in most cases this overhead should be
absolutely negligible or even be optimized away by the compiler.
It may make a difference though when compiling without any
optimizations, but this shouldn&rsquo;t matter too much in reality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How V8 Is Used in ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2015/08/01/how-v8-is-used-in-arangodb/"/>
    <updated>2015-08-01T19:06:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/08/01/how-v8-is-used-in-arangodb</id>
    <content type="html"><![CDATA[<p>ArangoDB allows running user-defined JavaScript code in the database.
This can be used for more complex, <em>stored procedures</em>-like database operations.
Additionally, ArangoDB&rsquo;s <a href="https://www.arangodb.com/foxx/">Foxx framework</a> can
be used to make any database functionality available via an HTTP REST API.
It&rsquo;s easy to build microservices with it, using the scripting functionality
for tasks like access control, data validation, sanitation etc.</p>

<p>We often get asked how the scripting functionality is implemented under the hood.
Additionally, several people have asked how ArangoDB&rsquo;s JavaScript functionality
relates to node.js.</p>

<p>This post tries to explain that in detail.</p>

<!-- more -->


<h1>The C++ parts</h1>

<p><em>arangosh</em>, the ArangoShell, and <em>arangod</em>, the database server, are written in
C++ and they are shipped as native code executables. Some parts of both <em>arangosh</em>
and <em>arangod</em> itself are written in JavaScript (more on that later).</p>

<p>The I/O handling in <em>arangod</em> is written in C++ and uses libev (written in C)
for the low-level event handling. All the socket I/O, working scheduling and
queueing is written in C++, too. These are parts that require high parallelism,
so we want this to run in multiple threads.</p>

<p>All the indexes, the persistence layer and many of the fundamental operations,
like the ones for document inserts, updates, deletes, imports are written in C++ for
effective control of memory usage and parallelism. AQL&rsquo;s query parser is written
using the usual combination of Flex and Bison, which generate C files that are
compiled to native code. The AQL optimizer, AQL executor and many AQL functions are
writting in C++ as well.</p>

<p>Some AQL functions however, are written in JavaScript. And if an AQL query
invokes a user-defined function, this function will be a JavaScript function, too.</p>

<h1>How ArangoDB uses V8</h1>

<p>How is JavaScript code executed in ArangoDB?</p>

<p>Both <em>arangosh</em> and <em>arangod</em> are linked against the <a href="https://developers.google.com/v8/">V8 JavaScript engine</a>
library. V8 (itself written in C++) is the component that runs the JavaScript
code in ArangoDB.</p>

<p>V8 requires JavaScript code to run in a so-called <em>isolate</em> (note: I&rsquo;ll be
oversimplifying a bit here &ndash; in reality there are isolates and contexts).
As the name suggests, isolates are completely isolated from each other.
Especially, data cannot be shared or moved across isolates, and each isolate
can be used by only one thread at a time.</p>

<p>Let&rsquo;s look at how <em>arangosh</em>, the ArangoShell, uses V8. All JavaScript commands
entered in <em>arangosh</em> will be compiled and executing with V8 immediately.
In <em>arangosh</em>, this happens using a single V8 isolate.</p>

<p>On the server side, things are a bit different. In <em>arangod</em>, there are multiple
V8 isolates. The number of isolates to create is a startup configuration
option (<code>--javascript.v8-contexts</code>). Creating multiple isolates allows running
JavaScript code in multiple threads, truly parallel. Apart from that, <em>arangod</em>
has multiple I/O threads (<code>--scheduler.threads</code> configuration option) for handling
the communication with client applications.</p>

<p>As mentioned earlier, part of ArangoDB&rsquo;s codebase itself is written in JavaScript,
and this JavaScript code is executed the same way as any user-defined will be executed.</p>

<h1>Executing JavaScript code with V8</h1>

<p>For executing any JavaScript code (built-in or user-defined), ArangoDB will invoke
V8&rsquo;s JIT compiler to compile the script code into native code and run it.</p>

<p>The JIT compiler in V8 will not try extremely hard to optimize the code on the
first invocation. On initial compilation, it will aim for a good balance of
optimizations and fast compilation time. If it finds some code parts are called
often, it may re-try to optimize these parts more aggressively automatically.
To make things even more complex, there are different JIT compilers in V8
(i.e. Crankshaft and Turbofan) with different sweet spots. JavaScript modes
(i.e. <em>strict mode</em> and <em>strong mode</em>) can also affect the level of optimizations
the compilers will carry out.</p>

<p>Now, after the JavaScript code has been compiled to native code, V8 will run it
until it returns or fails with an uncaught exception.</p>

<p>But how can the JavaScript code access the database data and server internals?
In other words, what actually happens if a JavaScript command such as the following
is executed?</p>

<p><code>js example JavaScript command
db.myCollection.save({ _key: "test" });
</code></p>

<h2>Accessing server internals from JavaScript</h2>

<p>Inside <em>arangod</em>, each V8 isolate is equipped with a global variable named <code>db</code>.
This JavaScript variable is a wrapper around database functionality written in C++.
When the <code>db</code> object is created, we tell V8 that its methods are C++ callbacks.</p>

<p>Whenever the <code>db</code> object is accessed in JavaScript, the V8 engine will therefore
call C++ methods. These provide full access to the server internals, can do whatever
is required and return data in the format that V8 requires. V8 then makes the
return data accessible to the JavaScript code.</p>

<p>Executing <code>db.myCollection.save(...)</code> is effectively two operations: accessing the
property <code>myCollection</code> on the object <code>db</code> and then calling function <code>save</code> on that
property. For the first operation, V8 will invoke the object&rsquo;s <code>NamedPropertyHandler</code>,
which is a C++ function that is responsible for returning the value for the property
with the given name (<code>myCollection</code>). In the case of <code>db</code>, we have a C++ function
that collection object if it exists, or <code>undefined</code> if not.</p>

<p>The collection object again has C++ bindings in the background, so calling function
<code>save</code> on it will call another C++ function. The collection object also has a (hidden)
pointer to the C++ collection. When <code>save</code> is called, we will extract that pointer
from the <code>this</code> object so we know which C++ data structures to work on. The <code>save</code>
function will also get the to-be-inserted document data as its payload. V8 will
pass this to the C++ function as well so we can validate it and convert it into
our internal data format.</p>

<p>On the server side, there are several objects exposed to JavaScript that have C++
bindings. There are also non-object functions that have C++ bindings. Some of these
functions are also bolted on regular JavaScript objects.</p>

<h2>Accessing server internals from ArangoShell</h2>

<p>When running the same command in <em>arangosh</em>, things will be completely different.
The ArangoShell may run on the same host as the <em>arangod</em> server process, but it may
also run on a completely different one. Providing <em>arangosh</em> access to server internals
such as pointers will therefore not work in general. Even if <em>arangosh</em> and <em>arangod</em>
do run on the same host, they are independent processes with no access to the each
other&rsquo;s data. The latter problem could be solved by having a shared memory segment
that both <em>arangosh</em> and <em>arangod</em> can use, but why bother with that special case
which will provide no help in the general case when the shell can be located on
<strong>any</strong> host.</p>

<p>To make the shell work in all these situations, it uses the HTTP REST API provided
by the ArangoDB server to talk to it. For <em>arangod</em>, any ArangoShell client is just
another client, with no special treatments or protocols.</p>

<p>As a consequence, all operations on databases and collections run from the ArangoShell
are JavaScript wrappers that call their respective server-side HTTP APIs.</p>

<p>Recalling the command example again (<code>db.myCollection.save(...)</code>), the shell will first
access the property <code>myCollection</code> of the object <code>db</code>. In the shell <code>db</code> is a regular
JavaScript object with no C++ bindings. When the shell is started, it will make an
HTTP call to <em>arangod</em> to retrieve a list of all available collections, and register
them as properties in its <code>db</code> object. Calling the <code>save</code> method on one of these
objects will trigger an HTTP POST request to the server API at <code>/_api/document?collection=myCollection</code>,
with the to-be-inserted data in its request body. Eventually the server will respond
and the command will return with the data retrieved from the server.</p>

<h2>Considerations</h2>

<p>Consider running the following JavaScript code:</p>

<p><code>js code to insert 1000 documents
for (var i = 0; i &lt; 1000; ++i) {
  db.myCollection.save({ _key: "test" + i });
}
</code></p>

<p>When run from inside the ArangoShell, the code will be executed in there. The shell will
perform an HTTP request to <em>arangod</em> for each call to <code>save</code>. We&rsquo;ll end up with 1,000
HTTP requests.</p>

<p>Running the same code inside <em>arangod</em> will trigger no HTTP requests, as the server-side
functions are backed with C++ internals and can access the database data directly. It will
be a lot faster to run this loop on the server than in <em>arangosh</em>. A while ago I wrote
<a href="/blog/2014/08/30/understanding-where-operations-are-executed/">another article</a> about this.</p>

<p>When replacing the ArangoShell with another client application, things are no different.
A client application will not have access to the server internals, so all it can do is to
make requests to the server (by the way, the principle would be no different if we used
MySQL or other database servers, only the protocols would vary).</p>

<p>Fortunately, there is a fix for this: making the code run server-side. For example, the
above code can be put into a Foxx route. This way it is not only fast but will be made
accessible via an HTTP REST API so client applications can call it with a single HTTP request.</p>

<p>In reality, database operations will be more complex than in the above example. And this
is where having a full-featured scripting language like JavaScript helps. It provides all
the features that are needed for more complex tasks such as validating and sanitizing input
data, access control, executing database queries and postprocessing results.</p>

<h1>The differences to node.js</h1>

<p>To start with: ArangoDB is not node.js, and vice versa. ArangoDB is not a node.js module
either. ArangoDB and node.js are completely indepedent.</p>

<p>But there is a commonality: both ArangoDB and node.js use the V8 engine for running
JavaScript code.</p>

<h2>Threading</h2>

<p>AFAIK, standard node.js only has a single V8 isolate to run all code in.
While that made the implementation easier (no hassle with multi-threading) it
also limits node.js to using only a single CPU.</p>

<p>It&rsquo;s not unusual to see a multi-core server with a node.js instance maxing out
one CPU while the other CPUs are sitting idle. In order to max out a multi-core
server, people often start multiple node.js instances on a single server. That will
work fine, but the node.js instances will be independent, and sharing data between
them is not possible in plain JavaScript.</p>

<p>And because a node.js instance is single-threaded, it is also important that
code written for node.js is non-blocking. Code that blocks while waiting for
some I/O operation would block the only available CPU. Using non-blocking
I/O operations allows node.js to queue the operation, and execute other code
in the meantime, allowing overall progress. This also makes it look like it
would be executing multiple actions in parallel, while it is actually executing
them sequentially.</p>

<p>Contrary, <em>arangod</em> is a multi-threaded server. It can serve multiple requests in
parallel, using multiple CPUs. Because <em>arangod</em> has multiple V8 isolates that
each can execute JavaScript code, it can run JavaScript in multiple threads in parallel.</p>

<p><em>arangosh</em>, the ArangoShell, is single-threaded and provides only a single V8 isolate.</p>

<h2>Usage of modules</h2>

<p>Both node.js and ArangoDB can load code at runtime so it can be organized into
modules or libraries. In both, extra JavaScript modules can be loaded using the
<code>require</code> function.</p>

<p>There is often confusion about whether node.js modules can be used in ArangoDB.
This is probably because the answer is &ldquo;<em>it depends!</em>&rdquo;.</p>

<p>node.js packages can be written in JavaScript but they can also compile to native
code using C++. The latter can be used to extend the functionality of node.js with
features that JavaScript alone wouldn&rsquo;t be capable of. Such modules however often
heavily depend on a specific V8 version (so do not necessarily compile in a node.js
version with a different version of V8) and often rely on node.js internals.</p>

<p>ArangoDB can load modules that are written in pure JavaScript. Modules that
depend on non-JavaScript functionality (such as native modules for node.js) or modules
that rely on node.js internals cannot be loaded in ArangoDB. As a rule of thumb,
any module will run in ArangoDB that is implemented in pure JavaScript, does not
access global variables and only requires other modules that obey the same restrictions.</p>

<p>ArangoDB also uses several externally maintained JavaScript-only libraries, such as
underscore.js. This module will run everywhere because it conforms to the mentioned
restrictions.</p>

<p>ArangoDB also uses several other modules that are maintained on npm.js.
An example module is <a href="https://www.npmjs.com/package/aqb">AQB</a>, a query builder for AQL.
It is written in pure JavaScript too, so it can be used from a node.js application and
from within ArangoDB. If there is an updated version of this module, we use npm to
install it in a subdirectory of ArangoDB. As per npm convention, the node.js modules
shipped with ArangoDB reside in a directory named <code>node_modules</code>. Probably this is
what caused some of the confusion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Less Intrusive Linking]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/less-intrusive-linking/"/>
    <updated>2015-05-07T19:52:53+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/less-intrusive-linking</id>
    <content type="html"><![CDATA[<p>A while ago our continuous integration builds on <a href="http://travis-ci.org">TravisCI</a>
began to fail seemingly randomly because the build worker was killed without
an apparent reason. Obviously the build process reached some resource limits
though we couldn&rsquo;t find any documented limit that the build obviously violated.</p>

<p>Some builds still succeeded without issues, but those builds that were killed
had one thing in common: they were all stuck waiting the linker to finish.</p>

<p>The default linker used on TravisCI is <em>GNU ld</em>. After some research, it turned
out that replacing <em>GNU ld</em> with <em>GNU gold</em> not only made the linking much
faster, but also less resource-intensive. Linking ArangoDB on my local machine
is almost twice as fast with <em>gold</em> as with <em>ld</em>. Even better, after reconfiguring
our TravisCI builds to also use <em>gold</em>, our builds weren&rsquo;t killed anymore by
TravisCI&rsquo;s build scheduling system.</p>

<p>To make TravisCI use <em>gold</em> instead of <em>ld</em>, add the following to your project&rsquo;s
<code>.travis.yml</code> in the <code>install</code> section (so it gets execute before the actual build
steps):</p>

<p><code>bash commands for wrapping gold
sudo apt-get -y install binutils-gold
mkdir -p ~/bin/gold
echo '#!/bin/bash' &gt; ~/bin/gold/ld
echo 'gold "$@"' &gt;&gt; ~/bin/gold/ld
chmod a+x ~/bin/gold/ld
export CFLAGS="-B$HOME/bin/gold $CFLAGS"
export CXXFLAGS="-B$HOME/bin/gold $CXXFLAGS"
</code></p>

<p>The script downloads and installs <em>gold</em> and creates a tiny wrapper script in a
file named <code>ld</code> in the user&rsquo;s home directory. The wrapper simply calls <em>gold</em>
with all the arguments passed to the wrapper. Finally, the script modifies the
environments <code>CFLAGS</code> and <code>CXXFLAGS</code> by setting the <code>-B</code> parameter to the
wrapper script&rsquo;s directory.</p>

<p><code>-B</code> is the option for the compiler&rsquo;s search path. The compiler (g++) at least
will look in this path for any helper tools it invokes. As we have a file named
<code>ld</code> in this directory, g++ will use our wrapper script instead of the original
<code>ld</code> binary. This way we can keep the original version of <code>ld</code> in <code>/usr/bin</code>,
and only override it using environment variables. This is also helpful in
other contexts, e.g. when <code>ld</code> shall remain as the system&rsquo;s default linker but
<code>gold</code>shall only be used for linking a few selected components.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Ccache When Working With Different Branches]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/07/using-ccache-when-working-with-different-branches/"/>
    <updated>2015-02-07T17:00:15+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/07/using-ccache-when-working-with-different-branches</id>
    <content type="html"><![CDATA[<p>Git makes working with many different branches in the same local repository easy and efficient.</p>

<p>In a C/C++ project, the code must be re-compiled after switching into another branch.
If the branches only differ minimally, running <code>make</code> will only re-compile the parts that are
actually different, and after that re-link them. That won&rsquo;t take too long, though especially
link times can be annoying.</p>

<p>However, if there are differences in central header files that are included from every file,
then <code>make</code> has no option but to <strong>re-compile everything</strong>. This can take significant amounts of
time (and coffee).</p>

<p>I just realized that there is a solution to speed up re-compilation in this situation:
<a href="http://linux.die.net/man/1/ccache">ccache</a>!</p>

<!-- more -->


<h2>Why ccache can help</h2>

<p>ccache is a wrapper for the actual compiler command. It will call the compiler with the specified
arguments, and capture the compiler output. When called again with the same arguments, it will
look in its internal cache for a ready-to-serve result. If one is present, it will return it
without invoking the compiler again. Otherwise, or if it detects some changes that forbid serving
outdated results from the cache, it will transparently invoke the compiler.</p>

<p>When switching back to another branch that you had already compiled before, running <code>make</code>
may re-build <em>everything</em> due to changes in headers. But it is not unlikely that you had built
the branch before already. If so, and ccache was involved in the previous build, it may still
have all the info required for re-compilation in its cache.</p>

<p>And everyone will be happy: <code>make</code> will run its full rebuild, but most operations won&rsquo;t be handed
to the compiler because ccache is sitting in between, serving results from its cache.
And you as a developer won&rsquo;t lose that much time.</p>

<h2>Some figures</h2>

<p>Following are some figures demonstrating its potential when running a <code>make</code> in the devel branch
after having returned from a different branch with significant changes.</p>

<h3>With ccache, but cache empty</h3>

<p><code>plain time make
real  12m43.501s
user  11m52.550s
sys 0m44.110s
</code></p>

<h3>With ccache, everything in cache</h3>

<p><code>plain time make
real  0m55.572s
user  0m26.346s
sys 0m7.551s
</code></p>

<p>That&rsquo;s a <strong>build time reduction of more than 90 %</strong>!</p>

<p>This is already the optimal result, as everything was already present in the cache.
However, the situation was not unrealistic. I often switch into another branch, try something
out or commit a small change, and the return to the original branch. I already started having
many separate directories for the different branches to avoid frequent recompilation.
ccache can be relief here.</p>

<p>By the way, timing results are from my laptop. I did not bother to run <code>make</code> with parallel
jobs as this has limited effect on my laptop, though on more decent hardware it may be beneficial
both with and without ccache, though I guess, with many parallel jobs and a full cache, linking
will become the most expensive part.</p>

<h2>How to use ccache</h2>

<p>For Ubuntu, ccache is available in package <code>ccache</code>. You can easily install it with:
<code>bash Installing ccache on Ubuntu
sudo apt-get install ccache
</code></p>

<p>The most convenient way to use ccache in your build is to change your <code>CC</code> and <code>CXX</code>
environment variables as follows:
<code>bash setting compilers environment variables
export CC="ccache gcc"
export CXX="ccache g++"
</code></p>

<p>I suggest putting that into <code>.bashrc</code> so the variables will be set in every session and not
just once. After that, running <code>configure</code> will write a <code>Makefile</code> that will use ccache for
building object files.</p>

<p>Note: that will change these environment variables globally, so ccache may be used for other
projects, too.</p>

<h3>What ccache cannot do</h3>

<p>I already forgot about ccache because when working in a single branch it does not provide that
many benefits. When making changes to your code, you can be pretty sure the new code won&rsquo;t be
in the cache yet. Running <code>make</code> then will invoke ccache, but this will result in a cache miss.
It cannot help here, because the new code was never compiled before and thus in no cache.</p>

<p>Additionally, <code>make</code> is smart enough on its own to only re-build the parts of the program that
have actually changed or depend on the changes you made.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Core Dumps of Failed TravisCI Builds]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/30/getting-core-dumps-of-failed-travisci-builds/"/>
    <updated>2014-10-30T23:05:48+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/30/getting-core-dumps-of-failed-travisci-builds</id>
    <content type="html"><![CDATA[<p>I recently wrote about <a href="/blog/2014/10/17/using-travis-ci-for-a-c-plus-plus-11-project/">using TravisCI for continuously testing C++11 projects</a>.</p>

<p><strong>Now, what if a build fails?</strong></p>

<p>Especially for C and C++ projects, build failures may mean crashed
programs. In a local setup, the usual way to analyze program crashes
is to manually inspect the core files that are written on crash.</p>

<p>With TravisCI, there is no way to log in to a build machine and
inspect a core dump interactively. There is no SSH access to
the build machines. TravisCI does not even persist any state of
builds but the result and the log output.</p>

<p>There is a way to get to the core dumps, but it was fiddly to find
out and set up.</p>

<!-- more -->


<p>The basic idea is to run <code>gdb</code> on the TravisCI build machine
automatically when a build fails. <code>gdb</code> can be scripted, so all
we need to do is to make it print a backtrace in all threads at
the time of the crash.</p>

<p>By default, no core dumps will be produced on TravisCI. To turn them
on, an appropriate ulimit value must be set. We also need to install
<code>gdb</code> so we can actually run it. Here is the <code>.travis.yml</code> adjustment
for these prerequisites:</p>

<p>```yaml adjustments for install and before_script hooks
install:
&ndash; sudo apt-get install -y gdb  # install gdb</p>

<p>before_script:
&ndash; ulimit -c unlimited -S       # enable core dumps
```</p>

<p>To get an idea of where the program crashed, we can finally install
an <code>after_failure</code> hook. This hook can check for a core file and use
<code>gdb</code> to print a nice backtrace.</p>

<p>The core file pattern on TravisCI seems to be <code>core-%p</code>, so core
filenames will include the executable&rsquo;s process id and change on
every run. We can use <code>find</code> to look for files named <code>core*</code> in the
cwd and pick the first one as there should only be at most one core
file per build:</p>

<p><code>yaml adjustments for after_failure hook
after_failure:
- COREFILE=$(find . -maxdepth 1 -name "core*" | head -n 1) # find core file
- if [[ -f "$COREFILE" ]]; then gdb -c "$COREFILE" example -ex "thread apply all bt" -ex "set pagination 0" -batch; fi
</code></p>

<p>A failed build might produce output like this:</p>

<p><img src="/downloads/screenshots/travis-ci-gdb.png"></p>

<p>I recommend compiling the executable to test with debug symbols on and
with all optimizations turned off (i.e. compiler options <code>-g -O0</code>).
Otherwise backtraces might reveal less useful information for debugging.</p>

<p>On a side note: the <a href="http://lint.travis-ci.org/">Travis WebLint</a> is a
handy tool for validating <code>.travis.yml</code> files <em>before</em> pushing them.</p>
]]></content>
  </entry>
  
</feed>
