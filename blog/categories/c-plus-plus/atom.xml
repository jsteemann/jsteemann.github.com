<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: C++ | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/c-plus-plus/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-08-01T23:40:26+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How V8 Is Used in ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2015/08/01/how-v8-is-used-in-arangodb/"/>
    <updated>2015-08-01T19:06:04+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/08/01/how-v8-is-used-in-arangodb</id>
    <content type="html"><![CDATA[<p>ArangoDB allows running user-defined JavaScript code in the database.
This can be used for more complex, <em>stored procedures</em>-like database operations.
Additionally, ArangoDB&rsquo;s <a href="https://www.arangodb.com/foxx/">Foxx framework</a> can
be used to make any database functionality available via an HTTP REST API.
It&rsquo;s easy to build microservices with it, using the scripting functionality
for tasks like access control, data validation, sanitation etc.</p>

<p>We often get asked how the scripting functionality is implemented under the hood.
Additionally, several people have asked how ArangoDB&rsquo;s JavaScript functionality
relates to node.js.</p>

<p>This post tries to explain that in detail.</p>

<!-- more -->


<h1>The C++ parts</h1>

<p><em>arangosh</em>, the ArangoShell, and <em>arangod</em>, the database server, are written in
C++ and they are shipped as native code executables. Some parts of both <em>arangosh</em>
and <em>arangod</em> itself are written in JavaScript (more on that later).</p>

<p>The I/O handling in <em>arangod</em> is written in C++ and uses libev (written in C)
for the low-level event handling. All the socket I/O, working scheduling and
queueing is written in C++, too. These are parts that require high parallelism,
so we want this to run in multiple threads.</p>

<p>All the indexes, the persistence layer and many of the fundamental operations,
like the ones for document inserts, updates, deletes, imports are written in C++ for
effective control of memory usage and parallelism. AQL&rsquo;s query parser is written
using the usual combination of Flex and Bison, which generate C files that are
compiled to native code. The AQL optimizer, AQL executor and many AQL functions are
writting in C++ as well.</p>

<p>Some AQL functions however, are written in JavaScript. And if an AQL query
invokes a user-defined function, this function will be a JavaScript function, too.</p>

<h1>How ArangoDB uses V8</h1>

<p>How is JavaScript code executed in ArangoDB?</p>

<p>Both <em>arangosh</em> and <em>arangod</em> are linked against the <a href="https://developers.google.com/v8/">V8 JavaScript engine</a>
library. V8 (itself written in C++) is the component that runs the JavaScript
code in ArangoDB.</p>

<p>V8 requires JavaScript code to run in a so-called <em>isolate</em> (note: I&rsquo;ll be
oversimplifying a bit here &ndash; in reality there are isolates and contexts).
As the name suggests, isolates are completely isolated from each other.
Especially, data cannot be shared or moved across isolates, and each isolate
can be used by only one thread at a time.</p>

<p>Let&rsquo;s look at how <em>arangosh</em>, the ArangoShell, uses V8. All JavaScript commands
entered in <em>arangosh</em> will be compiled and executing with V8 immediately.
In <em>arangosh</em>, this happens using a single V8 isolate.</p>

<p>On the server side, things are a bit different. In <em>arangod</em>, there are multiple
V8 isolates. The number of isolates to create is a startup configuration
option (<code>--javascript.v8-contexts</code>). Creating multiple isolates allows running
JavaScript code in multiple threads, truly parallel. Apart from that, <em>arangod</em>
has multiple I/O threads (<code>--scheduler.threads</code> configuration option) for handling
the communication with client applications.</p>

<p>As mentioned earlier, part of ArangoDB&rsquo;s codebase itself is written in JavaScript,
and this JavaScript code is executed the same way as any user-defined will be executed.</p>

<h1>Executing JavaScript code with V8</h1>

<p>For executing any JavaScript code (built-in or user-defined), ArangoDB will invoke
V8&rsquo;s JIT compiler to compile the script code into native code and run it.</p>

<p>The JIT compiler in V8 will not try extremely hard to optimize the code on the
first invocation. On initial compilation, it will aim for a good balance of
optimizations and fast compilation time. If it finds some code parts are called
often, it may re-try to optimize these parts more aggressively automatically.
To make things even more complex, there are different JIT compilers in V8
(i.e. Crankshaft and Turbofan) with different sweet spots. JavaScript modes
(i.e. <em>strict mode</em> and <em>strong mode</em>) can also affect the level of optimizations
the compilers will carry out.</p>

<p>Now, after the JavaScript code has been compiled to native code, V8 will run it
until it returns or fails with an uncaught exception.</p>

<p>But can how the JavaScript code access the database data and server internals?
In other words, what actually happens if a JavaScript command such as the following
is executed?</p>

<p><code>js example JavaScript command
db.myCollection.save({ _key: "test" });
</code></p>

<h2>Accessing server internals from JavaScript</h2>

<p>Inside <em>arangod</em>, each V8 isolate is equipped with a global variable named <code>db</code>.
This JavaScript variable is a wrapper around database functionality written in C++.
When the <code>db</code> object is created, we tell V8 that its methods are C++ callbacks.</p>

<p>Whenever the <code>db</code> object is accessed in JavaScript, the V8 engine will therefore
call C++ methods. These provide full access to the server internals, can do whatever
is required and return data in the format that V8 requires. V8 then makes the
return data accessible to the JavaScript code.</p>

<p>Executing <code>db.myCollection.save(...)</code> is effectively two operations: accessing the
property <code>myCollection</code> on the object <code>db</code> and then calling function <code>save</code> on that
property. For the first operation, V8 will invoke the object&rsquo;s <code>NamedPropertyHandler</code>,
which is a C++ function that is responsible for returning the value for the property
with the given name (<code>myCollection</code>). In the case of <code>db</code>, we have a C++ function
that collection object if it exists, or <code>undefined</code> if not.</p>

<p>The collection object again has C++ bindings in the background, so calling function
<code>save</code> on it will call another C++ function. The collection object also has a (hidden)
pointer to the C++ collection. When <code>save</code> is called, we will extract that pointer
from the <code>this</code> object so we know which C++ data structures to work on. The <code>save</code>
function will also get the to-be-inserted document data as its payload. V8 will
pass this to the C++ function as well so we can validate it and convert it into
our internal data format.</p>

<p>On the server side, there are several objects exposed to JavaScript that have C++
bindings. There are also non-object functions that have C++ bindings. Some of these
functions are also bolted on regular JavaScript objects.</p>

<h2>Accessing server internals from ArangoShell</h2>

<p>When running the same command in <em>arangosh</em>, things will be completely different.
The ArangoShell may run on the same host as the <em>arangod</em> server process, but it may
also run on a completely different one. Providing <em>arangosh</em> access to server internals
such as pointers will therefore not work in general. Even if <em>arangosh</em> and <em>arangod</em>
do run on the same host, they are independent processes with no access to the each
other&rsquo;s data. The latter problem could be solved by having a shared memory segment
that both <em>arangosh</em> and <em>arangod</em> can use, but why bother with that special case
which will provide no help in the general case when the shell can be located on
<strong>any</strong> host.</p>

<p>To make the shell work in all these situations, it uses the HTTP REST API provided
by the ArangoDB server to talk to it. For <em>arangod</em>, any ArangoShell client is just
another client, with no special treatments or protocols.</p>

<p>As a consequence, all operations on databases and collections run from the ArangoShell
are JavaScript wrappers that call their respective server-side HTTP APIs.</p>

<p>Recalling the command example again (<code>db.myCollection.save(...)</code>), the shell will first
access the property <code>myCollection</code> of the object <code>db</code>. In the shell <code>db</code> is a regular
JavaScript object with no C++ bindings. When the shell is started, it will make an
HTTP call to <em>arangod</em> to retrieve a list of all available collections, and register
them as properties in its <code>db</code> object. Calling the <code>save</code> method on one of these
objects will trigger an HTTP POST request to the server API at <code>/_api/document?collection=myCollection</code>,
with the to-be-inserted data in its request body. Eventually the server will respond
and the command will return with the data retrieved from the server.</p>

<h2>Considerations</h2>

<p>Consider running the following JavaScript code:</p>

<p><code>js code to insert 1000 documents
for (var i = 0; i &lt; 1000; ++i) {
  db.myCollection.save({ _key: "test" + i });
}
</code></p>

<p>When run from inside the ArangoShell, the code will be executed in there. The shell will
perform an HTTP request to <em>arangod</em> for each call to <code>save</code>. We&rsquo;ll end up with 1,000
HTTP requests.</p>

<p>Running the same code inside <em>arangod</em> will trigger no HTTP requests, as the server-side
functions are backed with C++ internals and can access the database data directly. It will
be a lot faster to run this loop on the server than in <em>arangosh</em>. A while ago I wrote
<a href="/blog/2014/08/30/understanding-where-operations-are-executed/">another article</a> about this.</p>

<p>When replacing the ArangoShell with another client application, things are no different.
A client application will not have access to the server internals, so all it can do is to
make requests to the server (by the way, the principle would be no different if we used
MySQL or other database servers, only the protocols would vary).</p>

<p>Fortunately, there is a fix for this: making the code run server-side. For example, the
above code can be put into a Foxx route. This way it is not only fast but will be made
accessible via an HTTP REST API so client applications can call it with a single HTTP request.</p>

<p>In reality, database operations will be more complex than in the above example. And this
is where having a full-featured scripting language like JavaScript helps. It provides all
the features that are needed for more complex tasks such as validating and sanitizing input
data, access control, executing database queries and postprocessing results.</p>

<h1>The differences to node.js</h1>

<p>To start with: ArangoDB is not node.js, and vice versa. ArangoDB is not a node.js module
either. ArangoDB and node.js are completely indepedent.</p>

<p>But there is a commonality: both ArangoDB and node.js use the V8 engine for running
JavaScript code.</p>

<h2>Threading</h2>

<p>AFAIK, standard node.js only has a single V8 isolate to run all code in.
While that made the implementation easier (no hassle with multi-threading) it
also limits node.js to using only a single CPU.</p>

<p>It&rsquo;s not unusual to see a multi-core server with a node.js instance maxing out
one CPU while the other CPUs are sitting idle. In order to max out a multi-core
server, people often start multiple node.js instances on a single server. That will
work fine, but the node.js instances will be independent, and sharing data between
them is not possible in plain JavaScript.</p>

<p>And because a node.js instance is single-threaded, it is also important that
code written for node.js is non-blocking. Code that blocks while waiting for
some I/O operation would block the only available CPU. Using non-blocking
I/O operations allows node.js to queue the operation, and execute other code
in the meantime, allowing overall progress. This also makes it look like it
would be executing multiple actions in parallel, while it is actually executing
them sequentially.</p>

<p>Contrary, <em>arangod</em> is a multi-threaded server. It can serve multiple requests in
parallel, using multiple CPUs. Because <em>arangod</em> has multiple V8 isolates that
each can execute JavaScript code, it can run JavaScript in multiple threads in parallel.</p>

<p><em>arangosh</em>, the ArangoShell, is single-threaded and provides only a single V8 isolate.</p>

<h2>Usage of modules</h2>

<p>Both node.js and ArangoDB can load code at runtime so it can be organized into
modules or libraries. In both, extra JavaScript modules can be loaded using the
<code>require</code> function.</p>

<p>There is often confusion about whether node.js modules can be used in ArangoDB.
This is probably because the answer is &ldquo;<em>it depends!</em>&rdquo;.</p>

<p>node.js packages can be written in JavaScript but they can also compile to native
code using C++. The latter can be used to extend the functionality of node.js with
features that JavaScript alone wouldn&rsquo;t be capable of. Such modules however often
heavily depend on a specific V8 version (so do not necessarily compile in a node.js
version with a different version of V8) and often rely on node.js internals.</p>

<p>ArangoDB can load modules that are written in pure JavaScript. Modules that
depend on non-JavaScript functionality (such as native modules for node.js) or modules
that rely on node.js internals cannot be loaded in ArangoDB. As a rule of thumb,
any module will run in ArangoDB that is implemented in pure JavaScript, does not
access global variables and only requires other modules that obey the same restrictions.</p>

<p>ArangoDB also uses several externally maintained JavaScript-only libraries, such as
underscore.js. This module will run everywhere because it conforms to the mentioned
restrictions.</p>

<p>ArangoDB also uses several other modules that are maintained on npm.js.
An example module is <a href="https://www.npmjs.com/package/aqb">AQB</a>, a query builder for AQL.
It is written in pure JavaScript too, so it can be used from a node.js application and
from within ArangoDB. If there is an updated version of this module, we use npm to
install it in a subdirectory of ArangoDB. As per npm convention, the node.js modules
shipped with ArangoDB reside in a directory named <code>node_modules</code>. Probably this is
what caused some of the confusion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Less Intrusive Linking]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/less-intrusive-linking/"/>
    <updated>2015-05-07T19:52:53+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/less-intrusive-linking</id>
    <content type="html"><![CDATA[<p>A while ago our continuous integration builds on <a href="http://travis-ci.org">TravisCI</a>
began to fail seemingly randomly because the build worker was killed without
an apparent reason. Obviously the build process reached some resource limits
though we couldn&rsquo;t find any documented limit that the build obviously violated.</p>

<p>Some builds still succeeded without issues, but those builds that were killed
had one thing in common: they were all stuck waiting the linker to finish.</p>

<p>The default linker used on TravisCI is <em>GNU ld</em>. After some research, it turned
out that replacing <em>GNU ld</em> with <em>GNU gold</em> not only made the linking much
faster, but also less resource-intensive. Linking ArangoDB on my local machine
is almost twice as fast with <em>gold</em> as with <em>ld</em>. Even better, after reconfiguring
our TravisCI builds to also use <em>gold</em>, our builds weren&rsquo;t killed anymore by
TravisCI&rsquo;s build scheduling system.</p>

<p>To make TravisCI use <em>gold</em> instead of <em>ld</em>, add the following to your project&rsquo;s
<code>.travis.yml</code> in the <code>install</code> section (so it gets execute before the actual build
steps):</p>

<p><code>bash commands for wrapping gold
sudo apt-get -y install binutils-gold
mkdir -p ~/bin/gold
echo '#!/bin/bash' &gt; ~/bin/gold/ld
echo 'gold "$@"' &gt;&gt; ~/bin/gold/ld
chmod a+x ~/bin/gold/ld
export CFLAGS="-B$HOME/bin/gold $CFLAGS"
export CXXFLAGS="-B$HOME/bin/gold $CXXFLAGS"
</code></p>

<p>The script downloads and installs <em>gold</em> and creates a tiny wrapper script in a
file named <code>ld</code> in the user&rsquo;s home directory. The wrapper simply calls <em>gold</em>
with all the arguments passed to the wrapper. Finally, the script modifies the
environments <code>CFLAGS</code> and <code>CXXFLAGS</code> by setting the <code>-B</code> parameter to the
wrapper script&rsquo;s directory.</p>

<p><code>-B</code> is the option for the compiler&rsquo;s search path. The compiler (g++) at least
will look in this path for any helper tools it invokes. As we have a file named
<code>ld</code> in this directory, g++ will use our wrapper script instead of the original
<code>ld</code> binary. This way we can keep the original version of <code>ld</code> in <code>/usr/bin</code>,
and only override it using environment variables. This is also helpful in
other contexts, e.g. when <code>ld</code> shall remain as the system&rsquo;s default linker but
<code>gold</code>shall only be used for linking a few selected components.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Ccache When Working With Different Branches]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/07/using-ccache-when-working-with-different-branches/"/>
    <updated>2015-02-07T17:00:15+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/07/using-ccache-when-working-with-different-branches</id>
    <content type="html"><![CDATA[<p>Git makes working with many different branches in the same local repository easy and efficient.</p>

<p>In a C/C++ project, the code must be re-compiled after switching into another branch.
If the branches only differ minimally, running <code>make</code> will only re-compile the parts that are
actually different, and after that re-link them. That won&rsquo;t take too long, though especially
link times can be annoying.</p>

<p>However, if there are differences in central header files that are included from every file,
then <code>make</code> has no option but to <strong>re-compile everything</strong>. This can take significant amounts of
time (and coffee).</p>

<p>I just realized that there is a solution to speed up re-compilation in this situation:
<a href="http://linux.die.net/man/1/ccache">ccache</a>!</p>

<!-- more -->


<h2>Why ccache can help</h2>

<p>ccache is a wrapper for the actual compiler command. It will call the compiler with the specified
arguments, and capture the compiler output. When called again with the same arguments, it will
look in its internal cache for a ready-to-serve result. If one is present, it will return it
without invoking the compiler again. Otherwise, or if it detects some changes that forbid serving
outdated results from the cache, it will transparently invoke the compiler.</p>

<p>When switching back to another branch that you had already compiled before, running <code>make</code>
may re-build <em>everything</em> due to changes in headers. But it is not unlikely that you had built
the branch before already. If so, and ccache was involved in the previous build, it may still
have all the info required for re-compilation in its cache.</p>

<p>And everyone will be happy: <code>make</code> will run its full rebuild, but most operations won&rsquo;t be handed
to the compiler because ccache is sitting in between, serving results from its cache.
And you as a developer won&rsquo;t lose that much time.</p>

<h2>Some figures</h2>

<p>Following are some figures demonstrating its potential when running a <code>make</code> in the devel branch
after having returned from a different branch with significant changes.</p>

<h3>With ccache, but cache empty</h3>

<p><code>plain time make
real  12m43.501s
user  11m52.550s
sys 0m44.110s
</code></p>

<h3>With ccache, everything in cache</h3>

<p><code>plain time make
real  0m55.572s
user  0m26.346s
sys 0m7.551s
</code></p>

<p>That&rsquo;s a <strong>build time reduction of more than 90 %</strong>!</p>

<p>This is already the optimal result, as everything was already present in the cache.
However, the situation was not unrealistic. I often switch into another branch, try something
out or commit a small change, and the return to the original branch. I already started having
many separate directories for the different branches to avoid frequent recompilation.
ccache can be relief here.</p>

<p>By the way, timing results are from my laptop. I did not bother to run <code>make</code> with parallel
jobs as this has limited effect on my laptop, though on more decent hardware it may be beneficial
both with and without ccache, though I guess, with many parallel jobs and a full cache, linking
will become the most expensive part.</p>

<h2>How to use ccache</h2>

<p>For Ubuntu, ccache is available in package <code>ccache</code>. You can easily install it with:
<code>bash Installing ccache on Ubuntu
sudo apt-get install ccache
</code></p>

<p>The most convenient way to use ccache in your build is to change your <code>CC</code> and <code>CXX</code>
environment variables as follows:
<code>bash setting compilers environment variables
export CC="ccache gcc"
export CXX="ccache g++"
</code></p>

<p>I suggest putting that into <code>.bashrc</code> so the variables will be set in every session and not
just once. After that, running <code>configure</code> will write a <code>Makefile</code> that will use ccache for
building object files.</p>

<p>Note: that will change these environment variables globally, so ccache may be used for other
projects, too.</p>

<h3>What ccache cannot do</h3>

<p>I already forgot about ccache because when working in a single branch it does not provide that
many benefits. When making changes to your code, you can be pretty sure the new code won&rsquo;t be
in the cache yet. Running <code>make</code> then will invoke ccache, but this will result in a cache miss.
It cannot help here, because the new code was never compiled before and thus in no cache.</p>

<p>Additionally, <code>make</code> is smart enough on its own to only re-build the parts of the program that
have actually changed or depend on the changes you made.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Core Dumps of Failed TravisCI Builds]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/30/getting-core-dumps-of-failed-travisci-builds/"/>
    <updated>2014-10-30T23:05:48+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/30/getting-core-dumps-of-failed-travisci-builds</id>
    <content type="html"><![CDATA[<p>I recently wrote about <a href="/blog/2014/10/17/using-travis-ci-for-a-c-plus-plus-11-project/">using TravisCI for continuously testing C++11 projects</a>.</p>

<p><strong>Now, what if a build fails?</strong></p>

<p>Especially for C and C++ projects, build failures may mean crashed
programs. In a local setup, the usual way to analyze program crashes
is to manually inspect the core files that are written on crash.</p>

<p>With TravisCI, there is no way to log in to a build machine and
inspect a core dump interactively. There is no SSH access to
the build machines. TravisCI does not even persist any state of
builds but the result and the log output.</p>

<p>There is a way to get to the core dumps, but it was fiddly to find
out and set up.</p>

<!-- more -->


<p>The basic idea is to run <code>gdb</code> on the TravisCI build machine
automatically when a build fails. <code>gdb</code> can be scripted, so all
we need to do is to make it print a backtrace in all threads at
the time of the crash.</p>

<p>By default, no core dumps will be produced on TravisCI. To turn them
on, an appropriate ulimit value must be set. We also need to install
<code>gdb</code> so we can actually run it. Here is the <code>.travis.yml</code> adjustment
for these prerequisites:</p>

<p>```yaml adjustments for install and before_script hooks
install:
&ndash; sudo apt-get install -y gdb  # install gdb</p>

<p>before_script:
&ndash; ulimit -c unlimited -S       # enable core dumps
```</p>

<p>To get an idea of where the program crashed, we can finally install
an <code>after_failure</code> hook. This hook can check for a core file and use
<code>gdb</code> to print a nice backtrace.</p>

<p>The core file pattern on TravisCI seems to be <code>core-%p</code>, so core
filenames will include the executable&rsquo;s process id and change on
every run. We can use <code>find</code> to look for files named <code>core*</code> in the
cwd and pick the first one as there should only be at most one core
file per build:</p>

<p><code>yaml adjustments for after_failure hook
after_failure:
- COREFILE=$(find . -maxdepth 1 -name "core*" | head -n 1) # find core file
- if [[ -f "$COREFILE" ]]; then gdb -c "$COREFILE" example -ex "thread apply all bt" -ex "set pagination 0" -batch; fi
</code></p>

<p>A failed build might produce output like this:</p>

<p><img src="/downloads/screenshots/travis-ci-gdb.png"></p>

<p>I recommend compiling the executable to test with debug symbols on and
with all optimizations turned off (i.e. compiler options <code>-g -O0</code>).
Otherwise backtraces might reveal less useful information for debugging.</p>

<p>On a side note: the <a href="http://lint.travis-ci.org/">Travis WebLint</a> is a
handy tool for validating <code>.travis.yml</code> files <em>before</em> pushing them.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What I Most Like About C++11]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/23/what-i-most-like-about-c-plus-plus-11/"/>
    <updated>2014-10-23T23:02:12+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/23/what-i-most-like-about-c-plus-plus-11</id>
    <content type="html"><![CDATA[<p>About half a year ago we started compiling our code with <code>-std=c++11</code>.</p>

<p>We had to fix a few, but not too many code parts for this. That was
the easy part.</p>

<p>Getting C++11 to work on all supported platforms, build and testing
environments was a bit more challenging, but we finally managed to do it.</p>

<p>Having used C++11 for some time now, I think it&rsquo;s time to share a few
of improvements in C++11 that solve common problems.</p>

<!-- more -->


<p>First of all, I don&rsquo;t regret we changed to it. In my opinion,
<strong>C++11 makes coding easier and safer.</strong> I will try to demonstrate that
with a few examples in a second.</p>

<p>Before I go into details, just let me state that I will only show a few
of my personal favorites here. There are so many more improvements in C++11
that are all worth having a look. If you haven&rsquo;t looked into C++11 much,
I recommend getting started at the <a href="http://en.wikipedia.org/wiki/C%2B%2B11">Wikipedia page about C++11</a>.</p>

<h2>Auto</h2>

<p>From a developer perspective, one of the most compelling features of
C++11 is the revamped <code>auto</code> keyword. Consider the following C++98/C++03 code:</p>

<p><code>c++ C++03 version
std::map&lt;std::string, std::string&gt;::iterator it = resultHeaders.find(value);
if (it != resultHeaders.end()) {
  // do something with value
}
</code></p>

<p>In C++11 this code can be simplified to:
<code>c++ C++11 version with auto
auto it = resultHeaders.find(value);
if (it != resultHeaders.end()) {
  // do something with value
}
</code>
In the C++11 version of the code, the compiler can figure out the type of
variable <code>it</code> all by itself. This allows writing less (i.e. better) code.</p>

<p>The more complex the types are, the more helpful this gets. Compare the
following two lines and check for yourself which one you prefer:</p>

<p><code>c++ C++03 version
std::map&lt;CollectionID, std::shared_ptr&lt;std::vector&lt;std::string&gt; &gt; &gt;::iterator it = shards.find(collectionID);
</code></p>

<p><code>c++ C++11 version with auto
auto it = shards.find(collectionID);
</code></p>

<p><code>auto</code> provides an <em>extra benefit</em>:
when using <code>auto</code> it is not necessary to repeat the type information
throughout the code. This is helpful when types need to be changed and the
change needs to be reflected everywhere. With <code>auto</code>, chances are that less
code needs to be adjusted. And it is not necessary to set up extra
typedefs for this.</p>

<p>If you think <code>auto</code> obfuscates the meaning too much, you can be a bit more
expressive, e.g.</p>

<p><code>c++ C++11 version with auto, const reference
auto const&amp; it = resultHeaders.find(value);
if (it != resultHeaders.end()) {
  // do something with value
}
</code></p>

<h2>Range-based loops</h2>

<p>We all have written a lot of code that iterates over a ranges, like this:</p>

<p><code>c++ C++03: iterating over a range
std::map&lt;string, AgencyCommResultEntry&gt;::iterator it;
for (it = result.values.begin(); it != result.values.end(); ++it) {
  // do something with it
}
</code></p>

<p>C++11 provides a special range-based syntax for <code>for</code> loops, which makes
this a lot easier and compact:
<code>c++ C++11: iterating over a range
for (auto it : result.values) {
  // do something with it
}
</code></p>

<h2>Decltype</h2>

<p>As we have seen, the compiler can deduce the type of expressions
automatically. C++11 also allows using this type information with the
<code>decltype</code> keyword. This allows to write more generic and maintainable code.</p>

<p>In the following code, the type of variable <code>document</code> is a pointer to a
<code>Document</code>. Variable <code>myDocument</code> has the same type:</p>

<p><code>c++ C++03 explicit type specification
Document* document = res-&gt;getDocumentCollection(registerId);
Document* myDocument = document;
</code></p>

<p>In C++03, we couldn&rsquo;t tell the compiler that the two variables should
always have the same types. In C++11, we can explicitly give <code>myDocument</code>
the same type as <code>document</code>, without any typedefs:</p>

<p><code>c++ C++11 automatic type deduction
</code>c++ C++11 automatic type deduction
auto* document = res->getDocumentCollection(registerId);
decltype(document) myDocument = document;  // myDocument has same type as document
```</p>

<p><code>decltype</code> can also be used to deduce the type of expressions.</p>

<h2>Lambdas / Closures</h2>

<p>Lambdas are available in most other mainstream languages today, and they
are available in C++11, too.</p>

<p>Probably one of the most common use cases for a lambda is a custom
comparator function for sorting:</p>

<p>```c++ custom comparator function using a lambda
std::sort(operations.begin(),</p>

<pre><code>      operations.end(), 
      [] (Operation const* left, Operation const* right) {
</code></pre>

<p>  return (left->id &lt; right->id);
});
```</p>

<p>In the above example, the lambda has two input parameters and produces a
boolean result. Note that the type of the result was not explicitly specified.
Again the compiler is able to figure it out automatically.</p>

<p>A lambda can be assigned to a variable, and it can be passed as a parameter
to another function/method. Lambdas can optionally have access to the
variables of the scope they were created in.</p>

<p>The following code defines a struct <code>ScopeGuard</code> that executes a lambda
in its constructor and another lambda in its destructor:</p>

<p>```c++ lambdas as function parameters
// define ScopeGuard struct
struct ScopeGuard {
  ScopeGuard (std::function&lt;void()> onEnter,</p>

<pre><code>          std::function&lt;void()&gt; onExit) 
: onExit(onExit) {
  onEnter();
</code></pre>

<p>  }</p>

<p>  ~ScopeGuard () {</p>

<pre><code>onExit();
</code></pre>

<p>  }</p>

<p>  std::function&lt;void()> onExit;
};</p>

<p>// lambda to be executed in constructor
auto onEnter = <a href="">&amp;engine</a> &ndash;> void {
  engine->getQuery()&ndash;>enterContext();
};</p>

<p>// lambda to be executed in destructor
auto onExit = <a href="">&amp;</a> &ndash;> void {
  for (auto expression : allVariableBoundExpressions) {</p>

<pre><code>expression-&gt;invalidate();
</code></pre>

<p>  }
  engine->getQuery()&ndash;>exitContext();
}</p>

<p>// create guard object with the lambdas
// this will instantly execute <code>onEnter</code>
ScopeGuard guard(onEnter, onExit);</p>

<p>// do something&hellip;</p>

<p>// when scope is left, <code>onExit</code> will be executed
```</p>

<p>As mentioned before, <code>decltype</code> can be used to determine the
return type of a function automatically. Here&rsquo;s an example:</p>

<p><code>c++ function with automatic return type deduction
auto add = [](int a, int b) -&gt; decltype(a + b) {
  return a + b;
};
</code></p>

<p>As can be seen in the examples above, C++11 has introduced an
alternative function declaration syntax, with the type of the
function result following a <code>-&gt;</code>. The return type can be omitted
if it can be unambiguously determined by the compiler. The new
function declaration syntax is mainly useful for lambdas, but
it can be used for regular functions, too.</p>

<h2>Enum class</h2>

<p>Enums in C++ are useful but just <em>don&rsquo;t feel right</em>: persisting a
struct that contains an enum value is not portable as the
underlying data type for the enum is implementation-dependent.</p>

<p>Additionally, enum values can be compared to almost any other values,
which in most cases doesn&rsquo;t make sense but obfuscates coding errors.
There were also <em>scoping issues</em> with enum values.</p>

<p>C++11 enum classes fix these problems. First of all, the underlying
data type for an enum can be specified. For example, this creates
an enum based with its value stored in an <code>std::uint8_t</code>:</p>

<p>```c++ enum class with specified data type
enum class StatusType : std::uint8_t {
  UNINITIALIZED = 0,
  STARTING,
  RUNNING,
  STOPPING,
  STOPPED
};</p>

<p>StatusType status;
```</p>

<p>Regarding the comparison of enum values to other values, C++11
enum classes are much stronger typed than regular enums.
Comparing the <code>status</code> variable from the above example to anything
but a value from its enum class won&rsquo;t even compile.</p>

<p>This provides much greater type safety than when using the old,
implicitly converting enums:</p>

<p><code>c++ invalid usage of enums
if (status == 0) {  // won't compile in C++11
  // ...
}
if (status == StatusType::UNINITIALIZED) {  // this would work
  // ...
}
</code></p>

<p>Finally, enum classes fix the scoping problems of regular enums.
In C++03, the following code did not complile because two enums
contained the same member name:</p>

<p>```c++ two enums with same member name
enum DirectionType {
  LEFT,
  RIGHT
};</p>

<p>enum AnswerType {
  RIGHT,  // won&rsquo;t compile in C++03 and C++11
  WRONG
};
```</p>

<p>With C++11 enum classes, the following code is all fine:
```c++ two enums class with same member name
enum class DirectionType {
  LEFT,
  RIGHT
};</p>

<p>enum class AnswerType {
  RIGHT,  // works!
  WRONG
};
```</p>

<h2>Additional containers</h2>

<p>C++11 provides the hash-based containers <code>std::unordered_map</code>
and <code>std::unordered_set</code> (plus their non-unique counterparts).
These containers are not sorted, so they can be more efficient
than <code>std::map</code> and <code>std::set</code>.</p>

<p>Turning an <code>std::map</code> into an <code>std::unordered_map</code> is simple as
the APIs are more or less identical.</p>

<p>There is now also a singly-linked list container, named
<code>std::forward_list</code>. This obviously allows forward iteration
only, but is more space efficient than the already existing
doubly-linked list container.</p>

<h2>More</h2>

<p>Other improvements include move semantics, atomic variables and
operations, a dedicated type for NULL pointers, STL support for
threads and mutexes, regular expressions, more Unicode support,
override, final &ndash; to name only a few&hellip;</p>
]]></content>
  </entry>
  
</feed>
