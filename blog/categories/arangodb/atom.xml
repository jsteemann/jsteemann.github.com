<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2014-12-20T19:11:15+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using ES6 Features in ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2014/12/19/using-es6-features-in-arangodb/"/>
    <updated>2014-12-19T19:12:06+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/12/19/using-es6-features-in-arangodb</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.4 will be shipped with an <em>updated version of V8</em>.</p>

<p>The V8 version included in 2.4 will be 3.29.59. This version will replace the
almost two year old 3.16.14. A lot of things happened in V8 since then, and a lot
of ES6 features have been added to it.</p>

<p><strong>ES6 is not finalized yet</strong>, and support for it is a <a href="http://kangax.github.io/compat-table/es6/">work in progress on all
platforms</a>.</p>

<p>ES6 provides many cool features that can make JavaScript developer&rsquo;s life easier.
In this blog post, I&rsquo;ll summarize a few ES6 features that are available in
ArangoDB, either for scripting purposes in the ArangoShell, or in your server-side
Foxx actions inside the database.</p>

<p>I don&rsquo;t want to put you off until Doomsday. ArangoDB 2.4 should be released next
week. Time to play with some ES6 features!</p>

<!-- more -->


<h1>Activating ES6 features</h1>

<p>Work on ES6, also dubbed <em>Harmony</em> or <em>ES.next</em>, is still in progress. At the time
of this writing, the ES6 specification was still in draft status.</p>

<p>Therefore no platform has implemented all ES6 features yet. And because ES6 is
still a moving target, the already implemented features should still be considered
experimental.</p>

<p>This is true for all environments that implement ES6 features. For example, Firefox
and other browsers contain lots of experimental ES6 features already, providing a
notice that these might change in future versions.</p>

<p>V8 is no exception here. It has turned most ES6 features off by default, but it
provides several command-line options to turn them on explicitly.</p>

<p>The V8 version used for ArangoDB 2.4 provides the following ES6-related switches:</p>

<ul>
<li><code>--harmony_scoping</code> (enable harmony block scoping)</li>
<li><code>--harmony_modules</code> (enable harmony modules (implies block scoping))</li>
<li><code>--harmony_proxies</code> (enable harmony proxies)</li>
<li><code>--harmony_generators</code> (enable harmony generators)</li>
<li><code>--harmony_numeric_literals</code> (enable harmony numeric literals (0o77, 0b11))</li>
<li><code>--harmony_strings</code> (enable harmony string)</li>
<li><code>--harmony_arrays</code> (enable harmony arrays)</li>
<li><code>--harmony_arrow_functions</code> (enable harmony arrow functions)</li>
<li><code>--harmony_classes</code> (enable harmony classes)</li>
<li><code>--harmony_object_literals</code> (enable harmony object literal extensions)</li>
<li><code>--harmony</code> (enable all harmony features (except proxies))</li>
</ul>


<p>These switches are all off by default. To turn on features for either arangod
or arangosh, start it with the V8 option(s) wrapped into the ArangoDB option
<code>--javascript.v8-options</code>, e.g.:</p>

<pre><code>arangosh --javascript.v8-options="--harmony_proxies --harmony_generators --harmony_array"
</code></pre>

<p>On a side note: node.js is also using V8. Turning on ES6 features in node.js almost
works the same way. Just omit the surrounding <code>--javascript.v8-options="..."</code>:</p>

<pre><code>node --harmony_proxies --harmony_generators --harmony_array
</code></pre>

<p>Note that the V8 options can only be set for the entire process (i.e. arangosh, arangod
or node.js), and not just for a specific script or application. In reality this shouldn&rsquo;t be
too problematic as the vast majority of ES6 features is downwards-compatible to ES5.1.</p>

<h1>ES6 features by example</h1>

<p>Following I have listed a few select ES6 features that are usable in ArangoDB 2.4,
in no particular order. I have omitted a few ES6 features that aren&rsquo;t supported in bundled
V8 version, and also omitted <em>classes</em> and <em>modules</em> due to lack of time.</p>

<h2>Arrow functions</h2>

<p>ES6 provides an optional arrow function syntax. The arrow function syntax is a shorthand for
writing a full-blown function declaration. Here&rsquo;s an example:</p>

<p>```js a simple arrow function
/<em> defines function pow </em>/
var pow = (value => value * value);</p>

<p>/<em> calls pow </em>/
pow(15);  /<em> 225 </em>/
```</p>

<p>Arrow functions can also take multiple parameters. The syntax then becomes:
```js arrow function with multiple parameters
/<em> defines function sum </em>/
var sum = (a, b) => a + b;</p>

<p>/<em> calls sum </em>/
sum(3, 7);  /<em> 10 </em>/
```</p>

<p>So far we have only seen arrow functions with simple expressions, but arrow function bodies
can also be more complex and can contain multiple statements:
```js more complex arrow functions
var translations = {
  &ldquo;en&rdquo; : &ldquo;English&rdquo;,
  &ldquo;fr&rdquo; : &ldquo;French&rdquo;,
  &ldquo;de&rdquo; : &ldquo;German&rdquo;
};</p>

<p>/<em> using multiple statements </em>/
[&ldquo;en&rdquo;, &ldquo;fr&rdquo;, &ldquo;xx&rdquo;].map(value => {
  if (translations.hasOwnProperty(value)) {</p>

<pre><code>return translations[value];
</code></pre>

<p>  }
  return &ldquo;unknown!&rdquo;;
});
```</p>

<p>Arrow functions are turned off by default. To enable them in arangod or arangosh, start them
with the option <code>--javascript.v8-options="--harmony_arrow_functions"</code>.</p>

<h2>Maps and sets</h2>

<p>ES6 maps provide a specialized alternative to regular objects in case a <em>lookup-by-key</em>
functionality is required.</p>

<p>When no maps are available, storing keys mapped to objects is normally done using a plain
object. With ES6, the same use case can be handled with using a <strong>Map</strong> object:
```js using an ES6 Map object
var m = new Map();</p>

<p>/<em> set 5M keys </em>/
for (var i = 0; i &lt; 5000000; ++i) {
  m.set(&ldquo;test&rdquo; + i, i);
}
```</p>

<p>ES6 maps can be more efficient than plain objects in some cases. For the above case of
storing 5M entries, an ES6 map is about twice as fast as a plain object on my laptop.
Though there might be cases we are plain objects are still faster.</p>

<p>There&rsquo;s more to ES6 Maps than just efficiency:</p>

<ul>
<li>ES6 maps provide a member <code>size</code> which keeps track of the number of objects in the map.
This is hard to achieve with a plain object.</li>
<li>Objects can only have string keys, whereas map keys can have different key types.</li>
<li>They don&rsquo;t inherit keys from the prototype, so there is no <code>hasOwnProperty</code> hassle with Maps.</li>
</ul>


<p>ES6 also comes with a specialized <strong>Set</strong> object. The Set object is a good alternative to
plain JavaScript objects when the use case is to track unique values. Using a Set is more
intuitive, potentially more efficient and may require even less memory than when implementing
the same functionality with a plain JavaScript object:</p>

<p>```js using an ES6 Set object
var s = new Set();</p>

<p>/<em> set 5M values </em>/
for (var i = 0; i &lt; 5000000; ++i) {
  s.add(&ldquo;test&rdquo; + i);
}
```</p>

<p>Maps and sets are enabled by default in arangod and arangosh. No special configuration is needed
to use them in your code.</p>

<h2>Proxy objects</h2>

<p>Proxy objects can be used to intercept object property accesses at runtime. This can be used
for meta-programming in many real-world situations, e.g.:</p>

<ul>
<li>preventing, auditing and logging property accesses</li>
<li>calculated / derived properties</li>
<li>adding a compatibility layer on top of an object</li>
</ul>


<p>Here&rsquo;s an example that logs property accesses on the proxied object:
```js using a Proxy object
var proxy = Proxy.create({
  get: function (obj, name) {</p>

<pre><code>console.log("read-accessing property '%s'", name);
</code></pre>

<p>  },
  set: function (obj, name) {</p>

<pre><code>console.log("write-accessing property '%s'", name);
</code></pre>

<p>  }
});</p>

<p>proxy.foo = &ldquo;bar&rdquo;;   /<em> write-accessing property &lsquo;foo&rsquo; </em>/
proxy.foo;           /<em> read-accessing property &lsquo;foo&rsquo; </em>/
```</p>

<p>Proxy objects are not available by default. To enable them in arangod or arangosh, start them
with the option <code>--javascript.v8-options="--harmony_proxies"</code>.</p>

<h2>Iterators and generators</h2>

<p>ES6 provides generators and iterators. They can be used individually or in combination.</p>

<p>Let&rsquo;s start with a simple example of a generator that will generate only two values:
```js a simple generator that generates two values
function* generate () {
  yield 23;
  yield 42;
}</p>

<p>var generator = generate();
console.log(generator.next());  /<em> { &ldquo;value&rdquo; : 23, &ldquo;done&rdquo; : false } </em>/
console.log(generator.next());  /<em> { &ldquo;value&rdquo; : 42, &ldquo;done&rdquo; : false } </em>/
console.log(generator.next());  /<em> { &ldquo;value&rdquo; : undefined, &ldquo;done&rdquo; : true } </em>/
<code>``
As can be seen above, the value yielded by the generator function will be returned
wrapped into an object with a</code>value<code>and a</code>done` attribute automatically.</p>

<p>The general pattern to consume all values from a generator function is to call
its <code>next()</code> method until its <code>done</code> value is <code>true</code>:
```js consuming all values from a generator function
var generator = generate();</p>

<p>while (true) {
  value = generator.next();
  if (value.done) {</p>

<pre><code>break;
</code></pre>

<p>  }
  console.log(value.value);
}
```</p>

<p>An alternative to that is to use an iterator (note the new <code>of</code> operator):
```js consuming all values from a generator function
var generator = generate();</p>

<p>for (var value of generator) {
  console.log(value);
}
```</p>

<p>Generator functions produce their values lazily. Therefore it is possible
and not inefficent to write generators that produce never-ending sequences.
Though one must be careful to abort iterating over the generator values
at some point if the sequence does not terminate:
```js a generator producing an endless sequence
function* generate () {
  var i = 0;
  while (true) {</p>

<pre><code>yield ++i;
</code></pre>

<p>  }
}</p>

<p>var generator = generate();</p>

<p>/<em> note: this will not terminate </em>/
for (var value of generator) {
  console.log(value);
}
```</p>

<p>We have now seen two uses of iterators as part of the previous examples.
As demoed, generator function values can be iterated with the <code>of</code> operator
without any further ado. Apart from generators, a few other built-in types
also provide ready-to-use iterators. The most prominent are <code>String</code> and <code>Array</code>:
```js iterating over the characters of a string
var text = &ldquo;this is a test string&rdquo;;</p>

<p>for (var value of text) {
  console.log(value);
}
```
The above example will iterate all the individual characters of the string.</p>

<p>The following example iterates the values of an Array:
```js iterating over the values of an array
var values = [ &ldquo;this&rdquo;, &ldquo;is&rdquo;, &ldquo;a&rdquo;, &ldquo;test&rdquo; ];</p>

<p>for (var value of values) {
  console.log(value);
}
<code>``
This will produce</code>&ldquo;this&rdquo;<code>,</code>&ldquo;is&rdquo;<code>,</code>&ldquo;a&rdquo;<code>,</code>&ldquo;test&rdquo;<code>. This is normally what is
desired when iterating over the values of an Array. Compare this to the</code>in<code>
operator which would produce</code>0<code>,</code>1<code>,</code>2<code>and</code>3` instead.</p>

<p>Map and Set objects also implement iterators:
```js iterating over the contents of a Map
var m = new Map();</p>

<p>m.set(&ldquo;Sweden&rdquo;, &ldquo;Europe&rdquo;);
m.set(&ldquo;China&rdquo;, &ldquo;Asia&rdquo;);
m.set(&ldquo;Bolivia&rdquo;, &ldquo;South America&rdquo;);
m.set(&ldquo;Australia&rdquo;, &ldquo;Australia&rdquo;);
m.set(&ldquo;South Africa&rdquo;, &ldquo;Africa&rdquo;);</p>

<p>for (var value of m) {
  console.log(value);
}
```</p>

<p>Note that Maps also provide dedicated iterators for just their keys or
their values:
```js iterating over keys and values of a Map
for (var country of m.keys()) {
  console.log(country);
}</p>

<p>for (var continent of m.values()) {
  console.log(continent);
}
```</p>

<p>Rolling an iterator for your own object is also possible by implementing the method
<code>Symbol.iterator</code> for it:
```js creating an iterator for an object
function Sentence (text) {
  this.text = text;
}</p>

<p>/<em> create the actual iterator method </em>/
/<em> note that the iterator is a generator function here </em>/
Sentence.prototype[Symbol.iterator] = function*() {
  var regex = /\S+/g;
  var text = this.text;
  var match;
  while (match = regex.exec(text)) {</p>

<pre><code>yield match[0]; 
</code></pre>

<p>  }
};</p>

<p>var sentence = new Sentence(&ldquo;The quick brown fox jumped over the lazy dog&rdquo;);
/<em> invoke the iterator </em>/
for (var word of sentence) {
  console.log(word);
}
```</p>

<p>Generators and iterators are not available by default. To enable them in arangod
or arangosh, start them with the option <code>--javascript.v8-options="--harmony_generators"</code>.</p>

<h2>String enhancements</h2>

<p>ES6 provides the following convenience string functions:</p>

<ul>
<li>string.startsWith(what)</li>
<li>string.endsWith(what)</li>
<li>string.contains(what)</li>
<li>string.repeat(count)</li>
<li>string.normalize(method)</li>
<li>string.codePointAt(position)</li>
<li>String.fromCodePoint(codePoint)</li>
</ul>


<p>These functions are mostly self-explaining, so I won&rsquo;t explain them in more
detail here. Apart from that, these functions are turned off by default.
To enable them in arangod or arangosh, start them with the option
<code>--javascript.v8-options="--harmony_strings"</code>.</p>

<h2>Array enhancements</h2>

<p>ES6 provides the following enhancements for the <code>Array</code> object:</p>

<ul>
<li>array.find(function)</li>
<li>array.findIndex(function)</li>
<li>array.keys()</li>
<li>array.values()</li>
<li>Array.observe(what, function)</li>
</ul>


<p>Here are a few examples demoing these functions:
```js Array enhancements
var values = [ 1, 2, 9, 23, 42 ];</p>

<p>/<em> returns the first Array element for which the function returns true </em>/
values.find(function (value) {
  return value === 23;
});</p>

<p>/<em> returns the first Array index for which the function returns true </em>/
values.findIndex(function (value) {
  return value === 23;
});</p>

<p>/<em> iterate over the keys of the Array </em>/
for (var key of values.keys()) {
  console.log(key);
}</p>

<p>/<em> iterate over the values of the Array </em>/
for (var key of values.values()) {
  console.log(key);
}</p>

<p>/<em> observe all changes to an Array </em>/
Array.observe(values, function (changes) {
  console.log(changes);
});</p>

<p>/<em> trigger a change to the observed Array </em>/
values.push(117);
```</p>

<p>The Array enhancements are turned off by default. To enable them in arangod or
arangosh, start them with the option <code>--javascript.v8-options="--harmony_arrays"</code>.</p>

<h2>Number enhancements</h2>

<p>The <code>Number</code> object is extended with the following ES6 functions:</p>

<ul>
<li>Number.isInteger(value)</li>
<li>Number.isSafeInteger(value)</li>
</ul>


<p>There are also <code>Number.MIN_SAFE_INTEGER</code> and <code>Number.MAX_SAFE_INTEGER</code> so applications
can programmatically check whether a numeric value can still be stored in the range of
-2<sup>53</sup> to +2<sup>53</sup> without potential precision loss.</p>

<h2>Constants</h2>

<p>The <code>const</code> keyword can be used to define a read-only constant. The constant must be
initialized and a variable with the same name should not be redeclared in the same scope.</p>

<p>Here is an example of using <code>const</code>:
<code>js using const to create a read-only variable
function calculate (value) {
  const magicPrime = 23;
  return magicPrime ^ value;
}
</code></p>

<p>In non-strict mode, <code>const</code> variables behave non-intuitively. Re-assigning a value
to a variable declared <code>const</code> does not throw an exception, but the assignment will
not be carried out either. Instead, the assignment will silently fail and the <code>const</code>
variable will keep its original value:</p>

<p><code>js re-assigning a value to a const variable
function mystery () {
  const life = 42;
  life = 23;        /* does not change value and does not throw! */
  return life;      /* will return 42 */
}
</code></p>

<p><code>js re-assigning a value to a const variable, using strict mode
function mystery () {
  "use strict";
  const life = 42;
  life = 23;        /* will throw SyntaxError "assignment to constant variable" */
}
</code></p>

<p>The <code>const</code> keyword is disabled by default. To enable it in arangod or arangosh,
start them with the option <code>--javascript.v8-options="--harmony_scoping"</code>.</p>

<h2>Enhanced object literals</h2>

<p>ES6 provides a shorthand for defining methods in object literals.</p>

<p>The following example creates a normal method named <code>save</code> in <code>myObject</code>:</p>

<p>```js shorthand method declaration
var myObject = {
  type: &ldquo;myType&rdquo;,
  save () {</p>

<pre><code>console.log("save");
</code></pre>

<p>  }
};
```</p>

<p>Interestingly, the object literals seem to work for method declarations only.
I did not get them to work for non-method object properties, though ES6 allows that.
It seems that this is not implemented in V8 yet.</p>

<p>Enhanced object literals are turned off by default. To enable them in arangod
or arangosh, start them with the option <code>--javascript.v8-options="--harmony_object_literals"</code>.</p>

<h2>Enhanced numeric literals</h2>

<p>For the ones that love working with binary- or octal-encoded numbers, ES6 has
support for this too:
<code>js numeric literals
var life = 0b101010;          /* binary, 42 in decimal */
var filePermissions = 0o777;  /* octal, 511 in decimal */
</code></p>

<p>Enhanced numeric literals are turned off by default. To enable them in arangod
or arangosh, start them with the option <code>--javascript.v8-options="--harmony_numeric_literals"</code>.</p>

<h2>Symbols</h2>

<p>ES6 also provides a Symbol type. Symbols are created using the global <code>Symbol()</code> function.
Each time this function is called, a new Symbol object will be created.
A Symbol can be given an optional name, but this name cannot be used to identify the
Symbol later. However, Symbols can be compared by identity.</p>

<p>What one normally wants is to use the same Symbol from different program parts. In this
case, a Symbol should not be created with the <code>Symbol()</code> function, but with <code>Symbol.for()</code>.
This will register the Symbol in a global symbol table if it is not there yet, and return
the Symbol if already created:</p>

<p>```js using named Symbols
var typeAttribute = Symbol.for(&ldquo;type&rdquo;);
var carType = Symbol.for(&ldquo;car&rdquo;);
var trainType = Symbol.for(&ldquo;train&rdquo;);</p>

<p>var object1 = { };
object1[typeAttribute] = carType;</p>

<p>var object2 = { };
object2[typeAttribute] = trainType;</p>

<p>/<em> check if the objects have the same type </em>/
object1[typeAttribute] === object2[typeAttribute];  /<em> false </em>/
object1[typeAttribute] === carType;                 /<em> true </em>/
object2[typeAttribute] === carType;                 /<em> false </em>/
object2[typeAttribute] === trainType;               /<em> true </em>/
```</p>

<p>Symbol object properties are not enumerated by default, so they can be used to implement
&ldquo;hidden&rdquo; or internal properties.</p>

<p>Symbols can be used by default in arangod and arangosh. No special configuration is required.</p>

<h2>TypedArrays</h2>

<p>TypedArrays are Arrays whose members all have the same type and size. They are more
specialized (read: limited but efficient) alternatives to the all-purpose <code>Array</code> type.</p>

<p>TypedArrays look and feel a bit like C arrays, and they are often used as an Array-like
view into binary data (for which JavaScript has no native support).</p>

<p>A TypedArray is created (and all of its memory is allocated) by invoking the appropriate
TypedArray constructor:</p>

<ul>
<li>Int8Array</li>
<li>Uint8Array</li>
<li>Uint8ClampedArray</li>
<li>Int16Array</li>
<li>Uint16Array</li>
<li>Int32Array</li>
<li>Uint32Array</li>
<li>Float32Array</li>
<li>Float64Array</li>
</ul>


<p>```js using an Array of unsigned 8 bit integers
var data = new Uint8Array(2);
data[0] = 0b00010101;  /<em> 23 </em>/
data[1] = 0b00101010;  /<em> 42 </em>/</p>

<p>console.log(data[0]);  /<em> 23 </em>/
console.log(data.length * data.BYTES_PER_ELEMENT); /<em> 2 bytes </em>/
```</p>

<p>```js using an Array of 64 bit floating point values
var data = new Float64Array(2);
data[0] = 23.23;</p>

<p>console.log(data[0]);  /<em> 23.23 </em>/
console.log(data[1]);  /<em> 0.0 </em>/
console.log(data.length * data.BYTES_PER_ELEMENT); /<em> 16 bytes </em>/
```</p>

<p>TypedArrays can be used in arangod and arangosh by default. No special
configuration is required to activate them.</p>

<h2>Unsupported ES6 features</h2>

<p>As mentioned before, V8 does not yet support every proposed ES6 feature.
For example, the following ES6 features are currently missing:</p>

<ul>
<li>template strings</li>
<li>function default parameters</li>
<li>rest function parameter</li>
<li>spread operator</li>
<li>destructuring</li>
<li>array comprehension</li>
<li><code>let</code></li>
</ul>


<p>I strongly hope these features will make it into the final version of ES6 and will be
implemented by the V8 team in future versions of V8.</p>

<p>Apart from that, a lot of nice ES6 features are there already and can be used
in ArangoDB applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Improvements for 2.4]]></title>
    <link href="http://jsteemann.github.io/blog/2014/12/12/aql-improvements-for-24/"/>
    <updated>2014-12-12T23:35:08+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/12/12/aql-improvements-for-24</id>
    <content type="html"><![CDATA[<p>While on a retreat in Belgium, we found some spare time to
work on improvements for AQL. These will be shipped with
ArangoDB version 2.4, and are already available in the devel
version for testing from now on.</p>

<p>Here&rsquo;s a short overview of the improvements:</p>

<!-- more -->


<h1>COLLECT WITH COUNT</h1>

<p>A common use case in query languages is to count the number of
documents returned by a query. The AQL solution for this has been
to use the <code>LENGTH</code> function and a subquery:</p>

<p>```
RETURN LENGTH((
  FOR doc IN collection</p>

<pre><code>FILTER doc.someAttribute == someValue
RETURN doc
</code></pre>

<p>  ))
```</p>

<p>This works but is probably unintuitive for people which have
used SQL for years.</p>

<p>We therefore now allow using the following alternative syntax,
using the new <code>COLLECT ... WITH COUNT INTO ...</code> clause:</p>

<p><code>
FOR doc IN collection
  FILTER doc.someAttribute == someValue
  COLLECT WITH COUNT INTO length
  RETURN length
</code></p>

<p>This query returns just the total number of matches, but not the
matches themselves. As this query is made for counting only, it
can be executed more efficiently than the original query.
In the query with the <code>COUNT INTO ...</code> clause, the documents found
by the filter condition will only be counted and then instantly
discarded. They will not be shipped around inside the query, from
the subquery to the top level into the <code>LENGTH()</code> function.
to shipped around inside the query.</p>

<p>As a bonus, there is no need to use a subquery anymore, though the
subquery variant is still fully supported and will be.</p>

<p><code>COLLECT ... WITH COUNT</code> also works for counting the number of items
per group:</p>

<p><code>
FOR doc IN collection
  COLLECT value = doc.someAttribute WITH COUNT INTO length
  RETURN { value: value, length : length }
</code></p>

<p>This returns the number of matches for each distinct <code>value</code>.</p>

<p>A quick unscientific benchmark reveals that the specialized
<code>WITH COUNT</code> clause seems to be faster than the old variant.
The following results show the differences for a collection with
500,000 small documents:</p>

<p>The old variant that counts the number of documents per age runs
in 4.75 seconds on my laptop:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age INTO g
  RETURN { age: age, length: LENGTH(g) }
</code></p>

<p>The new variant produces the same result, but runs in 0.6 seconds locally:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age WITH COUNT INTO length
  RETURN { age: age, length: length }
</code></p>

<p>A notable speedup can also be observed if only a fraction of the
groups is built (here: 1/8). The old variant for this runs in 0.6
seconds:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age INTO g
  RETURN { age: age, length: LENGTH(g) }
</code></p>

<p>The new variants runs in 0.12 seconds:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age WITH COUNT INTO length
  RETURN { age: age, length: length }
</code></p>

<p>The absolute times may vary greatly depending on the type of documents and
the hardware used, but in general the new variant should provide a
speedup.</p>

<h1>COLLECT with group expression</h1>

<p>Finally, <code>COLLECT ... INTO</code> has been extended to support just another variant
that can reduce the amount of copying inside a query.</p>

<p>Let&rsquo;s have a look at this example query:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age INTO g
  RETURN { age: age, maxDate: MAX(g[*].doc.dateRegistered) }
</code></p>

<p>In the above query, for each distinct <code>age</code> value, all documents are collected
into variable <code>g</code>. When the collecting phase is over, there will be an iteration
over all the collected documents again, to extract their <code>dateRegistered</code> value.
After that, the <code>dateRegistered</code> values will be passed into the <code>MAX()</code> function.</p>

<p>This query can be made more efficient now as follows:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age INTO g = doc.dateRegistered
  RETURN { age: age, maxDate: MAX(g) }
</code></p>

<p>The new thing about this variant is the expression following the <code>INTO</code>.
Having an expression there allows controlling what values are collected for
each group. Using a projection expression here can greatly reduce the
amount of copying afterwards, and thus make the query more efficient than if
it had to copy the entire documents.</p>

<h1>Removing filters covered by indexes</h1>

<p><code>FILTER</code> conditions which are completely covered by indexes will
now be removed from the execution plan if it is safe to do so.
Dropping the <code>FILTER</code> statements allows the optimizer to get rid
of not only the <em>FilterNode</em>, but also its corresponding <em>CalculationNode</em>.
This will save a lot of computation if the condition needs to be checked
for many documents.</p>

<p>For example, imagine the following query:</p>

<p><code>
FOR doc IN collection
  FILTER doc.value &lt; 10
  RETURN doc
</code></p>

<p>If there is a (skiplist) index on <code>doc.value</code>, the optimizer may
decide to use this index. It will replace the query&rsquo;s <em>EnumerateCollectionNode</em>
with an <em>IndexRangeNode</em> instead first. The <em>IndexRangeNode</em> will scan the index
on <code>doc.value</code> for the range [-inf, 10).</p>

<p>Following that, the optimizer rule <code>remove-filter-covered-by-index</code>
should fire and detect that the <code>FILTER</code> condition is already covered
by the <em>IndexRangeNode</em> alone. It can thus remove the <em>FilterNode</em>.
This also makes the <em>CalculationNode</em> of the <em>FilterNode</em> obsolete,
so these two nodes will be removed and computation is saved.</p>

<h1>Removing brackets for subquery function call parameters</h1>

<p>Since the beginning of AQL, the parser required the user the put
subqueries that were used as function parameters inside two pairs of
brackets.</p>

<p>For example, the following query did not parse in 2.3 and before:
<code>
RETURN LENGTH(FOR doc IN collection RETURN doc)
</code></p>

<p>Instead, it needed to be written as:
<code>
RETURN LENGTH((FOR doc IN collection RETURN doc))
</code></p>

<p>If you didn&rsquo;t notice the difference, the latter version of the query had
duplicate parentheses. The requirement to use duplicate parentheses has
caused several support questions over time, and this can be taken as a
proof that it was not intuitive.</p>

<p>The requirement for duplicate parentheses was an artifact required by the
AQL parser grammar in order to parse the query correctly.</p>

<p>For 2.4, the AQL grammar has been cleaned up in this respect.
Duplicate parentheses are still allowed and work fine in 2.4 but they are not
required anymore. This should make the first steps with AQL a bit easier
and more intuitive.</p>

<p>We&rsquo;re 1.5 days into our retreat now. Maybe there&rsquo;ll be some more
AQL-related improvements in the end. Let&rsquo;s see.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Tour Around the New AQL Query Optimizer]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer/"/>
    <updated>2014-11-07T22:30:10+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer</id>
    <content type="html"><![CDATA[<p>The major new feature in ArangoDB 2.3 is the shiny new
AQL query optimizer and executor. These parts of ArangoDB have been
rewritten in 2.3 to make AQL much better for our end users.</p>

<!-- more -->


<p>Since one of the initial releases, ArangoDB has been shipped with
AQL, the <em>ArangoDB Query Language</em>. AQL has since then been ArangoDB&rsquo;s
most versatile way of executing simple and also the not-so-simple
queries.</p>

<p>I&rsquo;ll start with an overview of query execution in previous versions
of ArangoDB, and then explain the new engine and explain the differences.</p>

<h2>History: query execution in pre-2.3</h2>

<p>Previous versions of ArangoDB executed any AQL query in the following
steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>collect filter conditions in AST and look for index usage opportunities</li>
<li>generate code</li>
<li>execute code</li>
</ul>


<p>This approach was simple and has worked for a lot of queries, but it also
had a few quirks:</p>

<p>First of all, most of the steps were carried out directly on the
abstract syntax tree, with the AST nodes being modified in place.
There was also just the one AST per query, so the old AQL executor
could not generate multiple, potentially very different execution
plan candidates for a given query.</p>

<p>The &ldquo;old&rdquo; optimizer was able to move AST nodes around during optimization
and it was already able to consider multiple index candidates for a query,
but it would not compare multiple plans and make a cost-based decision.
It was also limited in the amount and scope of transformations it
could safely apply to the AST.</p>

<p>When it came to code generation and execution, the &ldquo;old&rdquo; executor
fully relied on V8 to execute the queries. Result sets were created
using V8&rsquo;s value objects. Documents from collections that queries
needed to iterate over had to be made available to V8. While some
optimization was used for this, the conversions could have summed up
to significant overhead for certain kinds of queries.</p>

<p>The representation of queries via an AST also made it hard to generate
code that supported lazy evaluation during query execution.</p>

<p>Finally, the AQL optimizer so far did not provide much support for
queries that were to be executed in a distributed fashion inside a cluster
of servers.</p>

<h2>Query execution in ArangoDB 2.3</h2>

<p>We wanted to address all these issues with a rewrite of the AQL
infrastructure. Starting with ArangoDB 2.3, AQL queries are executed
in these steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>transform AST into execution plan</li>
<li>optimize and permute execution plans</li>
<li>estimate costs for execution plans and pick optimal plan</li>
<li>instanciate execution engine from optimal plan</li>
<li>(in cluster only) send execution plan parts to cluster nodes</li>
<li>execute query</li>
</ul>


<p>Tokenization and parsing of AQL queries hasn&rsquo;t changed much in 2.3:
query strings are still parsed using a Bison/Flex-based parser and
lexer combo. The AST structure has proven to be good during the parsing
stage, so the parser creates an initial AST from the query string first.</p>

<p>After that, simple optimizations are performed directly on the AST,
such as constant folding and constant propagation. Deterministic functions
with constant operands will be executed already in this stage and the
results be injected into the AST.</p>

<p>A major change in 2.3 is that no further transformations will be
carried out on the AST. Instead, the AST will be transformed into
an initial <em>execution plan</em>.</p>

<p>This execution plan is the starting point for the <em>query optimizer</em>.
It will take the initial execution plan and apply transformations to
it. Transformations will either update the existing plan in place or
create a new, modified plan. The result of the transformations carried
out will form the input for further transformations that can be carried
out by query optimizer.</p>

<p>The result of the query optimization stage is one or many execution
plans. For each plan, the optimizer will estimate a cost value, and
then finally pick the plan with the lowest total estimated cost.
This plan is considered to be the <em>optimal plan</em>. All other execution
plans will be discarded by the optimizer as it has considered them non-optimal.</p>

<p>The optimal execution plan is then executed by the <em>execution engine</em>.
For a single-server AQL query, this is straightforward: for each step
in the execution plan, a C++ object is created that is supposed to
execute the particular step. Query execution is then started by asking
the first of these objects for its results.</p>

<p>The objects for multiple processing steps are linked in a pipelined fashion
with lazy evaluation. Pulling data from the first object will eventually
trigger pulling data from the second object etc., until there are no more
results to produce.</p>

<p>For a distributed query, this is a bit more complicated. The different
execution steps will likely be shipped to different servers in the
cluster, and the objects need to be instanciated in different servers, too.
The different parts of the query may pull data from each other via HTTP
calls between cluster nodes.</p>

<h2>How execution plans work</h2>

<p>An execution plan is a sequence of query execution steps. Let&rsquo;s
start with a very simple example:</p>

<p><code>
FOR doc IN mycollection
  RETURN doc._key
</code></p>

<p>This query will be transformed into the following execution plan:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: iterates over all documents of a collection
and provides the current document in an output variable. In our example,
it will iterate over collection <code>mycollection</code> and provide each
document in variable <code>doc</code></li>
<li><em>CalculationNode</em>: evaluates an expression and returns its result.
In the example, it will calculate <code>doc._key</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>If this plan is going to be executed, the execution engine will start
pulling data from the node at the bottom, that is, the <em>ReturnNode</em>. The
<em>ReturnNode</em> at this stage cannot provide any data, so it will ask its
predecessor node, which in the example is the <em>CalculationNode</em>. The
<em>CalculationNode</em> again does not have own data yet, so it must ask the
node in front of it. The <em>EnumerateCollectionNode</em> will first ask the
<em>SingletonNode</em> for input data. So the execution flow has bubbled up from
the bottom of the sequence to the top.</p>

<p>The <em>SingletonNode</em> will now produce a single empty return value. It will
also internally set its processing status to <em>done</em>, so it will not produce
any more values if asked again. This is all a <em>SingletonNode</em> will ever do.
We&rsquo;ll see later why such a node may still be useful.</p>

<p>The single empty value will be provided as input to the <em>EnumerateCollectionNode</em>.
This node will now go through all the documents in the underlying collection,
and return them once for each input value its got. As its input value was
the singleton, it will return the documents of the collection just once.</p>

<p>Processing is executed in blocks of size 1000 by default. The
<em>EnumerateCollectionNode</em> will thus not return all documents to its successor
node, but just 1,000. The return value will be a vector with 1,000 documents,
stored under variable name <code>doc</code>.</p>

<p>The <em>CalculationNode</em>, still waiting for input data, can now execute its
expression <code>doc._key</code> on this input value. It will execute this expression
1,000 times, once for each input value. The expression results will be
stored in another variable. This variable is anonymous, as it hasn&rsquo;t been
named explicitly in the original query. The vector of results produced by
the <em>CalculationNode</em> is then returned to the <em>ReturnNode</em>, which will then
return it to the caller.</p>

<p>If the caller requests more documents, the procedure will repeat. Whenever
a processing step cannot produce any more data, it will ask its predecessor
step for more data. If the predecessor step already has status <em>done</em>, the
current step will set itself to <em>done</em> as well, so a query will actually
come to an end if there are no more results.</p>

<p>As can be seen, steps are executed with batches of values. We thought this
would be a good way to improve efficiency and reduce the number of hops
between steps.</p>

<h2>Joins</h2>

<p>Let&rsquo;s say we want to join documents from two collections, based on common
attribute values. Let&rsquo;s use <code>users</code> and <code>logins</code>, joined by their <code>id</code> and
<code>userId</code> attributes:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Provided that there are no indexes, the query may be turned into this
execution plan by the optimizer:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>logins</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>Now we can see why the <em>SingletonNode</em> is useful: it can be used as an
input to another node, telling this node to execute just once. Having the
<em>SingletonNode</em> will ensure that the outermost <em>EnumerateCollection</em>
will only iterate once over the documents in its underlying collection <code>users</code>.</p>

<p>The inner <em>EnumerateCollectionNode</em> for collection <code>logins</code> is now fed by
the outer <em>EnumerateCollectionNode</em> on <code>users</code>. Thus these two nodes will
produce a cartesian product. This will be done lazily, as producing results
will normally happen in chunks of 1,000 values each.</p>

<p>The results of the cartesian product are then post-filtered by the <code>FilterNode</code>,
which will only let those documents pass that match the filter condition of
the query. The <code>FilterNode</code> employs its predecessor, the <code>CalculationNode</code>,
to determine which values satisfy the condition.</p>

<h2>Using indexes</h2>

<p>Obviously creating cartesian products is not ideal. The optimizer will try
to avoid generating such plans if it can, but it has no choice if there are
no indexes present.</p>

<p>If there are indexes on attributes that are used in <code>FILTER</code> conditions of
a query, the optimizer will try to turn <code>EnumerateCollectionNode</code>s into
<code>IndexRangeNode</code>s. The purpose of an <code>IndexRangeNode</code> is to iterate over a
specific range in an index. This is normally more efficient than iterating
over all documents of a collection.</p>

<p>Let&rsquo;s assume there is an index on <code>logins.userId</code>. Then the optimizer might
be able to generate a plan like this:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>IndexRangeNode</em>: will iterate over the values in index <code>logins.userId</code> that
match the value of <code>users.id</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>To run this query, the execution engine must still iterate over all documents
in collection <code>users</code>, but for each of those, it only needs to find the documents
in <code>logins</code> that match the join condition. This most likely means a lot less
lookups and thus much faster execution.</p>

<h2>Permutation of loops</h2>

<p>Now consider adding an extra <code>FILTER</code> statement to the original query so we
end up with this:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
FILTER login.ts == 1415402319       /* added this one! */
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>The optimizer is free to permute the order of <code>FOR</code> loops as long as this
won&rsquo;t change the results of a query. In our case, permutation of the two
<code>FOR</code> loops is allowed (the query does not contain a <code>SORT</code> instruction so
the order of results is not guaranteed).</p>

<p>If the optimizer exchanges the two loops, it can also pull out the <code>FILTER</code>
statement on <code>login.ts</code> out of the inner loop, and move up into the outer loop.
It might come up with a plan like this, which may be more efficient if a
lot of documents from <code>logins</code> can be filtered out early:</p>

<p>```
FOR login IN logins
  FILTER login.ts == 1415402319
  FOR user IN users</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Exchanging the order of <code>FOR</code> loops may also allow the optimizer to use
additional indexes.</p>

<p>A last note on indexes: the optimizer in 2.3 is able to use (sorted)
skiplist indexes to eliminate extra <code>SORT</code> operations. For example, if
there is a skiplist index on <code>login.ts</code>, the <code>SORT</code> in the following
query can be removed by the optimizer:</p>

<p><code>
FOR login IN logins
  FILTER login.ts &gt; 1415402319
  SORT login.ts
  RETURN login
</code></p>

<p>The AQL optimizer in 2.3 can optimize away a <code>SORT</code> even if the sort
order is backwards or if no <code>FILTER</code> statement is used in the query at
all.</p>

<h2>Analyzing plans</h2>

<p>One particular improvement over 2.2 is that in ArangoDB 2.3 the optimizer
provides functionality for retrieving full execution plan information for
queries <strong>without</strong> executing them. The execution plan information can be
inspected by developers or DBAs, and, as it is JSON-encoded, can also be
analyzed programmatically.</p>

<p>Retrieving the execution plan for a query is straight-forward:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain();
</code></p>

<p>By default, the optimizer will return just the <em>optimal plan</em>, containing
all the plan&rsquo;s execution nodes with lots of extra information plus cost estimates.</p>

<p>The optimizer is also able to return the alternative plans it produced but
considered to be non-optimal:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain({ allPlans: true });
</code></p>

<p>This will hopefully allow developers and DBAs to get a better idea of how an
AQL query will be executed internally.</p>

<p>Additionally, simple execution statistics are returned by default when executing
a query. This statistics can also be used to get an idea of the runtime costs of
a query <strong>after</strong> execution.</p>

<h2>Writing optimizer rules</h2>

<p>The AQL optimizer itself is dumb. It will simply try to apply all transformations
from its rulebook to each input execution plan it is feeded with. This
will produce output execution plans, on which further transformations
may or may not be applied.</p>

<p>The more interesting part of the AQL optimizer stage is thus the rulebook.
Each rule in the rulebook is a C++ function that is executed for an input plan.</p>

<p>Adding a new optimizer rule to the rulebook is intentionally simple. One of
the design goals of the new AQL optimizer was to keep it flexible and extensible.
All that&rsquo;s need to be to add an optimizer rule is to implement a C++ function
with the following signature:</p>

<p><code>cpp
(Optimizer*, ExecutionPlan*, Optimizer::Rule const*) -&gt; int
</code></p>

<p>and register it once in the Optimizer&rsquo;s rulebook.</p>

<p>An optimizer rule function is called with an instance of the query optimizer
(it can use it to register a new plan), the current execution plan and some
information about the rule itself (this is the information about the rule from
the rulebook).</p>

<p>The optimizer rule function can then analyze the input execution plan, modifiy
it in place, and/or create additional plans. It must return a status code to
the optimizer to indicate if something went wrong.</p>

<h2>Outlook</h2>

<p>The AQL optimizer features described here are available in ArangoDB 2.3, which
is currently in <a href="https://www.arangodb.com/install-beta-version">beta stage</a>.</p>

<p>Writing a perfect query optimizer is a never-ending endeavour. Other databases
provide new optimizer features and fixes even decades after the initial version.</p>

<p>Our plan is to ship 2.3 with several essential and useful optimizer rules. We
will likely add more in future releases. We&rsquo;re also open to contributions.
If you can think of rules that are missing but you would like to see in ArangoDB,
please let us know. If you would like to contribute to the optimizer and write some
rule code, consider sending a pull request or an email to
<a href="hackers@arangodb.org">hackers@arangodb.org</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improved Non-unique Hash Indexes in 2.3]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes/"/>
    <updated>2014-11-07T20:51:12+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes</id>
    <content type="html"><![CDATA[<p>With ArangoDB 2.3 now getting into the <a href="https://www.arangodb.com/install-beta-version">beta stage</a>,
it&rsquo;s time to spread the word about new features and improvements.</p>

<p>Today&rsquo;s post will be about the changes made to non-unique hash
indexes.</p>

<!-- more -->


<p>Hash indexes allow looking up documents quickly if the indexed
attributes are all provided in a search query. They are not
suitable for range queries, but are the perfect choice if equality
comparisons are all that&rsquo;s needed.</p>

<p>Hash indexes have been available in ArangoDB ever since. There
have always been two variants of them:</p>

<ul>
<li>unique hash indexes</li>
<li>non-unique hash indexes</li>
</ul>


<p>There wasn&rsquo;t much to be done for unique hash indexes, and so there
haven&rsquo;t been any changes to them in 2.3. However, the non-unique
hash indexes were improved significantly in the new version.</p>

<p>The non-unique indexes already performed quite well if most of the
indexed values were unique and only few repetitions occurred. But their
performance suffered severly if the indexed attribute values repeated
a lot &ndash; that is, when the indexed value had a <strong>low cardinality</strong> and thus
the index had a <strong>low selectivity</strong>.</p>

<p>This was a problem because it slowed down inserting new documents into
a collection with such an index. And it also slowed down loading collections
with low cardinality hash indexes.</p>

<p>I am happy to state that in ArangoDB 2.3 this has been fixed, and the insert
performance of non-unique hash indexes has been improved significantly.
The index insertion time now scales quite well with the number
of indexed documents regardless of the cardinality of the indexed
attribute.</p>

<p>Following are a few measurements of non-unique hash index insertion
times from ArangoDB 2.3, for different cardinalities of the indexed
attribute.</p>

<p>The times reported are the net non-unique hash index
insertion times (the documents were present already, just the index
was created on them and index creation time was measured).</p>

<p>Let&rsquo;s start with a not too challenging case: indexing documents in
a collection with 100,000 different index values (<em>cardinality 100,000</em>):</p>

<p><code>text index insertion times for cardinality 100,000
number of documents:    128,000    =&gt;    time:   0.144 s
number of documents:    256,000    =&gt;    time:   0.231 s
number of documents:    512,000    =&gt;    time:   0.347 s
number of documents:  1,024,000    =&gt;    time:   0.694 s
number of documents:  2,048,000    =&gt;    time:   1.379 s
</code></p>

<p>The picture doesn&rsquo;t change much when reducing the cardinality
by a factor or 10 (i.e. <em>cardinality 10,000</em>):</p>

<p><code>text index insertion times for cardinality 10,000
number of documents:    128,000    =&gt;    time:   0.169 s
number of documents:    256,000    =&gt;    time:   0.194 s
number of documents:    512,000    =&gt;    time:   0.355 s
number of documents:  1,024,000    =&gt;    time:   0.668 s
number of documents:  2,048,000    =&gt;    time:   1.325 s
</code></p>

<p>Let&rsquo;s again divide cardinality by 10 (now <em>cardinality 1,000</em>):</p>

<p><code>text index insertion times for cardinality 1,000
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.152 s
number of documents:    512,000    =&gt;    time:   0.261 s
number of documents:  1,024,000    =&gt;    time:   0.524 s
number of documents:  2,048,000    =&gt;    time:   0.934 s
</code></p>

<p><em>Cardinality 100</em>:</p>

<p><code>text index insertion times for cardinality 100
number of documents:    128,000    =&gt;    time:   0.114 s
number of documents:    256,000    =&gt;    time:   0.148 s
number of documents:    512,000    =&gt;    time:   0.337 s
number of documents:  1,024,000    =&gt;    time:   0.452 s
number of documents:  2,048,000    =&gt;    time:   0.907 s
</code></p>

<p><em>Cardinality 10</em>:</p>

<p><code>text index insertion times for cardinality 10
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.327 s
number of documents:    512,000    =&gt;    time:   0.239 s
number of documents:  1,024,000    =&gt;    time:   0.442 s
number of documents:  2,048,000    =&gt;    time:   0.827 s
</code></p>

<p>Finally we get to <em>cardinality 1</em>, the definitive indicator
for the index being absolutely useless. Let&rsquo;s create it anyway,
for the sake of completeness of this post:</p>

<p><code>text index insertion times for cardinality 1
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    128,000    =&gt;    time:   0.095 s
number of documents:    256,000    =&gt;    time:   0.146 s
number of documents:    512,000    =&gt;    time:   0.246 s
number of documents:  1,024,000    =&gt;    time:   0.445 s
number of documents:  2,048,000    =&gt;    time:   0.925 s
</code></p>

<p>On a side note: all indexed values were numeric. In absolute terms,
indexing string values will be slower than indexing numbers, but insertion
should still scale nicely with the number of documents as long as everything
fits in RAM.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Test Data]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data/"/>
    <updated>2014-11-04T22:14:21+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data</id>
    <content type="html"><![CDATA[<p>Today I was asked to look at code that was supposed to read data
from a MySQL data source, process it and then import it into ArangoDB.</p>

<p>To run and debug the code I had to have some MySQL data source. So I
thought I&rsquo;d quickly set up a simple example table with a few rows.
It turned out that this took more time than what I had expected.</p>

<p>Maybe I&rsquo;m spoilt by JavaScript-enabled, schema-free databases where
creating such test setups is so much easier.</p>

<!-- more -->


<p>I worked with MySQL databases in production for 10+ years and spent
much time working with the mysql client. I always liked MySQL, but in
the past few years, I was driven away from it and lost contact.
Instead, I got sucked into the NoSQL landscape and enjoy it pretty much.</p>

<p>Getting back to the original problem: I needed some MySQL table with a
few thousand rows for a test. It turned out I didn&rsquo;t even have MySQL
installed on my computer, so I needed to install it first.</p>

<p>After setting up the MySQL server, I created a table <code>examples</code> for
storing my test data:</p>

<p><code>sql
CREATE DATABASE test;
USE test;
CREATE TABLE examples (attribute1 VARCHAR(20), attribute2 VARCHAR(20));
</code></p>

<p>Not really the black belt of schema design, but good enough for a quick
test.</p>

<p>Now the table needed some rows. 100,000 rows should be enough. I wrote
some bash script to create them as there is no sane way to do this with
the MySQL client alone:</p>

<p><code>``bash
for i in</code>seq 1 100000`
  do</p>

<pre><code>echo "INSERT INTO examples VALUES (\"test$i\", \"test$i\");" &gt;&gt; import.sql 
</code></pre>

<p>  done
```</p>

<p>Time to import the data!</p>

<p><code>bash
mysql -u user test &lt; import.sql
</code></p>

<p>At first I was a bit surprised this command did not return instantly. I let it
run for about a minute, and then began checking the import progress with a second mysql
client. It turned out only very few records had been imported, and the import
script continued to create only around 30-35 records per second.</p>

<p>Seems I had forgotten that I am working with a No-NoSQL database, with full
ACID semantics for everything. My import file contained 100,000 <code>INSERT</code>
statements, so I was asking to perform 100,000 transactions and fsync operations.
That import would have taken forever with my slow HDD!</p>

<p>I quickly changed the InnoDB setting to make it commit only about once per second:
<code>
mysql&gt; SET GLOBAL innodb_flush_log_at_trx_commit = 2;
Query OK, 0 rows affected (0.00 sec)
</code></p>

<p>Now the import finished in 7 seconds.</p>

<p>I finally got the data in MySQL, but overall it took me about 10 minutes to get
it done. Probably a bit less if I still were an active user of MySQL and had
remembered the default behavior right from the start.</p>

<p>Still, my feeling is that it takes too much time to get something so simple
done.</p>

<p>I don&rsquo;t blame the database for trying to commit all 100,000 single-row
<code>INSERT</code> operations and fsync them to disk. It simply cannot know if the data
are important or just throw-away test records.</p>

<p>But there are other reasons: I had to write a bash script to produce the
test data, as there is no sane way to do this with the MySQL client alone.
Writing bash scripts is fine, and in general I like it, but I don&rsquo;t want to
do it for a dead-simple test setup.</p>

<p>And by the way, what if it turns out that I need to generate slightly more
complex test data? In the MySQL case I probably would have resorted to sed
or awk or would have thrown away my bash script and had rewritten it in some
other language. So I would have wasted even more time.</p>

<p>I personally prefer the ability to use a scripting language for such tasks.
JavaScript is ubiquituous these days, and I want to use it in a database&rsquo;s
command-line client.</p>

<p>For example, here&rsquo;s how the test setup would look like in the ArangoShell:
<code>js
db._create("examples");
for (i = 0; i &lt; 100000; ++i) {
  db.examples.save({ attribute1: "test" + i, attribute2: "test" + i });
}
</code>
I find this much easier to use: it allows to do everything in one place,
removing the need to write another script that prepares a data file or an
SQL file first.</p>

<p>As a bonus, using a programming language is much more flexible and powerful.
If I needed to generate slightly more complex test data, I can just do it,
adjust the JavaScript code and re-run it.</p>

<p>Even more annoying to me is that I needed to provide a schema for the
table first. I could have got away with declaring all text fields as
<code>VARCHAR(255)</code> or <code>TEXT</code> so I can at least ignore string
length restrictions. But I still need to type in the table schema
once, even if it feels completely useless for this particular use case.</p>

<p>It would get even more annoying if during my test I noticed I needed more
or other columns. Then I would need to adjust the table schema using <code>ALTER TABLE</code>
or adjust the <code>CREATE TABLE</code> statement and run it again, keeping me
away from the original task.</p>

<p>Maybe using schema-free databases for too long has spoilt me, but I much
more prefer starting quickly and without a schema. I know the data that
I am going to load will have a structure and will be somewhat self-describing,
so the database can still figure out what the individual parts of a record are.</p>

<p><em>On a side-note: should you be a fan of using query languages, the same
test setup can also be achieved by running the following AQL query from
the ArangoShell</em>:
```js
db._query(&ldquo;FOR i IN 1..100000 LET value = CONCAT(&lsquo;test&rsquo;, i) &rdquo; +</p>

<pre><code>      "INSERT { attribute1: value, attribute2: value } INTO examples");
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
</feed>
