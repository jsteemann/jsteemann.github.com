<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-04T12:04:59+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Return Value Optimization for AQL]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql/"/>
    <updated>2015-05-04T10:32:43+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql</id>
    <content type="html"><![CDATA[<p>While in search for further AQL query optimizations last week, we found that intermediate AQL
query results were copied one time too often in some cases. Precisely, the data that a query&rsquo;s
<code>ReturnNode</code> will return to the caller was copied into the <code>ReturnNode</code>&rsquo;s own register. With
<code>ReturnNode</code>s never modifying their input data, this demanded for something that is called
<em>return-value optimization</em> in compilers.</p>

<p>2.6 will now optimize away these copies in many cases, and this post shows which performance
benefits can be expected due to the optimization.</p>

<!-- more -->


<p>The effect of the optimization can be demonstrated easily with a few simple AQL queries.
Let&rsquo;s start with a query that simply returns all 100,000 documents from a collection <code>users</code>:
<code>FOR u IN users RETURN u</code> (the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>).</p>

<p>This query&rsquo;s execution plan is already straight-forward and simple:</p>

<p><img src="/downloads/screenshots/return.png"></p>

<p>So where&rsquo;s the problem?</p>

<p>The <code>ReturnNode</code> in this query (and other queries too) will copy its input data into its own output
register, only to finally hand the results to the query&rsquo;s caller. This copying is most often unnecessary
as the <code>ReturnNode</code> will not modify its input. So the idea was to get rid of the copying action and
tell the query&rsquo;s calling code in which (now different) register to look for the results.</p>

<p>Optimizing away the copying inside the <code>ReturnNode</code> made the query run faster already.
The same query now returns the 100,000 documents in 0.24 to 0.26 seconds, compared to 0.27 to 0.30 s
before applying the optimization.</p>

<p>Returning just an attribute of each document shows about the same improvement rates. The execution
times of the query <code>FOR u IN users RETURN u._key</code> drop to between 0.13 and 0.14 seconds with the
optimization, from initially between 0.15 and 0.17 seconds.</p>

<p>Another example query, <code>FOR i IN 1..1000000 RETURN i</code>, now runs in 0.58 to 0.61 seconds with
the optimization, compared to between 0.77 and 0.81 seconds without it.</p>

<p>These absolute figures may not look overly impressive, but they indicate relative improvements of
between 10 and 25 %, which is quite nice. This is effectively saved CPU time that can now be used
for something more productive.</p>

<p>Of course the performance improvements may not be that high for every imaginable AQL query.
Though the optimization may be active in most AQL queries, its effect will only be measurable
for queries that return a significant number of documents/values. Otherwise the share of the
<code>ReturnNode</code>&rsquo;s work in the query&rsquo;s overall computations may be too low to have any effect.
Additionally, the more work a query spends in performing other operations (e.g. filtering,
sorting, collecting), the less relevant will be the overall effect of the optimized <code>ReturnNode</code>.
Finally, when query results need to be shipped from the server to the client over a network,
the relative effect of the optimization may diminish further.</p>

<p>So your mileage may vary. But the optimization will not do any harm, and together with some
other query optimizations already finished for 2.6 it will contribute to many AQL queries
running faster than before.</p>

<p>AQL queries will benefit from the optimization automatically in ArangoDB 2.6, without requiring
any adjustments to the query string, the server configuration etc. The optimizer will automatically
apply the optimization for the main-level <code>ReturnNode</code> of every AQL query.</p>

<p>On a side note: the optimization will not be shown in the list of applied optimizer rules for the
query. This is because the optimization is performed in some different place in the query
executor, after applying the optimizer rules.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exporting Data for Offline Processing]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/24/exporting-data-for-offline-processing/"/>
    <updated>2015-04-24T15:47:31+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/24/exporting-data-for-offline-processing</id>
    <content type="html"><![CDATA[<p>A few weeks ago I wrote about ArangoDB&rsquo;s
<a href="https://jsteemann.github.io/blog/2015/04/04/more-efficient-data-exports/">specialized export API</a>.</p>

<p>The export API is useful when the goal is to extract all documents from a given collection
and to process them outside of ArangoDB.</p>

<p>The export API can provide quick and memory-efficient snapshots of the data in the underlying
collection, making it suitable for extract all documents of the collection. It will be able
to provide data much faster than with an AQL query that will extract all documents.</p>

<p>In this post I&rsquo;ll show how to use the export API to extract data and process it with PHP.</p>

<!-- more -->


<p>A prerequiste for using the export API is using an ArangoDB server with version 2.6
or higher. As there hasn&rsquo;t been an official 2.6 release yet, this currently requires
building the <code>devel</code> branch of ArangoDB from source. When there is a regular 2.6
release, this should be used instead.</p>

<h2>Importing example data</h2>

<p>First we need some data in an ArangoDB collection that we can process externally.</p>

<p>For the following examples, I&rsquo;ll use a collection named <code>users</code> which I&rsquo;ll populate
with 100k <a href="/downloads/code/users-100000.json.tar.gz">example documents</a>. Here&rsquo;s how
to get this data into ArangoDB:</p>

<p>```bash commands for fetching and importing data</p>

<h1>download data file</h1>

<p>wget <a href="https://jsteemann.github.io/downloads/code/users-100000.json.tar.gz">https://jsteemann.github.io/downloads/code/users-100000.json.tar.gz</a></p>

<h1>uncompress it</h1>

<p>tar xvfz users-100000.json.tar.gz</p>

<h1>import into ArangoDB</h1>

<p>arangoimp &mdash;file users-100000.json &mdash;collection users &mdash;create-collection true
```</p>

<p>There should now be 100K documents present in a collection named <code>users</code>. You can
quickly verify that by peeking into the collection using the web interface.</p>

<h2>Setting up ArangoDB-PHP</h2>

<p>An easy way of trying the export API is to use it from PHP. We therefore clone the
devel branch of the <strong>arangodb-php</strong> Github repository into a local directory:</p>

<p><code>bash cloning arangodb-php
git clone -b devel "https://github.com/arangodb/arangodb-php.git"
</code></p>

<p>Note: when there is an official 2.6 release, the <code>2.6</code> branch of arangodb-php should
be used instead of the <code>devel</code> branch.</p>

<p>We now write a simple PHP script that establishes a connection to the ArangoDB
server running on localhost. We&rsquo;ll extend that file gradually. Here&rsquo;s a skeleton
file to start with. The code can be downloaded <a href="/downloads/code/export-skeleton.php">here</a>:</p>

<p>```php skeleton file for establishing a connection
&lt;?php</p>

<p>namespace triagens\ArangoDb;</p>

<p>// use the driver&rsquo;s autoloader to load classes
require &lsquo;arangodb-php/autoload.php&rsquo;;
Autoloader::init();</p>

<p>// set up connection options
$connectionOptions = array(
  // endpoint to connect to
  ConnectionOptions::OPTION_ENDPOINT     => &lsquo;tcp://localhost:8529&rsquo;,
  // can use Keep-Alive connection
  ConnectionOptions::OPTION_CONNECTION   => &lsquo;Keep-Alive&rsquo;,         <br/>
  // use basic authorization
  ConnectionOptions::OPTION_AUTH_TYPE    => &lsquo;Basic&rsquo;,               <br/>
  // user for basic authorization
  ConnectionOptions::OPTION_AUTH_USER    => &lsquo;root&rsquo;,                    <br/>
  // password for basic authorization
  ConnectionOptions::OPTION_AUTH_PASSWD  => &lsquo;&rsquo;,                    <br/>
  // timeout in seconds
  ConnectionOptions::OPTION_TIMEOUT      => 30,
  // database name
  ConnectionOptions::OPTION_DATABASE     => &lsquo;_system&rsquo;
);</p>

<p>try {
  // establish connection
  $connection = new Connection($connectionOptions);</p>

<p>  echo &lsquo;Connected!&rsquo; . PHP_EOL;</p>

<p>  // TODO: now do something useful with the connection!</p>

<p>} catch (ConnectException $e) {
  print $e . PHP_EOL;
} catch (ServerException $e) {
  print $e . PHP_EOL;
} catch (ClientException $e) {
  print $e . PHP_EOL;
}
```</p>

<p>Running that script should simply print <code>Connected!</code>. This means the PHP script
can connect to ArangoDB and we can go on.</p>

<h2>Extracting the data</h2>

<p>With a working database connection we can now start with the actual processing.
In place of the <code>TODO</code> in the skeleton file, we can actually run an export of
the data in collection <code>users</code>. The following simple function extracts all
documents from the collection and writes them to an output file <code>output.json</code>
in JSON format.</p>

<p>It will also print some statistics about the number of documents and the total
data size. The full script can be downloaded <a href="/downloads/code/export.php">here</a>:</p>

<p>```php exporting data into a file
function export($collection, Connection $connection) {
  $fp = fopen(&lsquo;output.json&rsquo;, &lsquo;w&rsquo;);</p>

<p>  if (! $fp) {</p>

<pre><code>throw new Exception('could not open output file!');
</code></pre>

<p>  }</p>

<p>  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true       // use simple PHP arrays
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // statistics
  $count   = 0;
  $batches = 0;
  $bytes   = 0;</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';
foreach ($docs as $doc) {
  $output .= json_encode($doc) . PHP_EOL;
} 

// write out chunk
fwrite($fp, $output);

// update statistics
$count += count($docs);
$bytes += strlen($output);
++$batches;
</code></pre>

<p>  }</p>

<p>  fclose($fp);</p>

<p>  echo sprintf(&lsquo;written %d documents in %d batches with %d total bytes&rsquo;,</p>

<pre><code>           $count,
           $batches, 
           $bytes) . PHP_EOL;
</code></pre>

<p>}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<p>Running this version of the script should print something similar to the following
and also produce a file named <code>output.json</code>. Each line in the file should be a JSON
object representing a document in the collection.</p>

<p><code>plain script output
written 100000 documents in 20 batches with 40890013 total bytes
</code></p>

<h2>Applying some transformations</h2>

<p>We now use PHP to transform data as we extract it. With an example script, we&rsquo;ll apply
the following transformations on the data:</p>

<ul>
<li>rewrite the contents of the <code>gender</code> attribute:

<ul>
<li><code>female</code> should become <code>f</code></li>
<li><code>male</code> should become <code>m</code></li>
</ul>
</li>
<li>rename attribute <code>birthday</code> to <code>dob</code></li>
<li>change date formats in <code>dob</code> and <code>memberSince</code> from YYYY-MM-DD to MM/DD/YYYY</li>
<li>concatenate the contents of the <code>name.first</code> and <code>name.last</code> subattributes</li>
<li>transform array in <code>contact.email</code> into a flat string</li>
<li>remove all other attributes</li>
</ul>


<p>Here&rsquo;s a transformation function that does this, and a slightly simplified export
function. This version of the script can also be downloaded <a href="/downloads/code/export-transform.php">here</a>:</p>

<p>```php transformation and export functions
function transformDate($value) {
  return preg_replace(&lsquo;/^(\d+)&ndash;(\d+)&ndash;(\d+)$/&rsquo;, &lsquo;\2/\3/\1&rsquo;, $value);
}</p>

<p>function transform(array $document) {
  static $genders = array(&lsquo;male&rsquo; => &rsquo;m', &lsquo;female&rsquo; => &lsquo;f&rsquo;);</p>

<p>  $transformed = array(</p>

<pre><code>'gender'      =&gt; $genders[$document['gender']],
'dob'         =&gt; transformDate($document['birthday']),
'memberSince' =&gt; transformDate($document['memberSince']),
'fullName'    =&gt; $document['name']['first'] . ' ' . $document['name']['last'],
'email'       =&gt; $document['contact']['email'][0]
</code></pre>

<p>  );</p>

<p>  return $transformed;
}</p>

<p>function export($collection, Connection $connection) {
  $fp = fopen(&lsquo;output-transformed.json&rsquo;, &lsquo;w&rsquo;);</p>

<p>  if (! $fp) {</p>

<pre><code>throw new Exception('could not open output file!');
</code></pre>

<p>  }</p>

<p>  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true       // use simple PHP arrays
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';
foreach ($docs as $doc) {
  $output .= json_encode(transform($doc)) . PHP_EOL;
} 

// write out chunk
fwrite($fp, $output);
</code></pre>

<p>  }</p>

<p>  fclose($fp);
}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<p>The adjusted version of the PHP script will now produce an output file named
<code>output-transformed.json</code>.</p>

<h2>Filtering attributes</h2>

<p>In the last example we discarded a few attributes of each document. Instead of
filtering out these attributes with PHP, we can configure the export to already
exclude these attributes server-side. This way we can save some traffic.</p>

<p>Here&rsquo;s an adjusted configuration that will exclude the unneeded attributes <code>_id</code>,
<code>_rev</code>, <code>_key</code> and <code>likes</code>:</p>

<p>```php configuration for attribute exclusion
// settings to use for the export
$settings = array(
  &lsquo;batchSize&rsquo; => 5000,  // export in chunks of 5K documents
  &lsquo;_flat&rsquo; => true,      // use simple PHP arrays
  &lsquo;restrict&rsquo; => array(</p>

<pre><code>'type' =&gt; 'exclude',
'fields' =&gt; array('_id', '_rev', '_key', 'likes')
</code></pre>

<p>  )
);
```</p>

<p>The full script that employs the adjusted configuration can be downloaded
<a href="/downloads/code/export-exclude.php">here</a>.</p>

<p>Instead of excluding specific attributes we can also do it the other way and only
include certain attributes in an export. The following script demonstrates this by
extracting only the <code>_key</code> and <code>name</code> attributes of each document. It then prints the
key/name pairs in CSV format.</p>

<p>The full script can be downloaded <a href="/downloads/code/export-csv.php">here</a>.</p>

<p>```php export function that prints key/name pairs in CSV format
function export($collection, Connection $connection) {
  // settings to use for the export
  $settings = array(</p>

<pre><code>'batchSize' =&gt; 5000,  // export in chunks of 5K documents
'_flat' =&gt; true,      // use simple PHP arrays
'restrict' =&gt; array(
  'type' =&gt; 'include',
  'fields' =&gt; array('_key', 'name')
)
</code></pre>

<p>  );</p>

<p>  $export = new Export($connection, $collection, $settings);</p>

<p>  // execute the export. this will return an export cursor
  $cursor = $export->execute();</p>

<p>  // now we can fetch the documents from the collection in batches
  while ($docs = $cursor->getNextBatch()) {</p>

<pre><code>$output = '';

foreach ($docs as $doc) {
  $values = array(
    $doc['_key'], 
    $doc['name']['first'] . ' ' . $doc['name']['last']
  );

  $output .= '"' . implode('","', $values) . '"' . PHP_EOL;
}

// print out the data directly 
print $output;
</code></pre>

<p>  }
}</p>

<p>// run the export
export(&lsquo;users&rsquo;, $connection);
```</p>

<h2>Using the API without PHP</h2>

<p>The export API REST interface is simple and it can be used with any client that can
speak HTTP. This includes <em>curl</em> obviously:</p>

<p>The following command fetches the initial 5K documents from the <code>users</code> collection
using <em>curl</em>:</p>

<p><code>bash using the export API with curl
curl                                                   \
  -X POST                                              \
  http://localhost:8529/_api/export?collection=users   \
  --data '{"batchSize":5000}'
</code></p>

<p>The HTTP response will contain a <code>result</code> attribute that contains the actual
documents. It will also contain an attribute <code>hasMore</code> that will indicate whether
there are more documents for the client to fetch. If it is set to <code>true</code>, the
HTTP response will also contain an attribute <code>id</code>. The client can use this id
for sending follow-up requests like this (assuming the returned id was <code>13979338067709</code>):</p>

<p><code>bash sending a follow-up request with curl
curl                                                   \
  -X PUT                                               \
  http://localhost:8529/_api/export/13979338067709  
</code></p>

<p>That&rsquo;s about it. Using the export API it should be fairly simple to ship bulk
ArangoDB data to client applications or data processing tools.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Functions Improvements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/23/aql-functions-improvements/"/>
    <updated>2015-04-23T10:24:53+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/23/aql-functions-improvements</id>
    <content type="html"><![CDATA[<p>Waiting for a <code>git pull</code> to complete over an 8 KiB/s internet connection is boring.
So I thought I&rsquo;d rather use the idle time and quickly write about some performance
improvements for certain AQL functions that were recently completed and that will
become available with ArangoDB 2.6.</p>

<p>The improvements affect the following AQL functions:</p>

<ul>
<li><code>UNSET()</code>: remove specified attributes from an object/document</li>
<li><code>KEEP()</code>: keep only specified attributes of an object/document</li>
<li><code>MERGE()</code>: merge the attributes of multiple objects/documents</li>
</ul>


<p>This blog post shows a few example queries that will benefit from <strong>50 to more than 60 %
reductions</strong> in query execution times due to the changes done to these functions.</p>

<!-- more -->


<h2>When to expect benefits</h2>

<p>Reductions in execution time can be expected for AQL queries invoking one of the above
AQL functions many times, and if the AQL function is used in a so-called <em>simple</em>
calculation. Whether or not a calculation is considered <em>simple</em> is shown in the detailed
JSON output of an <code>explain()</code>.</p>

<p>Queries will not benefit if they invoke the AQL functions only a few times or when the
function call is contained in an expression that is executed using the non-<em>simple</em> code path.</p>

<h2>Example queries</h2>

<p>Following are a few example queries for the three AQL functions, showing the reductions in
execution times. They all use the <em>simple</em> code path so the benefits can be reaped.</p>

<p>For comparing the execution times between 2.5 and 2.6, I have prepared a simple test setup.
Here is a test function that will create a collection named <code>test</code> and populate it
with a configurable amount documents. It will then run an AQL query that will update
each document in the collection, using one of the named AQL functions. The function
will return the execution time for the AQL query, excluding the collection setup time:</p>

<p>```js test function
var run = function (n, query) {
  var time = require(&ldquo;internal&rdquo;).time;
  var db = require(&ldquo;org/arangodb&rdquo;).db;</p>

<p>  // drop and re-create test collection
  db.<em>drop(&ldquo;test&rdquo;);
  var c = db.</em>create(&ldquo;test&rdquo;);</p>

<p>  // insert n documents
  for (var i = 0; i &lt; n; ++i) {</p>

<pre><code>c.insert({ value1: i, value2: i, value3: 'foobar' + i }); 
</code></pre>

<p>  }</p>

<p>  // flush write-ahead log and wait a few seconds before running query
  require(&ldquo;internal&rdquo;).wal.flush();
  require(&ldquo;internal&rdquo;).wait(5);</p>

<p>  // run query
  var s = time();
  db._query(query);
  return time() &ndash; s;
};
```</p>

<h3>UNSET()</h3>

<p>Let&rsquo;s start with the <code>UNSET()</code> AQL function. Its purpose is to remove or multiple
attributes from an object/document. Here is an example AQL query that removes
attribute <code>value2</code> from each document in the <code>test</code> collection:</p>

<p><code>js invocation of UNSET()
run(n, "FOR t IN test LET modified = UNSET(t, 'value2') REPLACE t WITH modified IN test");
</code></p>

<p>The execution times (in seconds) for different <code>n</code> values in ArangoDB 2.5 and 2.6 are:</p>

<p>```plain execution times of UNSET() query in 2.5 and 2.6</p>

<h2>value of n       time 2.5       time 2.6       speedup</h2>

<p>   100,000         3.28 s         1.13 s          65 %
   500,000        16.93 s         5.38 s          68 %
 1,000,000        32.60 s        11.40 s          65 %
```</p>

<h3>KEEP()</h3>

<p>The purpose of <code>KEEP()</code> is to remove all attributes from an object/document but the
specified ones. Here&rsquo;s an example query that uses <code>KEEP()</code> to remove all attributes
from the documents in the <code>test</code> collectionn but attribute <code>value2</code>:</p>

<p><code>js invocation of KEEP()
run(n, "FOR t IN test LET modified = KEEP(t, 'value2') REPLACE t WITH modified IN test");
</code></p>

<p>The execution times (in seconds) for different <code>n</code> values in ArangoDB 2.5 and 2.6 are:</p>

<p>```plain execution times of KEEP() query in 2.5 and 2.6</p>

<h2>value of n       time 2.5       time 2.6       speedup</h2>

<p>   100,000         1.98 s         0.87 s          56 %
   500,000         9.34 s         4.09 s          56 %
 1,000,000        18.86 s         8.23 s          56 %
```</p>

<h3>MERGE()</h3>

<p>Finally, the <code>MERGE()</code> function can be used to merge multiple objects/documents in a
single one. The following query will add an attribute <code>value4</code> to all documents in
collection <code>test</code>:</p>

<p><code>js invocation of MERGE()
run(n, "FOR t IN test LET modified = MERGE(t, { value4 : 1 }) REPLACE t WITH modified IN test");
</code></p>

<p>The execution times (in seconds) for different <code>n</code> values in ArangoDB 2.5 and 2.6 are:</p>

<p>```plain execution times of MERGE() query in 2.5 and 2.6</p>

<h2>value of n       time 2.5       time 2.6       speedup</h2>

<p>   100,000         3.93 s         1.22 s          68 %
   500,000        19.17 s         5.91 s          69 %
 1,000,000        38.27 s        12.33 s          67 %
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[COLLECTing With a Hash Table]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/22/collecting-with-a-hash-table/"/>
    <updated>2015-04-22T13:53:10+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/22/collecting-with-a-hash-table</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.6 will feature an alternative <strong>hash</strong> implementation of the AQL <code>COLLECT</code>
operation. The new implementation can speed up some AQL queries that can not exploit indexes
on the <code>COLLECT</code> group criteria.</p>

<p>This blog post provides a preview of the feature and shows some nice performance improvements.
It also explains the <code>COLLECT</code>-related optimizer parts and how the optimizer will decide whether
to use the new or the traditional implementation.</p>

<!-- more -->


<h2>Introduction to COLLECT</h2>

<p>A quick recap: in AQL, the <code>COLLECT</code> operation can be used for grouping and optionally counting values.</p>

<p>Here&rsquo;s an example, using flight data:</p>

<p><code>plain AQL COLLECT example
FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count
  RETURN { from: from, count: count }
</code></p>

<p>This query will iterate over all documents in collection <code>flights</code>, and count the
number of flights per different <code>_from</code> value (origin airport). The query result will
contain only unique <code>from</code> values plus a counter for each:</p>

<p><code>json query result, grouped by from
[
  { "from" : "airports/ABE", "count" : 6205 },
  { "from" : "airports/ABQ", "count" : 39346 },
  { "from" : "airports/ACV", "count" : 362 },
  ...
  { "from" : "airports/YAP", "count" : 285 },
  { "from" : "airports/YKM", "count" : 879 },
  { "from" : "airports/YUM", "count" : 2275 }
]
</code></p>

<p>As the <code>COLLECT</code> will group its result according to the specified group criteria (<code>flights._from</code>
in the above query), it needs a way of figuring out to which group any input value does belong.</p>

<p>Before ArangoDB 2.6, there was a single method for determining the group. Starting with ArangoDB
2.6, the query optimizer can choose between two different <code>COLLECT</code> methods, the <strong>sorted</strong> method
and the <strong>hash</strong> method.</p>

<h2>Sorted COLLECT method</h2>

<p>The traditional method for determining the group values is the <strong>sorted</strong> method. It has been
available in ArangoDB since the very start.</p>

<p>The sorted method of <code>COLLECT</code> requires its input to be sorted by the group criteria specified
in the <code>COLLECT</code> statement. Because there is no guarantee that the input data are already sorted
in the same way, the query optimizer will automatically insert a <code>SORT</code> statement into the query
in front of the <code>COLLECT</code>. In case there is a sorted index present on the group criteria attributes,
the optimizer may be able to optimize away the <code>SORT</code> again. If there is no sorted index present
on the group criteria attributes, the <code>SORT</code> will remain in the execution plan.</p>

<p>Here is the execution plan for the above query using the <strong>sorted</strong> method of <code>COLLECT</code>. We can see
the extra <code>SortNode</code> with id #7 being added by the optimizer in front of the <code>COLLECT</code>:</p>

<p><img src="/downloads/screenshots/collect-sorted.png"></p>

<p>The <strong>sorted</strong> method of <code>COLLECT</code> is efficient because it can write out a group result whenever
an input value will start a new group. Therefore it does not need to keep the whole <code>COLLECT</code>
result in memory. The downside of using the sorted method is that it requires its input to be
sorted, and that this requires adding an extra <code>SORT</code> for not properly sorted input.</p>

<h2>Hash COLLECT method</h2>

<p>Since ArangoDB 2.6, the query optimizer can also employ the <strong>hash</strong> method for <code>COLLECT</code>. The
hash method works by assigning the input values of the <code>COLLECT</code> to slots in a hash table. It
does not require its input to be sorted. Because the entries in the hash table do not have a
particular order, the query optimizer will add a post-<code>COLLECT</code> <code>SORT</code> statement. With this extra
sort of the <code>COLLECT</code> result, the optimizer ensures that the output of the sorted <code>COLLECT</code> will
be the same as the output of the hash <code>COLLECT</code>.</p>

<p>Here is the execution plan for the above query when using the <strong>hash</strong> method of <code>COLLECT</code>.
Here we can see the extra <code>SortNode</code> with id #7 being added post-<code>COLLECT</code>:</p>

<p><img src="/downloads/screenshots/collect-hash.png"></p>

<p>The <strong>hash</strong> method is beneficial because it does not require sorted input and thus no extra
<code>SORT</code> step in front. However, as the input is not sorted, it is never clear when a group is
actually finished. The hash method therefore needs to build the whole <code>COLLECT</code> result in memory
until the input is exhausted. Then it can safely write out all group results. Additionally,
the result of the hash <code>COLLECT</code> is unsorted. Therefore the optimizer will add a post-<code>COLLECT</code>
sort to ensure the result will be identical to a <strong>sorted</strong> <code>COLLECT</code>.</p>

<h2>Which method will be used when?</h2>

<p>The query optimizer will always take the initial query plan and specialize its <code>COLLECT</code> nodes to
using the <strong>sorted</strong> method. It will also add the pre-<code>COLLECT</code> <code>SORT</code> in the original plan.</p>

<p>In addition, for every <code>COLLECT</code> statement not using an <code>INTO</code> clause, the optimizer will create
a plan variant that uses the <strong>hash</strong> method. In that plan variant, the post-<code>COLLECT</code> <code>SORT</code>
will be added. Note that a <code>WITH COUNT INTO</code> is still ok here, but that using a regular <code>INTO</code>
clause will disable the usage of the <strong>hash</strong> method:</p>

<p><code>plain a query that cannot use the hash method
FOR flight IN flights
  COLLECT from = flight._from INTO allFlights
  RETURN { from: from, flights: allFlights }
</code></p>

<p>If more than one <code>COLLECT</code> method can be used for a query, the created plans will be shipped through
the regular optimization pipeline. In the end, the optimizer will pick the plan with the lowest
estimated total cost as it will do for all other queries.</p>

<p>The <strong>hash</strong> variant does not require an up-front sort of the <code>COLLECT</code> input, and will thus be
preferred over the <strong>sorted</strong> method if the optimizer estimates many input elements for the <code>COLLECT</code>
and cannot use an index to process them in already sorted order. In this case, the optimizer
will estimate that post-sorting the result of the <strong>hash</strong> <code>COLLECT</code> will be more efficient than
pre-sorting the input for the <strong>sorted</strong> <code>COLLECT</code>.</p>

<p>The main assumption behind this estimation is that the result of any <code>COLLECT</code> statement will
contain at most as many elements as there are input elements to it. Therefore, the output of
a <code>COLLECT</code> is likely to be smaller (in terms of rows) than its input, making post-sorting more
efficient than pre-sorting.</p>

<p>If there is a sorted index on the <code>COLLECT</code> group criteria that the optimizer can exploit, the
optimizer will pick the <strong>sorted</strong> method because thanks to the index it can optimize away the
pre-<code>COLLECT</code> sort, leaving no sorts left in the final execution plan.</p>

<p>To override the optimizer decision, <code>COLLECT</code> statements now have an <code>OPTIONS</code> modifier. This
modifier can be used to force the optimizer to use the <strong>sorted</strong> variant:</p>

<p><code>plain forcing the use of the sorted variant
FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count OPTIONS { method: "sorted" }
  RETURN { from: from, count: count }
</code></p>

<p>Note that specifying <strong>hash</strong> in <code>method</code> will not force the optimizer to use the <strong>hash</strong> method.
The reason is that the <strong>hash</strong> variant cannot be used for all queries (only <code>COLLECT</code> statements
without an <code>INTO</code> clause are eligible). If <code>OPTIONS</code> are omitted or any other method than <code>sorted</code>
is specified, the optimizer will ignore it and use its regular cost estimations.</p>

<h2>Understanding execution plans</h2>

<p>Which method is actually used in a query can found out by explaining it and looking at its
execution plan.</p>

<p>A <code>COLLECT</code> is internally handled by an object called <code>AggregateNode</code>, so we have to look for that.
In the above screenshots, the <code>AggregateNode</code>s are tagged with either <strong>hash</strong> or <strong>sorted</strong>. This can
also be checked programatically by looking at the <code>aggregationOptions.method</code> attributes in the
JSON result of an explain().</p>

<p>Here is some example code to extract this information, limited to the <code>AggregateNode</code>s of the
query already:</p>

<p><code>js extracting just the AggregateNodes from an explain
var query = `
  FOR flight IN flights
  COLLECT from = flight._from WITH COUNT INTO count
  RETURN { from: from, count: count }
`;
var stmt = db._createStatement(query);
var plan = stmt.explain().plan;
plan.nodes.filter(function(node) {
  return node.type === 'AggregateNode';
});
</code></p>

<p>For the above query, this will produce something like this:</p>

<p>```json JSON explain result for AggregateNode
[
  {</p>

<pre><code>"type" : "AggregateNode", 
...
"aggregationOptions" : { 
  "method" : "hash" 
}  
</code></pre>

<p>  }
]
```</p>

<p>Here we can see that the query is using the <strong>hash</strong> method.</p>

<h2>Optimizing away post-COLLECT sorts</h2>

<p>If a query uses the <strong>hash</strong> method for a <code>COLLECT</code> but the sort order of the <code>COLLECT</code> result
is irrelevant to the user, the user can provide a hint to the optimizer to remove the
post-<code>COLLECT</code> sort.</p>

<p>This can be achieved by simplying appending a <code>SORT null</code> to the original <code>COLLECT</code> statement.
Here we can see that this removes the post-<code>COLLECT</code> sort:</p>

<p><img src="/downloads/screenshots/collect-nosort.png"></p>

<h2>Performance improvements</h2>

<p>The improvements achievable by using the <strong>hash</strong> method instead of the <strong>sorted</strong> method obviously
depend on whether there are appropriate indexes present for the group criteria. If an index can
be exploited, the <strong>sorted</strong> method may be just fine. However, there are cases when no indexes are
present, for example, when running arbitrary ad-hoc queries or when indexes are too expensive
(indexes need to be updated on insert/update/remove and also will use memory).</p>

<p>Following are a few comparisons of the <strong>sorted</strong> and the <strong>hash</strong> methods in case no indexes can be
used.</p>

<p>Here&rsquo;s the setup for the test data. This generates 1M documents with both unique and repeating
string and numeric values. For the non-unique values, we&rsquo;ll use 20 different categories:</p>

<p>```js setting up test data
var test = db._create(&ldquo;test&rdquo;);
for (var i = 0; i &lt; 1000000; ++i) {
  test.insert({</p>

<pre><code>uniqueNumber: i, 
uniqueString: String("test" + i), 
repeatingNumber: (i % 20), 
repeatingString: String("test" + (i % 20)) 
</code></pre>

<p>  });
}
```</p>

<p>Now let&rsquo;s run the following query on the data and measure its execution time:</p>

<p><code>plain test query
FOR v IN test
  COLLECT value = v.@attribute WITH COUNT INTO count
  RETURN { value: value, count: count }
</code></p>

<p>The worst case is when the <code>COLLECT</code> will produce as many output rows as there are input
rows. This will happen when using a unique attribute as the grouping criterion. We&rsquo;ll run
tests on both numeric and string values.</p>

<p>Here are the execution times for unique inputs. It can be seen that the <strong>hash</strong> method
here will be beneficial if the post-<code>COLLECT</code> sort can be optimized away. As demonstrated
above, this can be achieved by adding an extra <code>SORT null</code> after the <code>COLLECT</code> statement.
If the post-<code>COLLECT</code> sort is not optimized away, it will make the hash method a bit more
expensive than the <strong>sorted</strong> method:</p>

<p>```plain COLLECT performance with unique inputs</p>

<h2>collect method       @attribute                duration</h2>

<p>sorted               uniqueNumber               11.92 s
hash                 uniqueNumber               13.40 s
hash (sort null)     uniqueNumber               10.13 s
sorted               uniqueString               22.04 s
hash                 uniqueString               27.35 s
hash (sort null)     uniqueString               12.12 s
```</p>

<p>Now let&rsquo;s check the results when we group on an attribute that is non-unique. Following
are the results for numeric and string attributes with 20 different categories each:</p>

<p>```plain COLLECT performance with non-unique inputs</p>

<h2>collect method       @attribute                duration</h2>

<p>sorted               repeatingNumber             5.56 s
hash                 repeatingNumber             0.94 s
hash (sort null)     repeatingNumber             0.94 s
sorted               repeatingString            10.56 s
hash                 repeatingString             1.09 s
hash (sort null)     repeatingString             1.09 s
```</p>

<p>In these cases, the result of the <code>COLLECT</code> will be much smaller than its input (we&rsquo;ll
only get 20 result rows out instead of 1M). Therefore the post-<code>COLLECT</code> sort for the <strong>hash</strong>
method will not make any difference, but the pre-<code>COLLECT</code> sort for the <strong>sorted</strong> method
will still need to sort 1M input values. This is also the reason why the <strong>hash</strong> method
is significantly faster here.</p>

<p>As usual, your mileage may vary, so please run your own tests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updating Documents With Arangoimp]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/14/updating-documents-with-arangoimp/"/>
    <updated>2015-04-14T14:55:45+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/14/updating-documents-with-arangoimp</id>
    <content type="html"><![CDATA[<p>Inspired by the feature request in <a href="https://github.com/arangodb/arangodb/issues/1298">Github issue #1298</a>,
we added update and replace support for ArangoDB&rsquo;s import facilities.</p>

<p>This extends ArangoDB&rsquo;s HTTP REST API for importing documents plus the arangoimp binary so they
can not only insert new documents but also update existing ones.</p>

<p>Inserts and updates can also be mixed in a single import run.
This blog post provides a few usage examples.</p>

<!-- more -->


<h2>Traditional import</h2>

<p>Previously, the HTTP REST API for importing documents and the arangoimp binary only supported
document inserts, so they could not be used to update existing documents. Bulk-updating existing
documents with data from a file or mixing inserts with updates required to write custom scripts
or run multiple commands or queries.</p>

<p>I won&rsquo;t show this in detail but want to concentrate solely on what the import did. I will only
show arangoimp and not the HTTP import API.</p>

<p>Let&rsquo;s assume there is already a collection named <em>users</em> containing the following documents:</p>

<p><code>json data in collection before import
{ "_key" : "user1", "name" : "John Doe" }
{ "_key" : "user2", "name" : "Jane Smith" }
</code></p>

<p>Now, importing the following data via arangoimp would produce errors for line 1 and 2 (i.e.
for keys <code>user1</code> and <code>user2</code>) because these documents already exist in the target collection:</p>

<p><code>json data to be imported
{ "_key" : "user1", "country" : "AU" }
{ "_key" : "user2", "country" : "UK" }
{ "_key" : "user3", "name" : "Joe Public", "country" : "ZA" }
</code></p>

<p>Here&rsquo;s what happened when importing the above data into the collection with the two existing
documents:</p>

<pre><code>&gt; arangoimp --file data.json --collection users

2015-04-14T18:23:32Z [27441] WARNING at position 1: creating document failed with error 'unique constraint violated', offending document: {"_key":"user1","country":"AU"}
2015-04-14T18:23:32Z [27441] WARNING at position 2: creating document failed with error 'unique constraint violated', offending document: {"_key":"user2","country":"UK"}

created:          1
warnings/errors:  2
</code></pre>

<p>After the traditional import, the collection contained the following documents:</p>

<p><code>json collection contents after traditional import
{ "_key" : "user1", "name" : "John Doe" }
{ "_key" : "user2", "name" : "Jane Smith" }
{ "_key" : "user3", "country" : "ZA", "name" : "Joe Public" }
</code></p>

<p>As can be seen, the first two documents (<code>user1</code> and <code>user2</code>) remain unmodified, and the third
document (<code>user3</code>) was inserted because it did not yet exist in the target collection.</p>

<h2>Using &mdash;on-duplicate</h2>

<p>So what&rsquo;s the change?</p>

<p>As announced, a single import run can now both insert new documents and update existing ones.
What exactly will happen is configurable by setting arangoimp&rsquo;s new command-line option
<code>--on-duplicate</code>.</p>

<p>By default, even in <code>devel</code> there will be errors reported for the two already existing documents.</p>

<p>Good news is that this behavior can be changed by setting <code>--on-duplicate</code> to a value of <code>update</code>,
<code>replace</code> or <code>ignore</code>:</p>

<ul>
<li><p><code>error</code>: if a document with the specified <code>_key</code> already exists in the target collection, the
import will not modify it and instead return an error. This is the default behavior and
compatible with all previous versions of ArangoDB.</p>

<p>We have seen the result above in the <em>traditional import</em>.</p></li>
<li><p><code>update</code>: if a document with the specified <code>_key</code> already exists in the target collection, the
import will (partially) update the existing document with the specified attributes.
Only the attributes present in the import data will be updated, and all other attributes of
the document present in the collection will be preserved.</p>

<pre><code>&gt; arangoimp --file data.json --collection users --on-duplicate update

created:          1
warnings/errors:  0
updated/replaced: 2
ignored:          0
</code></pre>

<p>The first two documents (<code>user1</code> and <code>user2</code>) were updated (attribute <code>country</code>
was added) and the third document (<code>user3</code>) was inserted because it did not exist in the
target collection:</p>

<pre><code>{ "_key" : "user1", "country" : "AU", "name" : "John Doe" }
{ "_key" : "user2", "country" : "UK", "name" : "Jane Smith" } 
{ "_key" : "user3", "country" : "ZA", "name" : "Joe Public" } 
</code></pre></li>
<li><p><code>replace</code>: if a document with the specified <code>_key</code> already exists in the target collection, the
import will fully replace the existing document with the specified attributes.
Only the attributes present in the import data will be preserved, and all other attributes of
the document present in the collection will be removed.</p>

<pre><code>&gt; arangoimp --file data.json --collection users --on-duplicate replace

created:          1
warnings/errors:  0
updated/replaced: 2
ignored:          0
</code></pre>

<p>The first two documents (<code>user1</code> and <code>user2</code>) were replaced (attribute <code>country</code> was present
in the import data, previously existing attribute <code>name</code> was removed). The third document
(<code>user3</code>) was inserted because it did not exist in the target collection before:</p>

<pre><code>{ "_key" : "user1", "country" : "AU" } 
{ "_key" : "user2", "country" : "UK" } 
{ "_key" : "user3", "country" : "ZA", "name" : "Joe Public" } 
</code></pre></li>
<li><p><code>ignore</code>: if a document with the specified <code>_key</code> already exists in the target collection, the
import will ignore and not modify it. The difference to <code>error</code> is that ignored documents will
not be counted as errors. No errors/warnings will be reported for duplicate <code>_key</code> values, but
the number of duplicate key occurrences will be reported in the <code>ignored</code> attribute</p>

<pre><code>&gt; arangoimp --file data.json --collection users --on-duplicate ignore

created:          1
warnings/errors:  0
updated/replaced: 0
ignored:          2
</code></pre>

<p>Collection contents are the same as in the <code>error</code> case.</p></li>
</ul>


<p>The above examples were for the arangoimp import binary, but the HTTP import API was adjusted
as well. The duplicate key behavior can be controlled there by using the new <code>onDuplicate</code> URL
parameter. Possible values are also <code>error</code>, <code>update</code>, <code>replace</code> and <code>ignore</code> as shown for arangoimp.</p>

<h2>Caveats</h2>

<p>All matching is done using document keys (i.e. <code>_key</code> attributes) and no other attributes. That
means existing documents can only be updated if their <code>_key</code> attributes are present in the import
data. When no <code>_key</code> attribute is present for a document in the import data, the import will try
to insert a new document.</p>

<p>The extended functionality is available in the <code>devel</code> branch, which will eventually turn into
a stable 2.6 release.</p>

<p><strong>Enjoy!</strong></p>
]]></content>
  </entry>
  
</feed>
