<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-04-05T01:10:27+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[More Efficient Data Exports]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/04/more-efficient-data-exports/"/>
    <updated>2015-04-04T21:51:33+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/04/more-efficient-data-exports</id>
    <content type="html"><![CDATA[<p>I recently wrote about the <a href="https://jsteemann.github.io/blog/2015/04/01/improvements-for-the-cursor-api/">performance improvements for the cursor API</a>
made in ArangoDB 2.6. The performance improvements are due to a rewrite of the cursor API&rsquo;s internals.</p>

<p>As a byproduct of this rewrite, an extra API was created for exporting all documents from a
collection to a client application. With this being its only use case, it is clear that the new
API will not solve every data export problem. However, the API&rsquo;s limitedness facilitated a very efficient
implementation, resulting in <strong>nice speedups</strong> and <strong>lower memory usage</strong> when compared to the alternative
way of exporting all documents into a client application.</p>

<!-- more -->


<p>There did not exist an <em>official</em> export API before. So users often ran AQL queries like the following
to export all documents from a collection:</p>

<p><code>plain AQL query to export all documents
FOR doc IN collection
  RETURN doc
</code></p>

<p>While such AQL queries will work for smaller result sets, they will get problematic when results get
bigger. The reason is that the AQL very will effectively create a snapshot of all the documents present
in the collection. Creating the snapshot is required for data consistency. Once the snapshot is created,
clients can incrementally fetch the data from the snapshot and will still get a consistent result even
if the underlying collections get modified.</p>

<p>For smaller result sets, snapshotting is not a big issue. But when exporting all documents from a
bigger collection, big result sets will be produced. In this case, the snapshotting can become expensive
in terms of CPU time and also memory consumption.</p>

<p>We couldn&rsquo;t get around the snapshotting completely, but we could take advantage of the fact that when exporting
documents from a collection, all that can be snapshotted are documents. This is different to snapshotting
arbitrary AQL queries, which can produce any kind and combination of JSON.</p>

<p>Dealing only with documents allowed us to take an efficiency shortcut: instead of copying the complete
documents it will only copy pointers to the document revisions presently in th collection. Not only is this
much faster than doing a full copy of the document, but it also saves a lot of memory.</p>

<h2>Invoking the API</h2>

<p>While the invocation of the cursor API and the export API is slightly different, their result formats
have intentionally been kept similar. This way client programs do not need to be adjusted much to consume
the export API instead of the cursor API.</p>

<p>An example command for exporting via the cursor API is:</p>

<p>```bash exporting all documents via the cursor API
curl -X POST \</p>

<pre><code> "http://127.0.0.1:8529/_api/cursor" \
 --data '{"query":"FOR doc IN collection RETURN docs"}'
</code></pre>

<p>```</p>

<p>A command for exporting via the new export API is:</p>

<p>```bash exporting all documents via the export API
curl -X POST \</p>

<pre><code> "http://127.0.0.1:8529/_api/export?collection=docs"
</code></pre>

<p>```</p>

<p>In both cases, the result will look like this:
```json API results
{
  &ldquo;result&rdquo;: [</p>

<pre><code>...
</code></pre>

<p>  ],
  &ldquo;hasMore&rdquo;:true,
  &ldquo;id&rdquo;:&ldquo;2221050516478&rdquo;
}
```</p>

<p>The <code>result</code> attribute will contain the first few (1,000 by default) documents. The
<code>hasMore</code> attribute will indicate whether there are more documents to fetch from the
server. In this case the client can use the cursor id specified in the <code>id</code> attribute
to fetch more result.</p>

<p>The API can be invoked via any HTTP-capable client such as <code>curl</code> (as shown above).</p>

<p>I have also added <a href="https://github.com/arangodb/arangodb-php/blob/devel/README.md#exporting_data">bindings to the ArangoDB-PHP driver</a>
today (contained in the driver&rsquo;s <code>devel</code> branch).</p>

<h2>API performance</h2>

<p>Now, what can be gained by using the export API?</p>

<p>The following table shows the execution times for fetching the first 1,000 documents
from collections of different sizes, both with via the cursor API and the export API.
Figures for the cursor API are shown for ArangoDB 2.5 and 2.6 (the version in which
it was rewritten):</p>

<p>```plain execution times for cursor API and export API</p>

<h1>of documents    cursor API (2.5)    cursor API (2.6)      export API</h1>

<hr />

<pre><code>   100,000               1.9 s               0.3 s          0.04 s
   500,000               9.5 s               1,4 s          0.08 s
 1,000,000              19.0 s               2.8 s          0.14 s
 2,000,000              39,0 s               7.5 s          0.19 s
 5,000,000               n/a                 n/a            0.55 s
10,000,000               n/a                 n/a            1.32 s
</code></pre>

<p>```</p>

<p>Execution times are from my laptop, which only has 4 GB of RAM and a slow disk.</p>

<p>As can be seen, the rewritten cursor API in 2.6 is already much faster than the one
in 2.5. However, for exporting documents from one collection only, the new export API
is superior.</p>

<p>The export API also uses a lot less memory for snapshotting, as can be nicely seen in the
two bottom rows of the results. For these cases, the snapshots done by the cursor API
were bigger than the available RAM and the OS started swapping heavily. Snapshotting
didn&rsquo;t complete within 15 minutes, so no results are shown above.</p>

<p>Good news is that this didn&rsquo;t happen with the export API, due to the fact that the
snapshots it creates are much more compact.</p>

<p>Another nice side effect of the speedup is that the first results will arrive much
earlier in the client application. This will help in reducing client connection timeouts
in case clients are enforcing them on temporarily non-responding connections.</p>

<h2>Summary</h2>

<p>ArangoDB 2.6 provides a specialized export API for exporting all documents from a
collection and shipping them to a client application. It is rather limited but
faster than the general-purpose AQL cursor API and can store its snapshots using less
memory.</p>

<p>Therefore, exporting all documents from bigger collections calls for using the new
export API from 2.6 on. The new export API is present in the <code>devel</code> branch, which
will eventually turn into a 2.6 release.</p>

<p>For other cases, when still using the cursor API, 2.6 will also provide significant
performance improvements when compared to 2.5. This can be seen from the comparison
table above and also from the observations made
<a href="https://jsteemann.github.io/blog/2015/04/01/improvements-for-the-cursor-api/">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improvements for the Cursor API]]></title>
    <link href="http://jsteemann.github.io/blog/2015/04/01/improvements-for-the-cursor-api/"/>
    <updated>2015-04-01T13:59:22+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/04/01/improvements-for-the-cursor-api</id>
    <content type="html"><![CDATA[<p>This week we pushed some modifications for ArangoDB&rsquo;s cursor API into the <code>devel</code> branch.
The change will result in less copying of AQL query results between the AQL and the HTTP layers.
As a positive side effect, this will reduce the amount of garbage collection the built-in V8
has to do.</p>

<p>These modifications should improve the cursor API performance significantly for many cases,
while at the same time keeping its REST API stable.</p>

<p>This blog post shows some first unscientific performance tests comparing the old cursor API with
its new, improved implementation.</p>

<!-- more -->


<p>A good way to test the cursor API performance is to issue lots of queries from the
ArangoShell. The ArangoShell will send the query to the server for execution. The server
will respond with the first 1,000 results for the query.</p>

<p>Additionally the server will create a server-side cursor if the result set is bigger than
1,000 documents. In this case, the ArangoShell will issue subsequent HTTP requests that fetch
the outstanding documents from the server.</p>

<p>The above behavior is triggered automatically when <code>db._query(query).toArray()</code> is run in
the ArangoShell.</p>

<p>Here is a test function that executes a query <em>n</em> times and measures the total execution time.
It will issue <em>n</em> HTTP requests to the server&rsquo;s cursor API for executing the query. It will
also issue further HTTP requests if the total result set size is bigger than 1,000 documents.
What is getting measured is thus the total execution time from the ArangoShell&rsquo;s point of view,
including time spent in the server-side cursor functions as well as in HTTP traffic.</p>

<p>```js function for testing the cursor API
var test = function(query, n) {
  var time = require(&ldquo;internal&rdquo;).time;
  var s = time();
  for (var i = 0; i &lt; n; ++i) {</p>

<pre><code>db._query(query).toArray(); 
</code></pre>

<p>  }
  return time() &ndash; s;
};
```</p>

<p>The test function was run with different queries to check which types of queries will benefit
from the cursor API change.</p>

<p>Note that the ArangoShell will issue all its requests to the cursor API sequentially. This is
ok for the purpose of this test, as the purpose was to measure the relative performance change
between the old and the new API implementation.</p>

<p>The ArangoShell and ArangoDB server were running on the same physical machine during the tests,
so this is a <strong>localhost</strong> benchmark.</p>

<h2>Detailed test results</h2>

<p>Here are the results from my local machine.</p>

<p>The first query was about the simplest one I could come up with. The query was sent to the
server 10,000 times. The result set size per query ws 1, resulting in 10,000 calls to the cursor
API with not much data to be transferred per call:</p>

<p><code>js test query
test("RETURN 1", 10000);
</code></p>

<p>Execution took 7.225 s with the old API, and 5.195 s with the new API (<strong>28 % improvement</strong>).</p>

<p>A query returning a slightly more complex result value:</p>

<p><code>js test query
test("RETURN { one: 'test', two: 'another-value', three: [ 1, 2, 3 ] }", 10000);
</code></p>

<p>This took 8.046 s with the old API, and 5.829 s with the new one (<strong>27 % improvement</strong>).</p>

<p>Another simple query, again executed 10,000 times, but now returning 10 values per query:</p>

<p><code>js test query
test("FOR i IN 1..10 RETURN i", 10000);
</code></p>

<p>Execution of this query took 7.951 s with the old, and 5.779 s with the new API (<strong>27 % improvement</strong>).</p>

<p>Now raising the number of return values per query from 10 to 1,000:</p>

<p><code>js test query
test("FOR i IN 1..1000 RETURN i", 10000);
</code></p>

<p>This took 31.650 s with the old, and 28.504 s with the new API (<strong>10 % improvement</strong>).</p>

<p>So far all query results contained 1,000 or less values. In this case the server is able to
send the whole query result in response in one go, so there were only as many calls to the
cursor API as there were queries. Even though the ArangoShell called the cursor API, the
cursor only existed temporarily on the server but directly vanished when the server sent its
response.</p>

<p>Now let&rsquo;s run a query that returns more than 1,000 values each. The first call to the
cursor API will then only return the first 1,000 results and additionally establish a
server-side cursor so the client can fetch more results. This will mean that for each client
query, there will be multiple HTTP requests.</p>

<p>The following run issues 100,000 calls to the cursor API (10,000 queries times 10 batches per
query):</p>

<p><code>js test query
test("FOR i IN 1..10000 RETURN i", 10000);
</code></p>

<p>This took 307.108 s with the old API, in contrast to 232.322 s with the new API (<strong>24 % improvement</strong>).</p>

<p>The next queries I tested were collection-based. They returned data from a collection named
<code>docs</code>. The collection contained 10,000 documents, and each document in the collection had
5 attributes.</p>

<p>The first query returned only a single one (random) document from the collection per query.</p>

<p><code>js test query
test("FOR i IN docs LIMIT 1 RETURN i", 10000);
</code></p>

<p>This took 8.689 s with the old API and 6.245 s with the new API (<strong>28 % improvement</strong>).</p>

<p>The next query returned all the documents from the collection. The query was executed
only 1,000 times because the result sets already got quite big. The combined size of all
result sets was 1,000,000 documents (10,000 documents, 1,000 queries).</p>

<p><code>js test query
test("FOR i IN docs RETURN i", 1000);
</code></p>

<p>This took 453.736 s with the old, and 197.543 s with the new API (<strong>56 % improvement</strong>).</p>

<p>The final query returned all document keys from the collection. The combined size of all result
sets was 10,000,000 values (10,000 documents, 10,000 queries):</p>

<p><code>js test query
test("FOR i IN docs RETURN i._key", 10000);
</code></p>

<p>With the old API, this took 529.765 s, and with the new API it took 348.243 s (<strong>34 % improvement</strong>).</p>

<h2>Summary</h2>

<p>The new cursor API was faster than its old counterpart for all queries tested here. Total execution
time as measured by the ArangoShell (representative for any other client program sending queries to
ArangoDB) was consistenly lower than it was with the old API implementation.</p>

<p>The improvements measured were varying. For the queries tested, the improvements fell into a range
of 10 % to even more than 50 % improvement.</p>

<p>How much gain can be achieved in reality obviously depends on the type of query executed. There will
also be queries that do not benefit from the new API implementation. For example, queries that do not
return any results will not benefit much. This is because most of the optimizations done affect
the buffering and the data transport internals of the cursor API. Furthermore, queries that run for
a very long time but return only small amounts of data may not benefit considerably for the same reason.
However, there should not be any queries which are negatively affected by the change.</p>

<p>All in all, this looks quite promising, especially as the change will come <strong>for free</strong> for client
applications. Client programs do not need to be adjusted to reap the benefits. This is because all
that has changed were the <em>internals</em> of the cursor API. Its public REST interface remains unchanged.</p>

<p>The changes are included in the <code>devel</code> branch and can be tested there.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improvements for Data-modification Queries]]></title>
    <link href="http://jsteemann.github.io/blog/2015/03/27/improvements-for-data-modification-queries/"/>
    <updated>2015-03-27T23:29:19+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/03/27/improvements-for-data-modification-queries</id>
    <content type="html"><![CDATA[<p>Data-modification queries were enhanced in ArangoDB 2.4 to be able to also return
the inserted, update or removed documents.</p>

<p>For example, the following statement inserted a few documents and also returned
them with all their attributes:</p>

<p><code>plain AQL insert query returning documents
FOR i IN 1..10
  INSERT { value: i } IN test
  LET inserted = NEW
  RETURN inserted
</code></p>

<p>The syntax for returning documents from data-modification queries only supported
the exact above format. Using a <code>LET</code> clause was required, and the <code>RETURN</code> clause
was limited to returning the variable introduced by the <code>LET</code>.</p>

<p>These syntax restrictions have been lifted in the <code>devel</code> branch, which will become
release 2.6 eventually. The changes make returning values from data-modification
statements easier and also more flexible.</p>

<!-- more -->


<h2>Simpler syntax</h2>

<p>For example, specifying a <code>LET</code> clause is not required anymore (though still fully
supported). Instead, the <code>RETURN</code> clause can directly refer to the <code>NEW</code> pseudo-value,
making the query shorter and easier to write:</p>

<p><code>plain AQL insert query returning documents
FOR i IN 1..10
  INSERT { value: i } IN test
  RETURN NEW
</code></p>

<h2>Projections</h2>

<p>It is now also possible to return a projection instead of returning the entire documents.
This can be used to reduce the amount of data returned by queries.</p>

<p>For example, the following query will return just the keys of the inserted documents:</p>

<p><code>plain AQL insert query returning a projection
FOR i IN 1..10
  INSERT { value: i } IN test
  RETURN NEW._key
</code></p>

<h2>Using OLD and NEW in the same query</h2>

<p>In previous versions, <code>UPDATE</code> and <code>REPLACE</code> statements could refer to <strong>either</strong>
the <code>OLD</code> or the <code>NEW</code> pseudo-value, but not to both. 2.6 lifts that restriction, so
now these queries can refer to both. One can utilize that to return both the previous
and the updated revision:</p>

<p><code>plain AQL update query returning old and new revisions
FOR doc IN test
  UPDATE doc WITH { value: 42 } IN test
  RETURN { old: OLD, new: NEW }
</code></p>

<h2>Calculations with OLD or NEW</h2>

<p>It is now also possible to run additional calculations with <code>LET</code> statements between
the data-modification part and the final <code>RETURN</code>:</p>

<p><code>plain AQL upsert query with some extra calculations
UPSERT { name: 'test' } INSERT { name: 'test' } UPDATE { } IN test
LET previousRevisionExisted = ! IS_NULL(OLD)
LET type = previousRevisionExisted ? 'update' : 'insert'
RETURN { _key: NEW._key, type: type }
</code></p>

<h2>Restrictions</h2>

<p>Still the following restrictions remain:</p>

<ul>
<li><p>a data-modification operation can optionally be followed by any number of <code>LET</code> clauses,
and a final <code>RETURN</code> clause. No other operations (e.g. <code>FOR</code>, <code>SORT</code>, <code>COLLECT</code>) can be
used after a data-modification operation</p></li>
<li><p>calculations following a data-modification operation must not access data in collections,
so using functions such as <code>GRAPH_TRAVERSAL</code> etc. is disallowed.</p></li>
</ul>


<p>The improvements are present in the <code>devel</code> branch and can be tested in there from now on.
As usual, feedback is welcome!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Preview of the UPSERT Command]]></title>
    <link href="http://jsteemann.github.io/blog/2015/03/27/preview-of-the-upsert-command/"/>
    <updated>2015-03-27T21:37:09+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/03/27/preview-of-the-upsert-command</id>
    <content type="html"><![CDATA[<p>This week saw the completion of the AQL <code>UPSERT</code> command.</p>

<p>This command will be very helpful in a lot of use cases, including the following:</p>

<ul>
<li>ensure that a document exists</li>
<li>update a document if it exists, otherwise create it</li>
<li>replace a document if it exists, otherwise create it</li>
</ul>


<p>The <code>UPSERT</code> command is executed on the server side and so delivers client
applications from issuing a fetch command followed by a separate, conditional <code>UPDATE</code>
or <code>INSERT</code> command.</p>

<!-- more -->


<p>The general format of an <code>UPSERT</code> statement is:</p>

<p><code>plain UPSERT format
UPSERT search-document
INSERT insert-expression
UPDATE update-expression
IN collection-name
</code></p>

<p>Following are a few example invocations of <code>UPSERT</code>.</p>

<h2>Ensure a document exists</h2>

<p>A simple use case of <code>UPSERT</code> is to ensure that a specific document exists.
For example, the following query will ensure that that there will be a document
with attribute <code>ip</code> equal to <code>192.168.173.13</code> in collection <code>hosts</code>:</p>

<p><code>plain ensuring a document exists
UPSERT { ip: '192.168.173.13' }
INSERT { ip: '192.168.173.13', name: 'flittard' }
UPDATE { }
IN hosts
</code></p>

<p>If the document does not yet exist, the <code>INSERT</code> part will be carried out. If the
document is already there, the empty <code>UPDATE</code> part will be run, which will not
modify the document.</p>

<p>After the query has finished, there will be a document with the specified <code>ip</code> value.</p>

<p>There is no need for client applications to check for document existence first,
and then to conditionally insert or update, or to try insert first and catch errors.</p>

<p>Note: this is the same as ActiveRecord&rsquo;s <code>find_or_create</code>.</p>

<h2>Update a document if it exists, otherwise create it</h2>

<p>Another common use case is to check whether a specific document exists, and then update it.
If it does not yet exist, the document shall be created.</p>

<p>Counters are a good example for this, so let&rsquo;s demo this pattern with a counter, too:</p>

<p><code>plain UPSERT example
UPSERT { ip: '192.168.173.13' }
INSERT { ip: '192.168.173.13', name: 'flittard', counter: 1 }
UPDATE { counter : OLD.counter + 1 }
IN hosts
</code></p>

<p>The above query will again look for a document with the specified <code>ip</code> attribute. If the
document exists, its <code>counter</code> attribute will be increased by one. This is achieved by
referring to the pseudo-value <code>OLD</code>, which in the <code>UPDATE</code> case contains the previous revision
of the document.</p>

<p>If the search document does not yet exist, the <code>INSERT</code>part will be carried out, inserting
the document and setting the initial value of <code>counter</code> to 1.</p>

<p>Assuming the collection was empty before, running the above query once will make the collection
contain this data:</p>

<p>```json collection contents after running query once
[
  {</p>

<pre><code>"counter": 1,
"ip": "192.168.173.13",
"name": "flittard"
</code></pre>

<p>  }
]
```</p>

<p>When running the <code>UPSERT</code> statement again, the collection will contain the updated document:</p>

<p>```json collection contents after running the UPSERT command again:
[
  {</p>

<pre><code>"counter": 2,
"ip": "192.168.173.13",
"name": "flittard"
</code></pre>

<p>  }
]
```</p>

<p>Now let&rsquo;s run the query with adjusted <code>ip</code> and <code>name</code> values:</p>

<p><code>plain UPSERT with different ip and name
UPSERT { ip: '192.168.173.73' }
INSERT { ip: '192.168.173.73', name: 'brueck', counter: 1 }
UPDATE { counter : OLD.counter + 1 }
IN hosts
</code></p>

<p>After that, the collection will contain two documents:</p>

<p>```json collection contents
[
  {</p>

<pre><code>"counter": 2,
"ip": "192.168.173.13",
"name": "flittard"
</code></pre>

<p>  },
  {</p>

<pre><code>"counter": 1,
"name": "brueck",
"ip": "192.168.173.73"
</code></pre>

<p>  }
]
```</p>

<h2>Replace a document if it exists, otherwise create it</h2>

<p>We&rsquo;ve seen <code>UPSERT</code> with an <code>INSERT</code> and an <code>UPDATE</code> clause so far.</p>

<p><code>UPDATE</code> will partially update the previous revision of the document if present.
Only those attributes specified in the <code>update-expression</code> will be updated, and
all non-specified attributes of the original document revision will remain
unchanged.</p>

<p>If instead a full replacement of the original document is required, the <code>REPLACE</code>
clause should be used instead of <code>UPDATE</code>. <code>REPLACE</code> will overwrite the previous
revision completely with what&rsquo;s in <code>update-expression</code>.</p>

<p><code>plain UPSERT replacing a document entirely
UPSERT { ip: '192.168.173.73' }
INSERT { ip: '192.168.173.73', name: 'brueck', counter: 1 }
REPLACE { location: 'dc1' }
IN hosts
</code></p>

<p><em>note</em>: an older version of this blog post contained a wrong example here. Thanks
Andy for pointing this out!</p>

<h2>Returning documents</h2>

<p><code>UPSERT</code> can be combined with a <code>RETURN</code> statement to return either the previous
document revision (in case of <code>UPDATE</code> or <code>REPLACE</code>) or the new version of the
document.</p>

<p>Client applications can use this to check whether the <code>UPSERT</code> statement has
inserted or updated the document. In case no previous revision was present, the
pseudo-value <code>OLD</code> will be <code>null</code>.</p>

<p><code>UPSERT</code> also provides a pseudo-value named <code>NEW</code> containing the insert, updated or
replaced version of the document:</p>

<p><code>plain UPSERT with a RETURN value
UPSERT { ip: '192.168.173.187' }
INSERT { ip: '192.168.173.187', name: 'kalk', counter: 1 }
UPDATE { counter : OLD.counter + 1 }
IN hosts
RETURN { old: OLD, new: NEW }
</code></p>

<p>In the <code>INSERT</code> case, we&rsquo;ll get:</p>

<p>```json query return value for INSERT case
[
  {</p>

<pre><code>"old": null,
"new": {
  "counter": 1,
  "name": "kalk",
  "ip": "192.168.173.187"
}
</code></pre>

<p>  }
]
```</p>

<p>When running the query again, we&rsquo;ll get into the <code>UPDATE</code> case, and the same query
will now return:</p>

<p>```json query return value for the UPDATE case
[
  {</p>

<pre><code>"old": {
  "counter": 1,
  "name": "kalk",
  "ip": "192.168.173.187"
},
"new": {
  "counter": 2,
  "name": "kalk",
  "ip": "192.168.173.187"
}
</code></pre>

<p>  }
]
```</p>

<h2>Complex updates</h2>

<p>Updating and returning <code>OLD</code> and <code>NEW</code> will work with arbitrary calculations.
For example, the following query adds a value <code>development</code> to the <code>tag</code> attribute
only if not yet present in the search document:</p>

<p><code>plain adding a value to an array if not yet present
UPSERT { ip: '192.168.173.94' }                                                                              
INSERT { ip: '192.168.173.94', name: 'chorweiler', tags: [ "development" ] }                                                    
UPDATE { tags: PUSH(OLD.tags, "development", true) }                                                                          
IN hosts                                                                                                      
RETURN { old: OLD, new: NEW }
</code></p>

<p>Running the query multiple times will ensure that <code>tags</code> will contain the value
<code>development</code> only once.</p>

<p>Note: <code>PUSH</code> is a regular <a href="https://docs.arangodb.com/Aql/ArrayFunctions.html">AQL array function</a>.</p>

<h2>Restrictions</h2>

<p>The <code>search-value</code> needs to be an object literal, with attribute names being inspectable
at query compile time. This means that neither variables nor bind parameters can be used
for <code>search-value</code>.</p>

<p>However, bind parameters and variables can be used <strong>inside</strong> <code>search-value</code>.
Dynamic attribute names cannot be used for specifying attribute names in search-value`.</p>

<p><code>UPSERT</code> does not require an index to be present on the attributes of <code>search-value</code>,
but in reality queries will benefit from indexes to find matching documents.</p>

<p>When more than one document in the collection matches <code>search-value</code>, one arbitrary match
will be used for executing the <code>UPDATE</code> clause. It is therefore recommended to use
<code>UPSERT</code> commands together with a unique index or to make sure from the client application
that at most one document will match the <code>search-value</code>. Ironically, one way to achieve this is
to use the <code>UPSERT</code> command for inserts&hellip;</p>

<h2>Availability</h2>

<p><code>UPSERT</code> is currently available in the <code>devel</code> branch of ArangoDB. This branch
will eventually become release 2.6. Until then, everyone is welcome to try it out and
provide feedback.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyzing Git Commits With ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2015/03/11/analyzing-git-commits-with-arangodb/"/>
    <updated>2015-03-11T12:37:58+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/03/11/analyzing-git-commits-with-arangodb</id>
    <content type="html"><![CDATA[<p>I often find myself searching for certain commits using <code>git log</code> and friends. While I really love
the power and flexibility that come with the git and other Unix command-line tools, sometimes it can be more
convenient to use a database to filter and aggregate commit data.</p>

<p>I gave it a quick try yesterday and imported the commit history of ArangoDB&rsquo;s Git repository into ArangoDB
and ran some queries on the data. While the query results for our repository may not be interesting for everyone,
I think it is still worth sharing what I did. Even though I didn&rsquo;t try it, I think the overall procedure is
applicable with any other Git repository.</p>

<!-- more -->


<h2>Converting the Git history to JSON</h2>

<p>The way to extract history and commit data from a local repository is to use <code>git log</code>. Though its output
is greatly customizable, it does not provide an out-of-the-box solution for producing JSON. So I wrote a simple
wrapper script (in PHP) around it. The script can be found <a href="https://gist.github.com/jsteemann/65ef221646449713b2c5">here</a>.</p>

<p>Here&rsquo;s how to run it:
<code>bash converting the git history to JSON
cd path/to/local/git-repository
wget https://gist.githubusercontent.com/jsteemann/65ef221646449713b2c5/raw/fef22c729e01dd0777b378ac1e17e951ea47c7dd/git-log-to-json.php
php git-log-to-json.php &gt; arango-commits-master-201503.json
</code></p>

<p>The script may run a few minutes on bigger repositories such as ours. In the end, it should produce a JSON
file named <code>arango-commits-master-201503.json</code>.</p>

<p>I have also uploaded the JSON file <a href="/downloads/code/arango-commits-master-201503.json">here</a>. Note that the
file only contains commits from the <code>master</code> branch and not all commits done in ArangoDB in total.</p>

<h2>Importing the commits into ArangoDB</h2>

<p>The simplest way to get the commits into ArangoDB is to use <code>arangoimp</code>:</p>

<p><code>bash importing the commits into ArangoDB
arangoimp                                   \
  --collection commits                      \
  --create-collection true                  \
  --file arango-commits-master-201503.json  \
  --overwrite true                          \
  --batch-size 32000000
</code></p>

<p>This should have imported all the commits into a collection <code>commits</code> in the default database.</p>

<h2>Querying the commit logs</h2>

<p>Following are a few example queries that I ran on the data from the ArangoShell.
As mentioned before, it should be possible to run the queries for other repositories' data.</p>

<p><code>js getting all contributors
query = 'FOR commit IN commits COLLECT author = commit.author.name RETURN author';
db._query(query).toArray();
</code></p>

<p><code>js retrieving the total number of commits
query = 'FOR commit IN commits COLLECT WITH COUNT INTO count RETURN count';
db._query(query).toArray();
</code></p>

<p><code>js retrieving the number of commits by contributor
query = 'FOR commit IN commits COLLECT author = commit.author.name WITH COUNT INTO count RETURN { author: author, count: count }';
db._query(query).toArray();
</code></p>

<p><code>js retrieving the tagged commits
query = 'FOR commit IN commits FILTER commit.tag != null SORT commit.date RETURN KEEP(commit, [ "date", "message", "tag" ])';
db._query(query).toArray();
</code></p>

<p><code>js retrieving number of commits per year
query = 'FOR commit IN commits COLLECT year = DATE_YEAR(commit.date) WITH COUNT INTO count RETURN { year: year, count: count }';
db._query(query).toArray();
</code></p>

<p><code>js retrieving number of commits per month / year
query = 'FOR commit IN commits COLLECT year = DATE_YEAR(commit.date), month = DATE_MONTH(commit.date)  WITH COUNT INTO count RETURN { month: CONCAT(year, "/", month), count: count }';
db._query(query).toArray();
</code></p>

<p><code>js retrieving number of commits per weekday
query = 'FOR commit IN commits COLLECT day = DATE_DAYOFWEEK(commit.date) WITH COUNT INTO count RETURN { day: TRANSLATE(day, { "0": "Sunday", "1": "Monday", "2": "Tuesday", "3": "Wednesday", "4": "Thursday", "5": "Friday", "6": "Saturday" }), count: count }';
db._query(query).toArray();
</code></p>

<p><code>js retrieving commits with string "issue #" in commit message
query = 'FOR commit IN commits FILTER LIKE(commit.message, "%issue #%") SORT commit.date DESC LIMIT 10 RETURN UNSET(commit, "files")';
db._query(query).toArray();
</code></p>

<p><code>js retrieving number of commits related to Foxx
query = 'FOR commit IN commits FILTER LIKE(LOWER(commit.message), "%foxx%") COLLECT year = DATE_YEAR(commit.date), month = DATE_MONTH(commit.date) WITH COUNT INTO count RETURN { month: CONCAT(year, "/", month), count: count }';
db._query(query).toArray();
</code></p>

<p><code>js retrieving commits that touched the most files
query = 'FOR commit IN commits LET count = LENGTH(commit.files || []) SORT count DESC LIMIT 10 RETURN MERGE(UNSET(commit, "files"), { files: count })';
db._query(query).toArray();
</code></p>

<p><code>js retrieving files modified most often
query = 'FOR commit IN commits FOR filename IN commit.files || [] COLLECT file = filename WITH COUNT INTO count SORT count DESC LIMIT 10 RETURN { file: file, count: count }';
db._query(query).toArray();
</code></p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
</feed>
