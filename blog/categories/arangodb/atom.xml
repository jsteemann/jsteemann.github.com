<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2014-11-05T01:28:35+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting Up Test Data]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data/"/>
    <updated>2014-11-04T22:14:21+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data</id>
    <content type="html"><![CDATA[<p>Today I was asked to look at code that was supposed to read data
from a MySQL data source, process it and then import it into ArangoDB.</p>

<p>To run and debug the code I had to have some MySQL data source. So I
thought I&rsquo;d quickly set up a simple example table with a few rows.
It turned out that this took more time than what I had expected.</p>

<p>Maybe I&rsquo;m spoilt by JavaScript-enabled, schema-free databases where
creating such test setups is so much easier.</p>

<!-- more -->


<p>I worked with MySQL databases in production for 10+ years and spent
much time working with the mysql client. I always liked MySQL, but in
the past few years, I was driven away from it and lost contact.
Instead, I got sucked into the NoSQL landscape and enjoy it pretty much.</p>

<p>Getting back to the original problem: I needed some MySQL table with a
few thousand rows for a test. It turned out I didn&rsquo;t even have MySQL
installed on my computer, so I needed to install it first.</p>

<p>After setting up the MySQL server, I created a table <code>examples</code> for
storing my test data:</p>

<p><code>sql
CREATE DATABASE test;
USE test;
CREATE TABLE examples (attribute1 VARCHAR(20), attribute2 VARCHAR(20));
</code></p>

<p>Not really the black belt of schema design, but good enough for a quick
test.</p>

<p>Now the table needed some rows. 100,000 rows should be enough. I wrote
some bash script to create them as there is no sane way to do this with
the MySQL client alone:</p>

<p><code>``bash
for i in</code>seq 1 100000`
  do</p>

<pre><code>echo "INSERT INTO examples VALUES (\"test$i\", \"test$i\");" &gt;&gt; import.sql 
</code></pre>

<p>  done
```</p>

<p>Time to import the data!</p>

<p><code>bash
mysql -u user test &lt; import.sql
</code></p>

<p>At first I was a bit surprised this command did not return instantly. I let it
run for about a minute, and then began checking the import progress with a second mysql
client. It turned out only very few records had been imported, and the import
script continued to create only around 30-35 records per second.</p>

<p>Seems I had forgotten that I am working with a No-NoSQL database, with full
ACID semantics for everything. My import file contained 100,000 <code>INSERT</code>
statements, so I was asking to perform 100,000 transactions and fsync operations.
That import would have taken forever with my slow HDD!</p>

<p>I quickly changed the InnoDB setting to make it commit only about once per second:
<code>
mysql&gt; SET GLOBAL innodb_flush_log_at_trx_commit = 2;
Query OK, 0 rows affected (0.00 sec)
</code></p>

<p>Now the import finished in 7 seconds.</p>

<p>I finally got the data in MySQL, but overall it took me about 10 minutes to get
it done. Probably a bit less if I still were an active user of MySQL and had
remembered the default behavior right from the start.</p>

<p>Still, my feeling is that it takes too much time to get something so simple
done.</p>

<p>I don&rsquo;t blame the database for trying to commit all 100,000 single-row
<code>INSERT</code> operations and fsync them to disk. It simply cannot know if the data
are important or just throw-away test records.</p>

<p>But there are other reasons: I had to write a bash script to produce the
test data, as there is no sane way to do this with the MySQL client alone.
Writing bash scripts is fine, and in general I like it, but I don&rsquo;t want to
do it for a dead-simple test setup.</p>

<p>And by the way, what if it turns out that I need to generate slightly more
complex test data? In the MySQL case I probably would have resorted to sed
or awk or would have thrown away my bash script and had rewritten it in some
other language. So I would have wasted even more time.</p>

<p>I personally prefer the ability to use a scripting language for such tasks.
JavaScript is ubiquituous these days, and I want to use it in a database&rsquo;s
command-line client.</p>

<p>For example, here&rsquo;s how the test setup would look like in the ArangoShell:
<code>js
db._create("examples");
for (i = 0; i &lt; 100000; ++i) {
  db.examples.save({ attribute1: "test" + i, attribute2: "test" + i });
}
</code>
I find this much easier to use: it allows to do everything in one place,
removing the need to write another script that prepares a data file or an
SQL file first.</p>

<p>As a bonus, using a programming language is much more flexible and powerful.
If I needed to generate slightly more complex test data, I can just do it,
adjust the JavaScript code and re-run it.</p>

<p>Even more annoying to me is that I needed to provide a schema for the
table first. I could have got away with declaring all text fields as
<code>VARCHAR(255)</code> or <code>TEXT</code> so I can at least ignore string
length restrictions. But I still need to type in the table schema
once, even if it feels completely useless for this particular use case.</p>

<p>It would get even more annoying if during my test I noticed I needed more
or other columns. Then I would need to adjust the table schema using <code>ALTER TABLE</code>
or adjust the <code>CREATE TABLE</code> statement and run it again, keeping me
away from the original task.</p>

<p>Maybe using schema-free databases for too long has spoilt me, but I much
more prefer starting quickly and without a schema. I know the data that
I am going to load will have a structure and will be somewhat self-describing,
so the database can still figure out what the individual parts of a record are.</p>

<p><em>On a side-note: should you be a fan of using query languages, the same
test setup can also be achieved by running the following AQL query from
the ArangoShell</em>:
```js
db._query(&ldquo;FOR i IN 1..100000 LET value = CONCAT(&lsquo;test&rsquo;, i) &rdquo; +</p>

<pre><code>      "INSERT { attribute1: value, attribute2: value } INTO examples");
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Set Up Bash Completion for ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/22/how-to-set-up-bash-completion-for-arangodb/"/>
    <updated>2014-10-22T23:10:32+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/22/how-to-set-up-bash-completion-for-arangodb</id>
    <content type="html"><![CDATA[<p>I was interested in how bash auto-completion works and how to write
a custom completer. After about an hour of work, I came up with a
solution that at least seems to work on Ubuntu. I now have auto-completion
for ArangoDB and all its client tools!</p>

<!-- more -->


<h2>The problem</h2>

<p>I use the command-line for almost everything, including starting
and stopping ArangoDB and its client tools. They provide lots
of options which I cannot completely memorize.</p>

<p>The bash solution for &ldquo;I don&rsquo;t know what I am looking for&rdquo; is to
press the <strong>TAB</strong> key. This will bring up a list of suggestions for
how to complete the currently entered word. I thought using the
same thing for ArangoDB&rsquo;s command-line options would be nice, too.</p>

<h2>The solution</h2>

<p>It turned out that I needed to put a shell script that generates the
auto completion for <code>arangod</code> and all the other tools into <code>/etc/bash_completion.d</code>.
From there, the system will automatically pick it up when auto-completion
is initialized.</p>

<p>The script is rather simple. For example, to have auto-completion for
<code>arangosh</code> it would look like this:</p>

<p>```bash completion script example for arangosh
_arangosh()
{</p>

<pre><code>local cur prev opts
COMPREPLY=()
cur="${COMP_WORDS[COMP_CWORD]}"
prev="${COMP_WORDS[COMP_CWORD-1]}"
opts="--help --server.endpoint --server.username" # ...all the options go here

if [[ ${cur} == -* ]] ; then
    COMPREPLY=( $(compgen -W "${opts}" -- ${cur}) )
    return 0
fi
</code></pre>

<p>}</p>

<p>complete -F _arangosh arangosh
```</p>

<p>As can be seen, the variable <code>opts</code> should be filled with the list of possible
options. Determining the options for a binary can be achieved by invoking it with its
<code>--help</code> option, e.g.:</p>

<p>```bash figuring out program options
arangosh &mdash;help | grep -o &ldquo;^\ +&mdash;[a-z-]+(.[a-z0-9-]+)\?&rdquo; | xargs</p>

<h1>this will generate something like the following output:</h1>

<h1>&mdash;audit-log &mdash;chunk-size &mdash;configuration &mdash;help &mdash;no-auto-complete &mdash;no-colors &mdash;pager &mdash;pretty-print &mdash;prompt &mdash;quiet &mdash;temp-path &mdash;use-pager &mdash;javascript.check &mdash;javascript.current-module-directory &mdash;javascript.execute &mdash;javascript.execute-string &mdash;javascript.gc-interval &mdash;javascript.startup-directory &mdash;javascript.unit-tests &mdash;jslint &mdash;log.level &mdash;log.use-local-time &mdash;server.connect-timeout &mdash;server.database &mdash;server.disable-authentication &mdash;server.endpoint &mdash;server.password &mdash;server.request-timeout &mdash;server.ssl-protocol &mdash;server.username</h1>

<p>```</p>

<p>That has to be repeated for all binaries in the ArangoDB package (i.e. arangob, arangosh,
arangoimp, arangodump, arangorestore, and arangod).</p>

<p>As the available options might change over time, I wrote a script that extracts them
from the binaries and puts together the completions file. This script can be downloaded
<a href="/downloads/code/build-completions.sh">here</a>. The script expects the already-built ArangoDB
binaries to be located in the <code>bin</code> subdirectory. Provided that ArangoDB was compiled from
source, this should already be the case.</p>

<p>The script should then be run from the base directory:
<code>bash
build-completions.sh arangodb
</code>
This will write the completions script for all binaries into the file <code>arangodb</code>.
An already generated version for devel can be found <a href="/downloads/code/completions-devel">here</a>.
Completions for current 2.2 can be found <a href="/downloads/code/completions-2.2">here</a>.</p>

<p>To activate completions, copy the appropriate file into <code>/etc/bash_completion.d/arangodb</code>.
Note that completion may need to be re-initialized once in order for it to work:
<code>bash
. /etc/bash_completion.d/arangodb
</code></p>

<h2>Quick setup</h2>

<p>For the impatient: the following command should install the completions for
2.2 and activate them:
```bash activate completion for ArangoDB 2.2
sudo \
  wget -O /etc/bash_completion.d/arangodb \</p>

<pre><code>https://jsteemann.github.io/downloads/code/completions-2.2 &amp;&amp; . /etc/bash_completion.d/arangodb
</code></pre>

<p>```</p>

<p>To see it in action, type <code>arangosh --</code> and then press <strong>TAB</strong>.</p>

<h2>Other environments (MacOS etc.)</h2>

<p><em>Note: I have checked that the above works on Ubuntu and OpenSuSE. I have no idea whether this works
with other Linux distributions let alone other shells.</em></p>

<p>Some Linux/Unix distros do not have <code>/etc/bash_completion.d</code> at all. I was told MacOS is one
of them. For such environments, downloading and sourcing the completions script should work:
```bash activate completion without bash_completion.d
wget -O ~/arangodb-completions-2.2 \</p>

<pre><code>https://jsteemann.github.io/downloads/code/completions-2.2
</code></pre>

<p>. ~/arangodb-completions-2.2
```</p>

<p>This will enable the completions in the current shell. To enable them permanently, add the
completions script to your <code>.bashrc</code> file:
<code>bash adding completions to .bashrc
echo ". ~/arangodb-completions-2.2" &gt;&gt; ~/.bashrc
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Compile ArangoDB From Source]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/16/how-to-compile-arangodb-from-source/"/>
    <updated>2014-10-16T22:24:48+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/16/how-to-compile-arangodb-from-source</id>
    <content type="html"><![CDATA[<p>Though we provide a lot of pre-built packages for the stable
versions of ArangoDB <a href="https://www.arangodb.org/install">here</a>, it
is often more interesting to play with the bleeding edge development
version. New ArangoDB features are normally added to the <em>devel</em>
branch, where they can be tested, documented and improved. When a
feature matures, it is either backported to a stable branch or will
eventually be released when the next stable branch is forked from
<em>devel</em>.</p>

<p>Contributing to the core of ArangoDB is also much easier with a
ready-to-go <em>devel</em> version. This post explains how to set one up
from scratch.</p>

<!-- more -->


<p>The following instructions are for Ubuntu 14.04 LTS, which seems to
be quite popular at the moment. Other flavors of Linux are probably
quite similar, though package manager and packages names will likely
be somewhat different.</p>

<h2>Using Vagrant</h2>

<p>If you don&rsquo;t have an Ubuntu 14 installation yet, you can easily
install one using <a href="http://www.vagrantup.com">Vagrant</a>. If you happen
to have a Linux installation already and are familiar with it, just
skip this section.</p>

<p>After installing Vagrant on your system, pick a suitable Vagrant box from
<a href="http://www.vagrantbox.es">here</a>. For example, I picked this 32 bit
box from the list:</p>

<pre><code>vagrant box add ubuntu-14.04-32 https://cloud-images.ubuntu.com/vagrant/trusty/current/trusty-server-cloudimg-i386-vagrant-disk1.box
</code></pre>

<p>After downloading the box, it can be made available via these commands:</p>

<pre><code>mkdir temp
cd temp
vagrant init ubuntu-14.04-32
vagrant up
</code></pre>

<p>After the VM is booted, connect to it via SSH:</p>

<pre><code>vagrant ssh
</code></pre>

<h2>Cloning the repository</h2>

<p>You&rsquo;re now on the Ubuntu VM. Next step is fetch the ArangoDB source
code from Github. Cloning the repository from there requires <code>git</code>. Let&rsquo;s
install it and clone the <em>devel</em> branch of the repository into a
directory named <em>devel</em> on the VM:</p>

<pre><code>sudo apt-get install git 
git clone -b devel https://github.com/triAGENS/ArangoDB.git
</code></pre>

<p>The repository contains a lot of history so cloning may take a while.
In case you don&rsquo;t need the full history, you can create a shallow
clone like this:</p>

<pre><code>git clone -b devel --single-branch --depth 1 https://github.com/triAGENS/ArangoDB.git 
</code></pre>

<p>This will reduce the download size from (currently) 375 MB to 56 MB
and should be much faster. The downside of using a shallow copy is
that there is no history and pushing and merging won&rsquo;t work most of
the time. So it&rsquo;s better used for throw-away tests only.</p>

<h2>Installing build tools and libraries</h2>

<p>Now that the repository has been cloned into directory <em>ArangoDB</em>,
we can install the required tools and libraries we need to build
from source:</p>

<pre><code>sudo apt-get install automake g++ libssl-dev libreadline-dev
</code></pre>

<p>If you prefer to install a different C++ compiler, please make sure it
has proper support for C++11.</p>

<p>Go 1.2 is also required. The official list of downloadable Go
versions can be found <a href="https://golang.org/dl/">here</a>. In the example,
I am using the 32 bit version in this example:</p>

<pre><code>wget https://storage.googleapis.com/golang/go1.2.2.linux-386.tar.gz
sudo tar -C /usr/local -xzf go1.2.2.linux-386.tar.gz
export PATH=$PATH:/usr/local/go/bin
echo "export PATH=\$PATH:/usr/local/go/bin" &gt;&gt; $HOME/.profile
</code></pre>

<h2>Compiling ArangoDB</h2>

<p>With all prerequisites set up, it&rsquo;s now time to compile ArangoDB.</p>

<p>You probably noticed that no <code>configure</code> file is shipped with ArangoDB
in the <code>devel</code> branch. To create it, we need to execute <code>make setup</code>
once. After that, <code>configure</code> can be executed to create the <code>Makefile</code>.
The <code>Makefile</code> finally contains the stuff that <code>make</code> needs:</p>

<pre><code>make setup
./configure --enable-all-in-one-icu --enable-all-in-one-v8 --enable-relative 
make
</code></pre>

<p>There first <code>make</code> run will take a while as it will compile all support
libraries (ICU, V8, libev, zlib) before it will actually compile ArangoDB.
Further invocations of <code>make</code> will not build these libraries again.
Only any changed code will be rebuilt.</p>

<p>Note that <code>make</code> can be parallelized if you have multiple processors
available. For 4 parallel <code>make</code> processes, use <code>make -j4</code>.</p>

<p><code>make</code> will produce a lot of output. The most important information, whether
or not an error occurred, can be found in its last line of its output. If
it does <strong>not</strong> say something like this, <code>make</code> has probably succeeded:
<code>
make: *** [all] Error 2
</code></p>

<h2>Starting ArangoDB</h2>

<p>When finished, <code>make</code> should have created all binaries in the <code>bin</code>
subdirectory. We can now start <code>arangod</code> and the binaries directly from
there without running a <code>make install</code>. In fact, <code>make install</code> is
awkward to do if you do many change-compile-test cycles.</p>

<pre><code>mkdir data          # creates a data directory
bin/arangod data    # starts the server
</code></pre>

<p>The server will be started as a foreground process (which is ideal
when developing the server). To stop the server, simply press CTRL-C.</p>

<h2>Connecting to ArangoDB</h2>

<p>To verify ArangoDB is actually working, open a separate terminal and
connect to it with the ArangoShell.</p>

<p>Note that if you used Vagrant, you will first need to connect to the
Vagrant box in the other terminal using <code>vagrant ssh</code> from the directory
you ran the <code>vagrant init</code> in. When connect to the Vagrant box, don&rsquo;t
forget to switch into the <code>ArangoDB</code> directory.</p>

<p>Once you&rsquo;re in the correct directory, just issue this:</p>

<pre><code>bin/arangosh
</code></pre>

<p>This should bring up the ArangoShell connected to your devel ArangoDB
instance.</p>

<h2>Making changes</h2>

<p>Time to make some changes in the code. A good place to start is usually
<code>main</code>. Here are a few places to get you started:
<code>
~/ArangoDB$ grep -r "int main" arangod/ arangosh/
arangod/RestServer/arango.cpp:int main (int argc, char* argv[]) {
arangosh/Benchmark/arangob.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangorestore.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangodump.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangoimp.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangosh.cpp:int main (int argc, char* argv[]) {
</code></p>

<p>Once you&rsquo;re done with your changes, you need to re-compile and run:</p>

<pre><code>make
bin/arangod data
</code></pre>

<p>Don&rsquo;t worry, <code>make</code> will only recompile what you changed plus what
depends on it and finally link it all together. This won&rsquo;t take as long
as on the previous run.</p>

<p>If you are serious about contributing to the server code, please let us
know <a href="https://groups.google.com/forum/#!forum/arangodb">here</a> so we can assist you.</p>

<h2>Getting updates</h2>

<p>We keep developing ArangoDB! To keep up to date and retrieve the latest
changes from our repository, issue the following commands:</p>

<pre><code>git pull origin devel
make
</code></pre>

<p>If <code>make</code> complains about files not found etc., the <code>Makefile</code> may have
changed. Then it&rsquo;s time for a rebuild:</p>

<pre><code>make clean
./configure --enable-all-in-one-icu --enable-all-in-one-v8 --enable-relative 
make
</code></pre>

<p>By the way, if you used special configure options and forgot them, you
can retrieve your previous options by typing <code>head config.log</code>.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Handling Binary Data in Foxx]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/15/handling-binary-data-in-foxx/"/>
    <updated>2014-10-15T20:41:30+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/15/handling-binary-data-in-foxx</id>
    <content type="html"><![CDATA[<p>Handling binary data in JavaScript applications is a bit
tricky because JavaScript does not provide a data type for
binary data. This post explains how to use binary data in
JavaScript actions written using ArangoDB&rsquo;s <a href="http://docs.arangodb.org/Foxx/README.html">Foxx</a>.</p>

<!-- more -->


<h1>Strings vs. binary data</h1>

<p>Internally, JavaScript strings are <a href="http://ecma-international.org/ecma-262/5.1/#sec-4.3.16">sequences of 16 bit integer values</a>.
Furthermore, the ECMAScript standard requires that a JavaScript
implementation should interpret characters in conformance with the
Unicode standard, using either UCS-2 or UTF-16 encoding.</p>

<p>While this is fine for handling natural language, it becomes problematic
when trying to work with arbitrary binary data. Binary data cannot be
used safely in a JavaScript string because it may not be valid UTF-16
data.</p>

<p>To make it work anyway, binary data needs to be stored in a wrapper
object. I won&rsquo;t go into details about ES6 typed arrays here, but will
focus on <code>Buffer</code> objects.</p>

<h1>Binary data in Foxx actions</h1>

<p>A Foxx route that shall handle HTTP POST requests containing arbitrary
(binary) body in the request body should not use <code>req.body()</code>. The
reason is that <code>req.body()</code> will return the body as a JavaScript string,
and this isn&rsquo;t going to work with arbitrary binary data.</p>

<p>Instead, the <code>req.rawBodyBuffer()</code> should be used. This will return the
request body inside a buffer.
Here&rsquo;s an example that stores the received data in a file on the server:</p>

<p>```js Foxx action that can handle binary input
controller.post(&lsquo;/receive-binary&rsquo;, function (req, res) {
  // fetch request body into the buffer
  var body = req.rawBodyBuffer();
  // create an absolute filename, local to the Foxx application directory
  var filename = applicationContext.foxxFilename(&ldquo;body&rdquo;);</p>

<p>  require(&ldquo;fs&rdquo;).write(filename, body);
});
```</p>

<p>This action can be invoked as follows if the app is mounted with name <code>app</code>:</p>

<pre><code>curl -X POST http://localhost:8529/app/receive-binary --data-binary @filename
</code></pre>

<p>This will send the contents of the file <code>filename</code> to the server. The Foxx
action will then store the received data as is in a file name <code>body</code> in the
application directory.</p>

<p>Returning binary data from a Foxx action is simple, too. Here&rsquo;s a way that
returns the contents of the file named <code>body</code> in the application&rsquo;s directory:
<code>js Foxx action that returns contents of a file
controller.get('/provide-binary-file', function (req, res) {
  // create an absolute filename, local to the Foxx application directory
  var filename = applicationContext.foxxFilename("body");
  // send the contents, this will also set mime type "application/octet-stream"
  res.sendFile(filename);
});
</code></p>

<p>It is also possible to return data from an arbitrary buffer:
```js Foxx action that returns data in a buffer
controller.get(&lsquo;/provide-binary-buffer&rsquo;, function (req, res) {
  // create an absolute filename, local to the Foxx application directory
  var filename = applicationContext.foxxFilename(&ldquo;body&rdquo;);
  // read the file content into a buffer
  var fileContent = require(&ldquo;fs&rdquo;).readBuffer(filename);</p>

<p>  // TODO: modify the contents of buffer here&hellip;</p>

<p>  // send the contents, this will also set mime type &ldquo;application/octet-stream&rdquo;
  res.send(fileContent);
});
```</p>

<h1>Example application</h1>

<p>I quickly put together an example application that shows how to handle arbitrary
binary data in Foxx actions. The example app allows uploading files to the server.
The server will then list these files and allows downloading them again.</p>

<p>The application has no CSS at all. Its only purpose is to demo the server-side code.
The application can be downloaded <a href="/downloads/code/filelist-app.tar.gz">here</a>.</p>

<p>Please note that the example application requires ArangoDB 2.3, which is currently
in development.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding Where Operations Are Executed]]></title>
    <link href="http://jsteemann.github.io/blog/2014/08/30/understanding-where-operations-are-executed/"/>
    <updated>2014-08-30T22:38:42+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/08/30/understanding-where-operations-are-executed</id>
    <content type="html"><![CDATA[<p>I recently had to deal with some data processing operation that took
about 20 minutes to complete. When looking into this, I found that the
easiest and most beneficial change to the whole setup was to make the
operation a <em>server-side</em> operation instead of executing it <em>client-side</em>.</p>

<p>This change reduced the operation&rsquo;s total execution time to a few seconds.</p>

<!-- more -->


<p>I can&rsquo;t show the original processing task here, so I&rsquo;ll start with a
contrived example. Imagine the following <em>for</em> loop inserting 100K documents
into a collection named <code>test</code>:
<code>js inserting 100k documents
for (i = 0; i &lt; 100000; ++i) {
  db.test.save({ value: i });
}
</code>
Now we only need a client application to execute the operation. As I don&rsquo;t
have a presentable client application right now, I will use the ArangoShell as
my client application.</p>

<h2>What&rsquo;s in a for loop?</h2>

<p>Running the above <em>for</em> loop inside the ArangoShell will lead to the loop being
executed inside the <em>arangosh</em> process.</p>

<p>In order to save a document in the collection, arangosh (our client) must make a
call to the ArangoDB server. This means issuing an HTTP POST request
to the server&rsquo;s REST API at <code>/_api/document/?collection=test</code>.
The server process will receive this request, insert the document, and
respond with an HTTP status code 201 or 202 to our client.
The client will then continue the loop until all documents have been inserted.</p>

<p>Now it&rsquo;s easy to see that the simple 3-line loop will issue 100,000 HTTP requests
in total. This means lots of data being pushed through the network stack(s).
It is pretty easy to imagine that this will come at a cost.</p>

<p>If we instead execute the above loop directly inside the ArangoDB server, we
can get rid of all the network overhead. The server has no need to send HTTP
calls to itself. It can simply execute the 100K inserts and is then done.
We therefore assume the loop to run somewhat faster when executed server-side.</p>

<p>A quick test on a crap laptop produced the following execution times for running
the loops:</p>

<ul>
<li>server-side execution (arangod): 1.34 seconds</li>
<li>client-side execution (arangosh): 17.32 seconds</li>
</ul>


<p><strong>Ouch</strong>. It looks like the client-server request-response overhead matters.</p>

<p>The following sections deal with how to get rid of some or even all the
client-server ping pong.</p>

<h2>Graph traversals</h2>

<p>The above <em>for</em> loop example was contrived, but imagine running
a client-side graph traversal instead. In fact, the original problem mentioned
in the introduction has been a graph traversal.</p>

<p>The problem of a graph traversal is that is often iterative and highly
dynamic. Decisions are made during the traversal as nodes are encountered,
leading to dynamic inclusion or exclusion etc. This means that it makes sense to
process nodes and edges only when needed, at the point when they are visited.</p>

<p>Even if the client can employ some sort of caching for already visited
nodes, the client still needs to ask the server about each visited
node&rsquo;s connections at least once. Otherwise it could not follow them.</p>

<p>This normally means lots of requests and responses. Compare this to the
<em>single</em> request-response alternative in which a client kicks off a server-side
traversal, and finally receives the overal result once it is assembled.</p>

<p><strong>Conclusion</strong>: traversals on anything but very small graphs should be run server-side.
A server-side action (see below) is a good way to do this. Please note that
running a server-side traversal does not mean giving up flexibility and
control flow functionality. Server-side traversals remain highly configurable
through custom JavaScript functions that allow implementation of user-defined
business logic.</p>

<h2>AQL queries</h2>

<p>We won&rsquo;t have your application send a series of 100,000 individual
insert statements to the relational database of our choice. We already
know from the past that this is going to be rather slow, so we have
learned to avoid this. In the relational context, we rather use SQL queries
that create or modify many rows in one go, e.g. an <code>INSERT INTO ... SELECT ...</code>,
bulk inserts etc.</p>

<p>ArangoDB is no different. In general, you should try to avoid issuing lots
of individual queries to the database from a client application. Instead and if
the queries look alike, try converting multiple individual operations into a
single AQL query. This will already save a lot of network overhead.</p>

<p>AQL provides multi-document operations to insert, update, and remove data. An
overview is given <a href="http://docs.arangodb.org/Aql/DataModification.html">here</a>.</p>

<p>The above 100K inserts from the contrived example can easily be transformed
into this single AQL query:
<code>
FOR i IN 1..100000 INSERT { value: i } INTO test
</code></p>

<h2>Bulk imports</h2>

<p>For importing larger amounts of documents from files, there is the specialized
<a href="http://docs.arangodb.org/Arangoimp/README.html">arangoimp</a> import tool. It can
load data from JSON and CSV files into ArangoDB. The tool is shipped with
ArangoDB.</p>

<p>ArangoDB also provides a REST API for <a href="http://docs.arangodb.org/HttpBulkImports/README.html">bulk imports</a>
of documents.</p>

<h2>Joins</h2>

<p>A special note about <em>joins</em>: the fact that several NoSQL databases do not
provide join functionality has driven some people to emulate join functionality
on the client-side, in their applications.</p>

<p>This can be a recipe for disaster: client-side join implementation might lead
to horrendous amounts of queries that might need to be sent to the database for
fetching all the records. More than that, if data are queried individually,
the overall result may lack consistency. By the way, the same is true for
fetching referenced or linked documents.</p>

<p>ArangoDB provides join functionality via AQL queries. Additionally, AQL queries
can be used to fetch other documents with the original documents. Note that
ArangoDB has no way of defining references or links between documents, but
still AQL allows combining arbitrary documents in one query.</p>

<p>In almost all cases it make more sense to use an AQL query that performs
joins or reference-fetching server-side and close to the data than having to
deal with that on the application-side of things.</p>

<p>AQL joins are described <a href="http://docs.arangodb.org/AqlExamples/Join.html">here</a>.</p>

<h2>Server-side actions</h2>

<p>With <em>stored procedures</em>, relational databases provide another way for an
application to trigger the execution of a large amount of queries. Stored
procedures are executed server-side, too, so they allow avoiding a lot of
request-response ping pong between the application and the database, at least
for defined tasks. Additionally, stored procedures provide control flow
functionality, which can also be handy when operations depend on each other.</p>

<p>Coming back to ArangoDB: complex data-processing tasks that need to execute
multiple operations or need control flow functionality might benefit if
converted from multiple application-side operations into a single server-side
action.</p>

<p>Server-side actions run inside the ArangoDB server, closer to the data, and
can be much faster than a series of client-side operations.
A server-side action is called with just one HTTP request from the application,
so it may lead to saving lots of request-response cycles and reduction in
network overhead. Apart from that, server-side actions in ArangoDB can employ
transactions and provide the necessary control over isolation and atomicity
when executing a series of operations.</p>

<p>Business logic and control flow functionality can be integrated
easily because server-side actions in ArangoDB are JavaScript functions,
with all of the language&rsquo;s programming features being available.</p>

<p>But there&rsquo;s even more to it: a single server-side operation can be written
to put together its result in a format most convenient for the client
application. This can also lead to better encapsulation, because all an
application needs to know about a server-side action is its API or contract.
Any internals of the action can be hidden from the client application. Overall,
this supports a service-oriented approach.</p>

<p>To learn more about how to write server-side actions, please have a look
at ArangoDB&rsquo;s <a href="http://docs.arangodb.org/Foxx/README.html">Foxx</a>. It is all
about making server-side actions available via REST APIs.</p>
]]></content>
  </entry>
  
</feed>
