<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2014-12-14T13:21:10+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AQL Improvements for 2.4]]></title>
    <link href="http://jsteemann.github.io/blog/2014/12/12/aql-improvements-for-24/"/>
    <updated>2014-12-12T23:35:08+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/12/12/aql-improvements-for-24</id>
    <content type="html"><![CDATA[<p>While on a retreat in Belgium, we found some spare time to
work on improvements for AQL. These will be shipped with
ArangoDB version 2.4, and are already available in the devel
version for testing from now on.</p>

<p>Here&rsquo;s a short overview of the improvements:</p>

<!-- more -->


<h1>COLLECT WITH COUNT</h1>

<p>A common use case in query languages is to count the number of
documents returned by a query. The AQL solution for this has been
to use the <code>LENGTH</code> function and a subquery:</p>

<p>```
RETURN LENGTH((
  FOR doc IN collection</p>

<pre><code>FILTER doc.someAttribute == someValue
RETURN doc
</code></pre>

<p>  ))
```</p>

<p>This is quite long and probably unintuitive for people which have
used SQL for years.</p>

<p>We therefore now allow using the following alternative version:</p>

<p><code>
FOR doc IN collection
  FILTER doc.someAttribute == someValue
  COLLECT WITH COUNT INTO length
  RETURN length
</code></p>

<p>This query returns just the total number of matches, but not the
matches themselves. As this query is made for counting only, it
enables the optimizer to execute this query more efficiently.
The documents found by the filter can be discarded
instantly after the filter condition is evaluated, and do not need
to shipped around inside the query.</p>

<p>As a bonus, there is no need to use a subquery anymore, though the
former variant is still fully supported and will be.</p>

<p><code>COLLECT WITH COUNT</code> also works with groups:</p>

<p><code>
FOR doc IN collection
  COLLECT value = doc.someAttribute WITH COUNT INTO length
  RETURN { value: value, length : length }
</code></p>

<p>This returns the number of matches for each distinct <code>value</code>.</p>

<p>A quick unscientific benchmark reveals that the specialized
<code>WITH COUNT</code> clause seems to be faster than the old variant.
The following results show the differences for a collection with
500,000 small documents:</p>

<p>The old variant that counts the number of documents by age runs
in 4.75 seconds on my laptop:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age INTO g
  RETURN { age: age, length: LENGTH(g) }
</code></p>

<p>The new variant does the same, but runs in 0.6 seconds locally:</p>

<p><code>
FOR doc IN collection
  COLLECT age = doc.age WITH COUNT INTO length
  RETURN { age: age, length: length }
0.6001598834991455
</code></p>

<p>A notable speedup can also be observed if only a fraction of the
groups is built (here: 1/8). The old variant for this runs in 0.6
seconds:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age INTO g
  RETURN { age: age, length: LENGTH(g) }
</code></p>

<p>The new variants runs in 0.12 seconds:</p>

<p><code>
FOR doc IN collection
  FILTER doc.age &lt; 20
  COLLECT age = doc.age WITH COUNT INTO length
  RETURN { age: age, length: length }
</code></p>

<p>The absolute times may vary greatly depending on the documents and
the hardware used, but in general the new variant should provide a
speedup.</p>

<h1>Removing filters covered by indexes</h1>

<p><code>FILTER</code> conditions which are completely covered by indexes will
now be removed from the execution plan if it is safe to do so.
Dropping the <code>FILTER</code> statements allows the optimizer to get rid
of not only the <em>FilterNode</em>, but also its corresponding <em>CalculationNode</em>.
This will save a lot of computation if many documents match the
<code>FILTER</code> condition.</p>

<p>For example, imagine the following query:</p>

<p><code>
FOR doc IN collection
  FILTER doc.value &lt; 10
  RETURN doc
</code></p>

<p>If there is a (skiplist) index on <code>doc.value</code>, the optimizer may
decide to use this index. It will insert an <em>IndexRangeNode</em> instead
of a full collection scan. The <em>IndexRangeNode</em> will scan the index
on <code>doc.value</code> for the range [-inf, 10).</p>

<p>Following that, the optimizer rule <code>remove-filter-covered-by-index</code>
should fire and detect that the <code>FILTER</code> condition is already covered
by the <em>IndexRangeNode</em>, and remove the <em>FilterNode</em>. This makes the
<em>CalculationNode</em> of the <em>FilterNode</em> obsolete, so these two nodes will
be removed.</p>

<h1>Removing brackets for subquery function call parameters</h1>

<p>Since the beginning of AQL, the parser required the user the put
subqueries that were used as function parameters inside two pairs of
brackets.</p>

<p>For example, the following query did not parse in 2.3 and before:
<code>
RETURN LENGTH(FOR doc IN collection RETURN doc)
</code></p>

<p>Instead, it needed to be written as:
<code>
RETURN LENGTH((FOR doc IN collection RETURN doc))
</code></p>

<p>This has caused several support questions over time, and I have to
admit the double brackets were not intuitive. In fact, they were an
artifact required by the AQL parser in order to parse the query
correctly.</p>

<p>For 2.4, the AQL grammar has now been cleaned up in this respect.
Double brackets are still allowed in 2.4 but are not required anymore.
This should make the first steps with AQL a bit easier.</p>

<p>We&rsquo;re 1.5 days into our retreat now. Maybe there&rsquo;ll be some more
AQL-related improvements in the end. Let&rsquo;s see.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Tour Around the New AQL Query Optimizer]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer/"/>
    <updated>2014-11-07T22:30:10+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/a-tour-around-the-aql-query-optimizer</id>
    <content type="html"><![CDATA[<p>The major new feature in ArangoDB 2.3 is the shiny new
AQL query optimizer and executor. These parts of ArangoDB have been
rewritten in 2.3 to make AQL much better for our end users.</p>

<!-- more -->


<p>Since one of the initial releases, ArangoDB has been shipped with
AQL, the <em>ArangoDB Query Language</em>. AQL has since then been ArangoDB&rsquo;s
most versatile way of executing simple and also the not-so-simple
queries.</p>

<p>I&rsquo;ll start with an overview of query execution in previous versions
of ArangoDB, and then explain the new engine and explain the differences.</p>

<h2>History: query execution in pre-2.3</h2>

<p>Previous versions of ArangoDB executed any AQL query in the following
steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>collect filter conditions in AST and look for index usage opportunities</li>
<li>generate code</li>
<li>execute code</li>
</ul>


<p>This approach was simple and has worked for a lot of queries, but it also
had a few quirks:</p>

<p>First of all, most of the steps were carried out directly on the
abstract syntax tree, with the AST nodes being modified in place.
There was also just the one AST per query, so the old AQL executor
could not generate multiple, potentially very different execution
plan candidates for a given query.</p>

<p>The &ldquo;old&rdquo; optimizer was able to move AST nodes around during optimization
and it was already able to consider multiple index candidates for a query,
but it would not compare multiple plans and make a cost-based decision.
It was also limited in the amount and scope of transformations it
could safely apply to the AST.</p>

<p>When it came to code generation and execution, the &ldquo;old&rdquo; executor
fully relied on V8 to execute the queries. Result sets were created
using V8&rsquo;s value objects. Documents from collections that queries
needed to iterate over had to be made available to V8. While some
optimization was used for this, the conversions could have summed up
to significant overhead for certain kinds of queries.</p>

<p>The representation of queries via an AST also made it hard to generate
code that supported lazy evaluation during query execution.</p>

<p>Finally, the AQL optimizer so far did not provide much support for
queries that were to be executed in a distributed fashion inside a cluster
of servers.</p>

<h2>Query execution in ArangoDB 2.3</h2>

<p>We wanted to address all these issues with a rewrite of the AQL
infrastructure. Starting with ArangoDB 2.3, AQL queries are executed
in these steps:</p>

<ul>
<li>tokenize and parse query string into an abstract syntax tree (AST)</li>
<li>perform simple AST optimizations</li>
<li>transform AST into execution plan</li>
<li>optimize and permute execution plans</li>
<li>estimate costs for execution plans and pick optimal plan</li>
<li>instanciate execution engine from optimal plan</li>
<li>(in cluster only) send execution plan parts to cluster nodes</li>
<li>execute query</li>
</ul>


<p>Tokenization and parsing of AQL queries hasn&rsquo;t changed much in 2.3:
query strings are still parsed using a Bison/Flex-based parser and
lexer combo. The AST structure has proven to be good during the parsing
stage, so the parser creates an initial AST from the query string first.</p>

<p>After that, simple optimizations are performed directly on the AST,
such as constant folding and constant propagation. Deterministic functions
with constant operands will be executed already in this stage and the
results be injected into the AST.</p>

<p>A major change in 2.3 is that no further transformations will be
carried out on the AST. Instead, the AST will be transformed into
an initial <em>execution plan</em>.</p>

<p>This execution plan is the starting point for the <em>query optimizer</em>.
It will take the initial execution plan and apply transformations to
it. Transformations will either update the existing plan in place or
create a new, modified plan. The result of the transformations carried
out will form the input for further transformations that can be carried
out by query optimizer.</p>

<p>The result of the query optimization stage is one or many execution
plans. For each plan, the optimizer will estimate a cost value, and
then finally pick the plan with the lowest total estimated cost.
This plan is considered to be the <em>optimal plan</em>. All other execution
plans will be discarded by the optimizer as it has considered them non-optimal.</p>

<p>The optimal execution plan is then executed by the <em>execution engine</em>.
For a single-server AQL query, this is straightforward: for each step
in the execution plan, a C++ object is created that is supposed to
execute the particular step. Query execution is then started by asking
the first of these objects for its results.</p>

<p>The objects for multiple processing steps are linked in a pipelined fashion
with lazy evaluation. Pulling data from the first object will eventually
trigger pulling data from the second object etc., until there are no more
results to produce.</p>

<p>For a distributed query, this is a bit more complicated. The different
execution steps will likely be shipped to different servers in the
cluster, and the objects need to be instanciated in different servers, too.
The different parts of the query may pull data from each other via HTTP
calls between cluster nodes.</p>

<h2>How execution plans work</h2>

<p>An execution plan is a sequence of query execution steps. Let&rsquo;s
start with a very simple example:</p>

<p><code>
FOR doc IN mycollection
  RETURN doc._key
</code></p>

<p>This query will be transformed into the following execution plan:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: iterates over all documents of a collection
and provides the current document in an output variable. In our example,
it will iterate over collection <code>mycollection</code> and provide each
document in variable <code>doc</code></li>
<li><em>CalculationNode</em>: evaluates an expression and returns its result.
In the example, it will calculate <code>doc._key</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>If this plan is going to be executed, the execution engine will start
pulling data from the node at the bottom, that is, the <em>ReturnNode</em>. The
<em>ReturnNode</em> at this stage cannot provide any data, so it will ask its
predecessor node, which in the example is the <em>CalculationNode</em>. The
<em>CalculationNode</em> again does not have own data yet, so it must ask the
node in front of it. The <em>EnumerateCollectionNode</em> will first ask the
<em>SingletonNode</em> for input data. So the execution flow has bubbled up from
the bottom of the sequence to the top.</p>

<p>The <em>SingletonNode</em> will now produce a single empty return value. It will
also internally set its processing status to <em>done</em>, so it will not produce
any more values if asked again. This is all a <em>SingletonNode</em> will ever do.
We&rsquo;ll see later why such a node may still be useful.</p>

<p>The single empty value will be provided as input to the <em>EnumerateCollectionNode</em>.
This node will now go through all the documents in the underlying collection,
and return them once for each input value its got. As its input value was
the singleton, it will return the documents of the collection just once.</p>

<p>Processing is executed in blocks of size 1000 by default. The
<em>EnumerateCollectionNode</em> will thus not return all documents to its successor
node, but just 1,000. The return value will be a vector with 1,000 documents,
stored under variable name <code>doc</code>.</p>

<p>The <em>CalculationNode</em>, still waiting for input data, can now execute its
expression <code>doc._key</code> on this input value. It will execute this expression
1,000 times, once for each input value. The expression results will be
stored in another variable. This variable is anonymous, as it hasn&rsquo;t been
named explicitly in the original query. The vector of results produced by
the <em>CalculationNode</em> is then returned to the <em>ReturnNode</em>, which will then
return it to the caller.</p>

<p>If the caller requests more documents, the procedure will repeat. Whenever
a processing step cannot produce any more data, it will ask its predecessor
step for more data. If the predecessor step already has status <em>done</em>, the
current step will set itself to <em>done</em> as well, so a query will actually
come to an end if there are no more results.</p>

<p>As can be seen, steps are executed with batches of values. We thought this
would be a good way to improve efficiency and reduce the number of hops
between steps.</p>

<h2>Joins</h2>

<p>Let&rsquo;s say we want to join documents from two collections, based on common
attribute values. Let&rsquo;s use <code>users</code> and <code>logins</code>, joined by their <code>id</code> and
<code>userId</code> attributes:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Provided that there are no indexes, the query may be turned into this
execution plan by the optimizer:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>logins</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>Now we can see why the <em>SingletonNode</em> is useful: it can be used as an
input to another node, telling this node to execute just once. Having the
<em>SingletonNode</em> will ensure that the outermost <em>EnumerateCollection</em>
will only iterate once over the documents in its underlying collection <code>users</code>.</p>

<p>The inner <em>EnumerateCollectionNode</em> for collection <code>logins</code> is now fed by
the outer <em>EnumerateCollectionNode</em> on <code>users</code>. Thus these two nodes will
produce a cartesian product. This will be done lazily, as producing results
will normally happen in chunks of 1,000 values each.</p>

<p>The results of the cartesian product are then post-filtered by the <code>FilterNode</code>,
which will only let those documents pass that match the filter condition of
the query. The <code>FilterNode</code> employs its predecessor, the <code>CalculationNode</code>,
to determine which values satisfy the condition.</p>

<h2>Using indexes</h2>

<p>Obviously creating cartesian products is not ideal. The optimizer will try
to avoid generating such plans if it can, but it has no choice if there are
no indexes present.</p>

<p>If there are indexes on attributes that are used in <code>FILTER</code> conditions of
a query, the optimizer will try to turn <code>EnumerateCollectionNode</code>s into
<code>IndexRangeNode</code>s. The purpose of an <code>IndexRangeNode</code> is to iterate over a
specific range in an index. This is normally more efficient than iterating
over all documents of a collection.</p>

<p>Let&rsquo;s assume there is an index on <code>logins.userId</code>. Then the optimizer might
be able to generate a plan like this:</p>

<ul>
<li><em>SingletonNode</em>: passes a single empty value to the following steps</li>
<li><em>EnumerateCollectionNode</em>: will iterate over all documents in collection
<code>users</code> and produce a variable named <code>user</code></li>
<li><em>IndexRangeNode</em>: will iterate over the values in index <code>logins.userId</code> that
match the value of <code>users.id</code> and produce a variable named <code>login</code></li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>user.id == login.userId</code></li>
<li><em>FilterNode</em>: will let only documents pass that match the filter condition
(calculated by the <em>CalculationNode</em> above it)</li>
<li><em>CalculationNode</em>: will calculate the result of the expression
<code>{ user: user, login: login }</code></li>
<li><em>ReturnNode</em>: returns results to the caller</li>
</ul>


<p>To run this query, the execution engine must still iterate over all documents
in collection <code>users</code>, but for each of those, it only needs to find the documents
in <code>logins</code> that match the join condition. This most likely means a lot less
lookups and thus much faster execution.</p>

<h2>Permutation of loops</h2>

<p>Now consider adding an extra <code>FILTER</code> statement to the original query so we
end up with this:</p>

<p>```
FOR user IN users
  FOR login IN logins</p>

<pre><code>FILTER user.id == login.userId
FILTER login.ts == 1415402319       /* added this one! */
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>The optimizer is free to permute the order of <code>FOR</code> loops as long as this
won&rsquo;t change the results of a query. In our case, permutation of the two
<code>FOR</code> loops is allowed (the query does not contain a <code>SORT</code> instruction so
the order of results is not guaranteed).</p>

<p>If the optimizer exchanges the two loops, it can also pull out the <code>FILTER</code>
statement on <code>login.ts</code> out of the inner loop, and move up into the outer loop.
It might come up with a plan like this, which may be more efficient if a
lot of documents from <code>logins</code> can be filtered out early:</p>

<p>```
FOR login IN logins
  FILTER login.ts == 1415402319
  FOR user IN users</p>

<pre><code>FILTER user.id == login.userId
RETURN { user: user, login: login }
</code></pre>

<p>```</p>

<p>Exchanging the order of <code>FOR</code> loops may also allow the optimizer to use
additional indexes.</p>

<p>A last note on indexes: the optimizer in 2.3 is able to use (sorted)
skiplist indexes to eliminate extra <code>SORT</code> operations. For example, if
there is a skiplist index on <code>login.ts</code>, the <code>SORT</code> in the following
query can be removed by the optimizer:</p>

<p><code>
FOR login IN logins
  FILTER login.ts &gt; 1415402319
  SORT login.ts
  RETURN login
</code></p>

<p>The AQL optimizer in 2.3 can optimize away a <code>SORT</code> even if the sort
order is backwards or if no <code>FILTER</code> statement is used in the query at
all.</p>

<h2>Analyzing plans</h2>

<p>One particular improvement over 2.2 is that in ArangoDB 2.3 the optimizer
provides functionality for retrieving full execution plan information for
queries <strong>without</strong> executing them. The execution plan information can be
inspected by developers or DBAs, and, as it is JSON-encoded, can also be
analyzed programmatically.</p>

<p>Retrieving the execution plan for a query is straight-forward:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain();
</code></p>

<p>By default, the optimizer will return just the <em>optimal plan</em>, containing
all the plan&rsquo;s execution nodes with lots of extra information plus cost estimates.</p>

<p>The optimizer is also able to return the alternative plans it produced but
considered to be non-optimal:</p>

<p><code>
arangosh&gt; db._createStatement({ query: &lt;query&gt; }).explain({ allPlans: true });
</code></p>

<p>This will hopefully allow developers and DBAs to get a better idea of how an
AQL query will be executed internally.</p>

<p>Additionally, simple execution statistics are returned by default when executing
a query. This statistics can also be used to get an idea of the runtime costs of
a query <strong>after</strong> execution.</p>

<h2>Writing optimizer rules</h2>

<p>The AQL optimizer itself is dumb. It will simply try to apply all transformations
from its rulebook to each input execution plan it is feeded with. This
will produce output execution plans, on which further transformations
may or may not be applied.</p>

<p>The more interesting part of the AQL optimizer stage is thus the rulebook.
Each rule in the rulebook is a C++ function that is executed for an input plan.</p>

<p>Adding a new optimizer rule to the rulebook is intentionally simple. One of
the design goals of the new AQL optimizer was to keep it flexible and extensible.
All that&rsquo;s need to be to add an optimizer rule is to implement a C++ function
with the following signature:</p>

<p><code>cpp
(Optimizer*, ExecutionPlan*, Optimizer::Rule const*) -&gt; int
</code></p>

<p>and register it once in the Optimizer&rsquo;s rulebook.</p>

<p>An optimizer rule function is called with an instance of the query optimizer
(it can use it to register a new plan), the current execution plan and some
information about the rule itself (this is the information about the rule from
the rulebook).</p>

<p>The optimizer rule function can then analyze the input execution plan, modifiy
it in place, and/or create additional plans. It must return a status code to
the optimizer to indicate if something went wrong.</p>

<h2>Outlook</h2>

<p>The AQL optimizer features described here are available in ArangoDB 2.3, which
is currently in <a href="https://www.arangodb.com/install-beta-version">beta stage</a>.</p>

<p>Writing a perfect query optimizer is a never-ending endeavour. Other databases
provide new optimizer features and fixes even decades after the initial version.</p>

<p>Our plan is to ship 2.3 with several essential and useful optimizer rules. We
will likely add more in future releases. We&rsquo;re also open to contributions.
If you can think of rules that are missing but you would like to see in ArangoDB,
please let us know. If you would like to contribute to the optimizer and write some
rule code, consider sending a pull request or an email to
<a href="hackers@arangodb.org">hackers@arangodb.org</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improved Non-unique Hash Indexes in 2.3]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes/"/>
    <updated>2014-11-07T20:51:12+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes</id>
    <content type="html"><![CDATA[<p>With ArangoDB 2.3 now getting into the <a href="https://www.arangodb.com/install-beta-version">beta stage</a>,
it&rsquo;s time to spread the word about new features and improvements.</p>

<p>Today&rsquo;s post will be about the changes made to non-unique hash
indexes.</p>

<!-- more -->


<p>Hash indexes allow looking up documents quickly if the indexed
attributes are all provided in a search query. They are not
suitable for range queries, but are the perfect choice if equality
comparisons are all that&rsquo;s needed.</p>

<p>Hash indexes have been available in ArangoDB ever since. There
have always been two variants of them:</p>

<ul>
<li>unique hash indexes</li>
<li>non-unique hash indexes</li>
</ul>


<p>There wasn&rsquo;t much to be done for unique hash indexes, and so there
haven&rsquo;t been any changes to them in 2.3. However, the non-unique
hash indexes were improved significantly in the new version.</p>

<p>The non-unique indexes already performed quite well if most of the
indexed values were unique and only few repetitions occurred. But their
performance suffered severly if the indexed attribute values repeated
a lot &ndash; that is, when the indexed value had a <strong>low cardinality</strong> and thus
the index had a <strong>low selectivity</strong>.</p>

<p>This was a problem because it slowed down inserting new documents into
a collection with such an index. And it also slowed down loading collections
with low cardinality hash indexes.</p>

<p>I am happy to state that in ArangoDB 2.3 this has been fixed, and the insert
performance of non-unique hash indexes has been improved significantly.
The index insertion time now scales quite well with the number
of indexed documents regardless of the cardinality of the indexed
attribute.</p>

<p>Following are a few measurements of non-unique hash index insertion
times from ArangoDB 2.3, for different cardinalities of the indexed
attribute.</p>

<p>The times reported are the net non-unique hash index
insertion times (the documents were present already, just the index
was created on them and index creation time was measured).</p>

<p>Let&rsquo;s start with a not too challenging case: indexing documents in
a collection with 100,000 different index values (<em>cardinality 100,000</em>):</p>

<p><code>text index insertion times for cardinality 100,000
number of documents:    128,000    =&gt;    time:   0.144 s
number of documents:    256,000    =&gt;    time:   0.231 s
number of documents:    512,000    =&gt;    time:   0.347 s
number of documents:  1,024,000    =&gt;    time:   0.694 s
number of documents:  2,048,000    =&gt;    time:   1.379 s
</code></p>

<p>The picture doesn&rsquo;t change much when reducing the cardinality
by a factor or 10 (i.e. <em>cardinality 10,000</em>):</p>

<p><code>text index insertion times for cardinality 10,000
number of documents:    128,000    =&gt;    time:   0.169 s
number of documents:    256,000    =&gt;    time:   0.194 s
number of documents:    512,000    =&gt;    time:   0.355 s
number of documents:  1,024,000    =&gt;    time:   0.668 s
number of documents:  2,048,000    =&gt;    time:   1.325 s
</code></p>

<p>Let&rsquo;s again divide cardinality by 10 (now <em>cardinality 1,000</em>):</p>

<p><code>text index insertion times for cardinality 1,000
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.152 s
number of documents:    512,000    =&gt;    time:   0.261 s
number of documents:  1,024,000    =&gt;    time:   0.524 s
number of documents:  2,048,000    =&gt;    time:   0.934 s
</code></p>

<p><em>Cardinality 100</em>:</p>

<p><code>text index insertion times for cardinality 100
number of documents:    128,000    =&gt;    time:   0.114 s
number of documents:    256,000    =&gt;    time:   0.148 s
number of documents:    512,000    =&gt;    time:   0.337 s
number of documents:  1,024,000    =&gt;    time:   0.452 s
number of documents:  2,048,000    =&gt;    time:   0.907 s
</code></p>

<p><em>Cardinality 10</em>:</p>

<p><code>text index insertion times for cardinality 10
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.327 s
number of documents:    512,000    =&gt;    time:   0.239 s
number of documents:  1,024,000    =&gt;    time:   0.442 s
number of documents:  2,048,000    =&gt;    time:   0.827 s
</code></p>

<p>Finally we get to <em>cardinality 1</em>, the definitive indicator
for the index being absolutely useless. Let&rsquo;s create it anyway,
for the sake of completeness of this post:</p>

<p><code>text index insertion times for cardinality 1
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    128,000    =&gt;    time:   0.095 s
number of documents:    256,000    =&gt;    time:   0.146 s
number of documents:    512,000    =&gt;    time:   0.246 s
number of documents:  1,024,000    =&gt;    time:   0.445 s
number of documents:  2,048,000    =&gt;    time:   0.925 s
</code></p>

<p>On a side note: all indexed values were numeric. In absolute terms,
indexing string values will be slower than indexing numbers, but insertion
should still scale nicely with the number of documents as long as everything
fits in RAM.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Test Data]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data/"/>
    <updated>2014-11-04T22:14:21+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data</id>
    <content type="html"><![CDATA[<p>Today I was asked to look at code that was supposed to read data
from a MySQL data source, process it and then import it into ArangoDB.</p>

<p>To run and debug the code I had to have some MySQL data source. So I
thought I&rsquo;d quickly set up a simple example table with a few rows.
It turned out that this took more time than what I had expected.</p>

<p>Maybe I&rsquo;m spoilt by JavaScript-enabled, schema-free databases where
creating such test setups is so much easier.</p>

<!-- more -->


<p>I worked with MySQL databases in production for 10+ years and spent
much time working with the mysql client. I always liked MySQL, but in
the past few years, I was driven away from it and lost contact.
Instead, I got sucked into the NoSQL landscape and enjoy it pretty much.</p>

<p>Getting back to the original problem: I needed some MySQL table with a
few thousand rows for a test. It turned out I didn&rsquo;t even have MySQL
installed on my computer, so I needed to install it first.</p>

<p>After setting up the MySQL server, I created a table <code>examples</code> for
storing my test data:</p>

<p><code>sql
CREATE DATABASE test;
USE test;
CREATE TABLE examples (attribute1 VARCHAR(20), attribute2 VARCHAR(20));
</code></p>

<p>Not really the black belt of schema design, but good enough for a quick
test.</p>

<p>Now the table needed some rows. 100,000 rows should be enough. I wrote
some bash script to create them as there is no sane way to do this with
the MySQL client alone:</p>

<p><code>``bash
for i in</code>seq 1 100000`
  do</p>

<pre><code>echo "INSERT INTO examples VALUES (\"test$i\", \"test$i\");" &gt;&gt; import.sql 
</code></pre>

<p>  done
```</p>

<p>Time to import the data!</p>

<p><code>bash
mysql -u user test &lt; import.sql
</code></p>

<p>At first I was a bit surprised this command did not return instantly. I let it
run for about a minute, and then began checking the import progress with a second mysql
client. It turned out only very few records had been imported, and the import
script continued to create only around 30-35 records per second.</p>

<p>Seems I had forgotten that I am working with a No-NoSQL database, with full
ACID semantics for everything. My import file contained 100,000 <code>INSERT</code>
statements, so I was asking to perform 100,000 transactions and fsync operations.
That import would have taken forever with my slow HDD!</p>

<p>I quickly changed the InnoDB setting to make it commit only about once per second:
<code>
mysql&gt; SET GLOBAL innodb_flush_log_at_trx_commit = 2;
Query OK, 0 rows affected (0.00 sec)
</code></p>

<p>Now the import finished in 7 seconds.</p>

<p>I finally got the data in MySQL, but overall it took me about 10 minutes to get
it done. Probably a bit less if I still were an active user of MySQL and had
remembered the default behavior right from the start.</p>

<p>Still, my feeling is that it takes too much time to get something so simple
done.</p>

<p>I don&rsquo;t blame the database for trying to commit all 100,000 single-row
<code>INSERT</code> operations and fsync them to disk. It simply cannot know if the data
are important or just throw-away test records.</p>

<p>But there are other reasons: I had to write a bash script to produce the
test data, as there is no sane way to do this with the MySQL client alone.
Writing bash scripts is fine, and in general I like it, but I don&rsquo;t want to
do it for a dead-simple test setup.</p>

<p>And by the way, what if it turns out that I need to generate slightly more
complex test data? In the MySQL case I probably would have resorted to sed
or awk or would have thrown away my bash script and had rewritten it in some
other language. So I would have wasted even more time.</p>

<p>I personally prefer the ability to use a scripting language for such tasks.
JavaScript is ubiquituous these days, and I want to use it in a database&rsquo;s
command-line client.</p>

<p>For example, here&rsquo;s how the test setup would look like in the ArangoShell:
<code>js
db._create("examples");
for (i = 0; i &lt; 100000; ++i) {
  db.examples.save({ attribute1: "test" + i, attribute2: "test" + i });
}
</code>
I find this much easier to use: it allows to do everything in one place,
removing the need to write another script that prepares a data file or an
SQL file first.</p>

<p>As a bonus, using a programming language is much more flexible and powerful.
If I needed to generate slightly more complex test data, I can just do it,
adjust the JavaScript code and re-run it.</p>

<p>Even more annoying to me is that I needed to provide a schema for the
table first. I could have got away with declaring all text fields as
<code>VARCHAR(255)</code> or <code>TEXT</code> so I can at least ignore string
length restrictions. But I still need to type in the table schema
once, even if it feels completely useless for this particular use case.</p>

<p>It would get even more annoying if during my test I noticed I needed more
or other columns. Then I would need to adjust the table schema using <code>ALTER TABLE</code>
or adjust the <code>CREATE TABLE</code> statement and run it again, keeping me
away from the original task.</p>

<p>Maybe using schema-free databases for too long has spoilt me, but I much
more prefer starting quickly and without a schema. I know the data that
I am going to load will have a structure and will be somewhat self-describing,
so the database can still figure out what the individual parts of a record are.</p>

<p><em>On a side-note: should you be a fan of using query languages, the same
test setup can also be achieved by running the following AQL query from
the ArangoShell</em>:
```js
db._query(&ldquo;FOR i IN 1..100000 LET value = CONCAT(&lsquo;test&rsquo;, i) &rdquo; +</p>

<pre><code>      "INSERT { attribute1: value, attribute2: value } INTO examples");
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Set Up Bash Completion for ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/22/how-to-set-up-bash-completion-for-arangodb/"/>
    <updated>2014-10-22T23:10:32+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/22/how-to-set-up-bash-completion-for-arangodb</id>
    <content type="html"><![CDATA[<p>I was interested in how bash auto-completion works and how to write
a custom completer. After about an hour of work, I came up with a
solution that at least seems to work on Ubuntu. I now have auto-completion
for ArangoDB and all its client tools!</p>

<!-- more -->


<h2>The problem</h2>

<p>I use the command-line for almost everything, including starting
and stopping ArangoDB and its client tools. They provide lots
of options which I cannot completely memorize.</p>

<p>The bash solution for &ldquo;I don&rsquo;t know what I am looking for&rdquo; is to
press the <strong>TAB</strong> key. This will bring up a list of suggestions for
how to complete the currently entered word. I thought using the
same thing for ArangoDB&rsquo;s command-line options would be nice, too.</p>

<h2>The solution</h2>

<p>It turned out that I needed to put a shell script that generates the
auto completion for <code>arangod</code> and all the other tools into <code>/etc/bash_completion.d</code>.
From there, the system will automatically pick it up when auto-completion
is initialized.</p>

<p>The script is rather simple. For example, to have auto-completion for
<code>arangosh</code> it would look like this:</p>

<p>```bash completion script example for arangosh
_arangosh()
{</p>

<pre><code>local cur prev opts
COMPREPLY=()
cur="${COMP_WORDS[COMP_CWORD]}"
prev="${COMP_WORDS[COMP_CWORD-1]}"
opts="--help --server.endpoint --server.username" # ...all the options go here

if [[ ${cur} == -* ]] ; then
    COMPREPLY=( $(compgen -W "${opts}" -- ${cur}) )
    return 0
fi
</code></pre>

<p>}</p>

<p>complete -F _arangosh arangosh
```</p>

<p>As can be seen, the variable <code>opts</code> should be filled with the list of possible
options. Determining the options for a binary can be achieved by invoking it with its
<code>--help</code> option, e.g.:</p>

<p>```bash figuring out program options
arangosh &mdash;help | grep -o &ldquo;^\ +&mdash;[a-z-]+(.[a-z0-9-]+)\?&rdquo; | xargs</p>

<h1>this will generate something like the following output:</h1>

<h1>&mdash;audit-log &mdash;chunk-size &mdash;configuration &mdash;help &mdash;no-auto-complete &mdash;no-colors &mdash;pager &mdash;pretty-print &mdash;prompt &mdash;quiet &mdash;temp-path &mdash;use-pager &mdash;javascript.check &mdash;javascript.current-module-directory &mdash;javascript.execute &mdash;javascript.execute-string &mdash;javascript.gc-interval &mdash;javascript.startup-directory &mdash;javascript.unit-tests &mdash;jslint &mdash;log.level &mdash;log.use-local-time &mdash;server.connect-timeout &mdash;server.database &mdash;server.disable-authentication &mdash;server.endpoint &mdash;server.password &mdash;server.request-timeout &mdash;server.ssl-protocol &mdash;server.username</h1>

<p>```</p>

<p>That has to be repeated for all binaries in the ArangoDB package (i.e. arangob, arangosh,
arangoimp, arangodump, arangorestore, and arangod).</p>

<p>As the available options might change over time, I wrote a script that extracts them
from the binaries and puts together the completions file. This script can be downloaded
<a href="/downloads/code/build-completions.sh">here</a>. The script expects the already-built ArangoDB
binaries to be located in the <code>bin</code> subdirectory. Provided that ArangoDB was compiled from
source, this should already be the case.</p>

<p>The script should then be run from the base directory:
<code>bash
build-completions.sh arangodb
</code>
This will write the completions script for all binaries into the file <code>arangodb</code>.
An already generated version for devel can be found <a href="/downloads/code/completions-devel">here</a>.
Completions for current 2.2 can be found <a href="/downloads/code/completions-2.2">here</a>.</p>

<p>To activate completions, copy the appropriate file into <code>/etc/bash_completion.d/arangodb</code>.
Note that completion may need to be re-initialized once in order for it to work:
<code>bash
. /etc/bash_completion.d/arangodb
</code></p>

<h2>Quick setup</h2>

<p>For the impatient: the following command should install the completions for
2.2 and activate them:
```bash activate completion for ArangoDB 2.2
sudo \
  wget -O /etc/bash_completion.d/arangodb \</p>

<pre><code>https://jsteemann.github.io/downloads/code/completions-2.2 &amp;&amp; . /etc/bash_completion.d/arangodb
</code></pre>

<p>```</p>

<p>To see it in action, type <code>arangosh --</code> and then press <strong>TAB</strong>.</p>

<h2>Other environments (MacOS etc.)</h2>

<p><em>Note: I have checked that the above works on Ubuntu and OpenSuSE. I have no idea whether this works
with other Linux distributions let alone other shells.</em></p>

<p>Some Linux/Unix distros do not have <code>/etc/bash_completion.d</code> at all. I was told MacOS is one
of them. For such environments, downloading and sourcing the completions script should work:
```bash activate completion without bash_completion.d
wget -O ~/arangodb-completions-2.2 \</p>

<pre><code>https://jsteemann.github.io/downloads/code/completions-2.2
</code></pre>

<p>. ~/arangodb-completions-2.2
```</p>

<p>This will enable the completions in the current shell. To enable them permanently, add the
completions script to your <code>.bashrc</code> file:
<code>bash adding completions to .bashrc
echo ". ~/arangodb-completions-2.2" &gt;&gt; ~/.bashrc
</code></p>
]]></content>
  </entry>
  
</feed>
