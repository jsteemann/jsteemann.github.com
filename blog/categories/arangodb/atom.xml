<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-03-10T17:08:56+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[More ES6 Features]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/26/more-es6-features/"/>
    <updated>2015-02-26T12:00:46+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/26/more-es6-features</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.5 comes with an upgraded version of V8, Google&rsquo;s open source JavaScript engine.</p>

<p>The built-in version of V8 has been upgraded from 3.29.54 to 3.31.74.1.</p>

<p>In addition to several already usable ES6 features (detailed in
<a href="https://jsteemann.github.io/blog/2014/12/19/using-es6-features-in-arangodb/">this blog</a>,
the following ES6 features are activated in ArangoDB 2.5 by default:</p>

<ul>
<li>iterators and generators</li>
<li>template strings</li>
<li>enhanced object literals</li>
<li>enhanced numeric literals</li>
<li>block scoping with <code>let</code> and constant variables using <code>const</code></li>
<li>additional String methods (such as <code>startsWith</code>, <code>repeat</code> etc.)</li>
</ul>


<p>The above features are available in ArangoDB 2.5, and can now be used for scripting purposes
in the ArangoShell and in server-side Foxx actions inside the database.</p>

<p>This blog post briefly explains the features provides some quick examples for using them.</p>

<!-- more -->


<h2>Iterators and generators</h2>

<p>Iterator and generator support was optional in 2.4, but is turned on by default since 2.5.</p>

<p>For everyone who is not familiar with generators in JavaScript, here&rsquo;s how they work:</p>

<p>Generators are special functions tagged with an asterisk (<code>*</code>). Values are returned to the
caller using the <code>yield</code> keyword:</p>

<p><code>js a simple generator that generates two values                                                            
function* generate () {                                                                                       
  yield 23;                                                                                                   
  yield 42;                                                                                                   
}                                                                                                            
</code></p>

<p>Calling the function with initialize/reset the generator. Calling the <code>next()</code> method on
the generator&rsquo;s initial call return value produces the next sequence element. The element
is returned in a <code>value</code> attribute. The <code>done</code> attribute indicates whether the sequence
has come to an end:</p>

<p><code>js invoking the generator
var generator = generate();                                                                                   
console.log(generator.next());  /* { "value" : 23, "done" : false } */                                        
console.log(generator.next());  /* { "value" : 42, "done" : false } */                                        
console.log(generator.next());  /* { "value" : undefined, "done" : true } */                                  
</code></p>

<p>Sequences produced by generators can also be consumed via a <code>for...of</code> loop:</p>

<p>```js consuming all values from a generator function
var generator = generate();</p>

<p>for (var value of generator) {
  console.log(value);
}
```</p>

<p>In general, every object that is iteratable can be consumed using the <code>of</code> operator.
Some built-in objects provide pre-defined iterators (e.g. <code>Map.keys()</code> or <code>Map.values()</code>),
but you can also create iterators for your own objects:</p>

<p>```js creating an iterator for an object
function Sentence (text) {
  this.text = text;
}</p>

<p>Sentence.prototype[Symbol.iterator] = function*() {
  var regex = /\S+/g;
  var text = this.text;
  var match;
  while (match = regex.exec(text)) {</p>

<pre><code>yield match[0]; 
</code></pre>

<p>  }
};</p>

<p>var sentence = new Sentence(&ldquo;The quick brown fox jumped over the lazy dog&rdquo;);
for (var word of sentence) {
  console.log(word);
}
```</p>

<h2>Template strings</h2>

<p>I know there are query string generators and such, but for the sake of the example, let&rsquo;s assume you
wanted to write a query string in JavaScript. You might end up with something like this:</p>

<p><code>js multi-line query string
var query =
  'FOR doc IN users\n' +
  '  FILTER doc.name == @name\n' +
  '  RETURN doc\n';
</code></p>

<p>This is hardly legible, and it is also very prone to errors.</p>

<p>ES6 template strings provide a way to define multi-line string literals in a much easier and simpler way.
Here&rsquo;s how to do it (note the backticks instead of the regular string quotes):</p>

<p><code>js using a multi-line template string
var query = `
FOR doc IN users
  FILTER doc.name == @name
  RETURN doc
`;
</code></p>

<p>Template strings also support value substitution, so you could even write something like this, too:
```js value substitution in template strings
var name = &ldquo;AQL injection attempt \&rdquo; OR true OR \&ldquo;&rdquo;;</p>

<p>var query = <code>
FOR doc IN users
  FILTER doc.name == ${JSON.stringify(name)}
  RETURN doc
</code>;
```</p>

<p>Note that while value substitution in template strings in convenient, you still have to be careful with
user-generated values. Otherwise you might be subject to value injection attacks, as you would be with
every other form of improper user value handling.</p>

<h2>Enhanced object literals</h2>

<p>Save some time when definining objects:</p>

<p>```js using enhanced object literals
var name = &ldquo;foobar&rdquo;;</p>

<p>myObject = {
  type : &ldquo;myType&rdquo;,   /<em> always worked </em>/
  name,              /<em> same as &ldquo;name&rdquo; : name </em>/
  save () {          /<em> same as &ldquo;save&rdquo; : function () &hellip; </em>/</p>

<pre><code>console.log("save called!"); 
</code></pre>

<p>  }
};</p>

<p>{
  &ldquo;type&rdquo; : &ldquo;myType&rdquo;,
  &ldquo;name&rdquo; : &ldquo;foobar&rdquo;,
  &ldquo;save&rdquo; : [Function &ldquo;console.log("save called!&rdquo;);&ldquo; &hellip;]
}
```</p>

<p>As can be seen above, enhanced object literal declarations can save some typing and reduce redundancies
in the code. Unfortunately we still cannot use object key names generated from expressions:</p>

<p><code>js does not work yet
myObject = {
  [ "foo" + bar" ] : "foobar"
};
</code></p>

<h2>Enhanced numeric literals</h2>

<p>Numeric values can now be specified in binary or octal if required:</p>

<p><code>js numeric literals                                                                                        
var life = 0b101010;          /* binary, 42 in decimal */                                                     
var filePermissions = 0o777;  /* octal, 511 in decimal */                                                     
</code></p>

<h2>Block scoping</h2>

<p>As a mostly C++ programmer, I am always puzzled about the scoping of JavaScript variables.
In the following example, variable <code>x</code> does not only live inside the curly brackets block in which
it was declared, but also afterwards:</p>

<p>```
function work () {
  {</p>

<pre><code>var x = 1;
</code></pre>

<p>  }
  return x;
}
```</p>

<p>The reason is that the curly brackets around <code>var x = 1;</code> are not a scope at all in traditional
JavaScript. This sucks, because variables can linger around in programs longer than necessary,
leading to unwanted side-effects.</p>

<p>With block-level scopes, this can be fixed. To use it, introduce variables not with the <code>var</code>
keyword, but with <code>let</code>. <code>let</code> only works in strict mode, so make sure your function or module
uses it.</p>

<p>Now, with block-level scoping, the above snippet looks like this:
```
function work () {
  &ldquo;use strict&rdquo;;
  {</p>

<pre><code>let x = 1; 
</code></pre>

<p>  }
  return x;
};
```</p>

<p>And it will actually produce an error when trying to access variable <code>x</code> in the <code>return</code> statement.
The reason is that the life of <code>x</code> was over as soon as its scope was left. The scope of variable <code>x</code> is
now only the one with the <code>let</code> declaration inside the curly brackets.</p>

<p>Someone else said &ldquo;<em>let is the new var</em>&rdquo;, and I think there&rsquo;s not much to add.</p>

<p>Additionally, the <code>const</code> keyword can be used to define a read-only variables. Trying to re-define a
constant will produce an error in strict mode (the desired behavior), and do nothing in non-strict mode.
Another reason to use the strict mode.</p>

<h2>Additional String methods</h2>

<p><code>String</code> objects now provide extra built-in methods:</p>

<ul>
<li><code>string.startsWith(what)</code></li>
<li><code>string.endsWith(what)</code></li>
<li><code>string.includes(what)</code></li>
<li><code>string.repeat(count)</code></li>
<li><code>string.normalize(method)</code></li>
<li><code>string.codePointAt(position)</code></li>
</ul>


<p>There is also an extra &ldquo;static&rdquo; method:</p>

<ul>
<li><code>String.fromCodePoint(codePoint)</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AQL Improvements in 2.5]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/26/aql-improvements-in-25/"/>
    <updated>2015-02-26T10:35:31+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/26/aql-improvements-in-25</id>
    <content type="html"><![CDATA[<p>Contained in 2.5 are some small but useful AQL language improvements plus several AQL optimizer improvements.</p>

<p>We are working on further AQL improvements for 2.5, but work is still ongoing.
This post summarizes the improvements that are already completed and will be shipped with the initial ArangoDB
2.5 release.</p>

<!-- more -->


<h1>Language improvements</h1>

<h2>Dynamic attribute names</h2>

<p>Often the need arises to dynamically name object attributes in return values.
In AQL this was not directly possible so far, though there were some workarounds available to achieve about
the same result. <a href="https://docs.arangodb.com/cookbook/UsingDynamicAttributeNames.html">This recipe</a> summarizes
the options that are available to pre-ArangoDB 2.5 users.</p>

<p>With ArangoDB 2.5, dynamic attribute names can be constructed much more easily and flexibly. Object
attribute names in ArangoDB 2.5 can be specified using static string literals, bind parameters,
and dynamic expressions.</p>

<p>Dynamic expressions are most interesting, and to disambiguate them from other regular string literal attribute
names, dynamic attribute names need to be enclosed in square brackets (<code>[</code> and <code>]</code>). I have written about
that before in <a href="http://jsteemann.github.io/blog/2015/02/03/using-dynamic-attribute-names-in-aql/">this blog</a>.</p>

<p>Here is an example query that uses the new syntax:</p>

<p><code>plain example query using dynamic attribute names
FOR i IN [ 17, 23, 42, 83 ]
  RETURN { [ CONCAT('value-of-', i, ' * ', i) ] : i * i }
</code></p>

<p>This will produce:</p>

<p>```json query result
[
  {</p>

<pre><code>"value-of-17 * 17" : 289 
</code></pre>

<p>  },
  {</p>

<pre><code>"value-of-23 * 23" : 529 
</code></pre>

<p>  },
  {</p>

<pre><code>"value-of-42 * 42" : 1764 
</code></pre>

<p>  },
  {</p>

<pre><code>"value-of-83 * 83" : 6889 
</code></pre>

<p>  }
]
```</p>

<h2>Functions added</h2>

<p>The following AQL functions have been added in 2.5:</p>

<ul>
<li><code>MD5(value)</code>: produces the MD5 hash of <code>value</code></li>
<li><code>SHA1(value)</code>: produces the SHA1 hash of <code>value</code></li>
<li><code>RANDOM_TOKEN(length)</code>: produces a pseudo-random string of the specified length.
 Such strings can be used for id or token generation. Tokens consist only of letters
 (lower and upper case) plus digits, so they are also URL-safe</li>
</ul>


<h1>Optimizer improvements</h1>

<h2>Optimizer rules</h2>

<p>The following AQL optimizer rules have been added in ArangoDB 2.5:</p>

<ul>
<li><p><code>propagate-constant-attributes</code></p>

<p>This rule will look inside <code>FILTER</code> conditions for constant value equality comparisons,
and insert the constant values in other places in <code>FILTER</code>s. For example, the rule will
insert <code>42</code> instead of <code>i.value</code> in the second <code>FILTER</code> of the following query:</p>

<pre><code>FOR i IN c1 
  FOR j IN c2 
    FILTER i.value == 42 
    FILTER j.value == i.value 
    RETURN 1
</code></pre></li>
<li><p><code>move-calculations-down</code></p>

<p>This rule moves calculations down in the execution plan as far as possible. The intention
is to move calculations beyond filters, in order to avoid calculations and computations
for documents that will be filtered away anyway.</p>

<p>If a query contains a lot of computations and a lot of documents will be skipped because
of filters, this rule might provide a big benefit.</p>

<p>A more detailed example is provided in
<a href="http://jsteemann.github.io/blog/2015/01/31/yaor-yet-another-optimizer-rule/">this post</a>.</p></li>
</ul>


<p>The already existing optimizer rule <code>use-index-for-sort</code> was also improved in the following way:</p>

<ul>
<li><p>the rule can now remove <code>SORT</code>s also in case a non-sorted index (i.e. a hash index) is used
for an equality lookup and all sort attributes are covered by the index.</p></li>
<li><p>the rule can also remove <code>SORT</code>s in case the sort critieria excludes the left-most index attributes,
but the left-most index attributes are used in a <code>FILTER</code> for equality-only lookups.</p>

<p>Here is an example that will use an existing skiplist index on [ <code>value1</code>, <code>value2</code> ] for sorting,
removing the extra <code>SORT</code>:</p>

<pre><code>FOR doc IN collection 
  FILTER doc.value1 == 1 
  SORT doc.value2 
  RETURN doc
</code></pre></li>
</ul>


<h2>Index usage</h2>

<p>The AQL optimizer now supports <a href="https://www.arangodb.com/2015/02/24/sparse-indexes-in-arangodb">sparse indexes</a>,
a feature added in 2.5.</p>

<p>It will use them automatically in queries when appropriate and when safe. Sparse indexes do exclude certain
documents purposely, so the optimizer always has to figure out whether it can use a sparse index to satisfy
a given <code>FILTER</code> condition.</p>

<p>The optimizer will also take into account index selectivity estimates when there are multiple index candidates.</p>

<h2>Estimates</h2>

<p>The optimizer estimates for the number of documents to be returned by a query or a subquery are more accurate
now for several types of queries. For example, if the optimizer can use a primary key, an edge index, or a hash
index in a given query part, it will use the index selectivity estimates for calculating the number of return
documents.</p>

<p>These estimates will be a lot more accurate than the previoulsy hard-coded filtering factors, and can lead to
better optimizer decisions and reporting (because estimates are returned in <code>explain</code> results, too).</p>

<h2>Memory savings</h2>

<p>Finally, the optimizer will now detect if the data-modification part in a data-modification query
can be executed in lockstep with the data-retrieval part of the same query. Previously, a data-modification
query always executed its data-retrieval part first, and then executed its data-modification part.
This could have resulted in big intermediate result sets which to retrieval part constructed in order
to pass them to the modification part of the query.</p>

<p>Here&rsquo;s an example query:</p>

<p><code>plain data-modification query
FOR doc IN test
  INSERT doc INTO backup
</code></p>

<p>In the above query, the <code>FOR</code> loop is the retrieval part, and the <code>INSERT</code> is the modification part.
The optimizer in 2.5 will check if the two parts of the query are independent, and if it turns out they are,
will execute them in lockstep instead of sequentially.</p>

<p>The execution in lockstep is not necessarily faster than sequential execution, but it can save lots of
memory if the data-retrieval part constructed big intermediate result sets.</p>

<h1>Miscellaneous changes</h1>

<p>The AQL query execution statistics now also provide an attribute <code>filtered</code>. Its value indicates how many
documents were filtered by <code>FilterNode</code>s in the AQL query. This can be used as an indicator for whether
indexes should be added, and for how effective indexes are used for filtering.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using ArangoDB as a Logstash Output]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/05/using-arangodb-as-a-logstash-output/"/>
    <updated>2015-02-05T23:39:23+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/05/using-arangodb-as-a-logstash-output</id>
    <content type="html"><![CDATA[<p>Inspired by a question on <a href="http://stackoverflow.com/questions/28314711/logstash-output-for-arangodb">StackOverflow</a>,
I did some investigation about how to make <a href="http://www.elasticsearch.org/overview/logstash/">Logstash</a>
send log events to ArangoDB.</p>

<p>There is no dedicated Logstash output plugin for ArangoDB on the
<a href="https://github.com/logstash-plugins">Logstash plugins page</a>, so I had already accepted to write
one on my own.</p>

<p>Browsing the plugins page for inspiration, I found an
<a href="https://github.com/logstash-plugins/logstash-output-http">HTTP output plugin for Logstash</a>.
It seems to be general enough that it can send the log event in JSON format to any HTTP-speaking backend.</p>

<p>ArangoDB&rsquo;s API is JSON over HTTP, so it sounded like a perfect match. I briefly tried it out and
it seemed to work fine.</p>

<!-- more -->


<p>Here are the steps I carried out to connect the two:</p>

<h2>Prepare an ArangoDB server</h2>

<p>I started an ArangoDB server with default configuration (binding to IP address 127.0.0.1 and port
8529). I then used the ArangoShell to create a collection named <code>logstash</code>:</p>

<p><code>js
db._create("logstash");
</code></p>

<p>This collection will be used for storing the log events sent by Logstash.</p>

<h2>Download Logstash</h2>

<p>In order to run Logstash, you must have Java installed, which I assume you already have.</p>

<p>Now it&rsquo;s time to download Logstash. You can download and unpack it with the command following.
The current version is 1.5.0 beta1 (<strong>warning: 100 MB download!</strong>):</p>

<p><code>bash
wget "http://download.elasticsearch.org/logstash/logstash/logstash-1.5.0.beta1.tar.gz"
tar xvfz logstash-1.5.0.beta1.tar.gz
cd Downloads/logstash-1.5.0.beta1
</code></p>

<h2>Connecting Logstash with ArangoDB</h2>

<p>We are now ready to start Logstash. I&rsquo;ll start it in a mode that will send all input from
stdin as log events to ArangoDB. I am using the <code>stdin</code> input plugin, and the <code>http</code> output
plugin for this. The <code>http</code> output plugin needs to know the URL to send the log events to.</p>

<p>The URL is ArangoDB&rsquo;s base URL plus the REST API method for storing a single document, with
the name of the target collection (<code>logstash</code>) appended.</p>

<p>Here is the full command:</p>

<p><code>bash
bin/logstash -e 'input { stdin } } output { http { http_method =&gt; "post" url =&gt; "http://127.0.0.1:8529/_api/document?collection=logstash" format =&gt; "json" } }'
</code></p>

<p>Logstash may need a few seconds to start. The HTTP plugin will print a message about itself
being a milestone 1 release only, but it works. Anything entered in the terminal should now be
sent as a log event to ArangoDB.</p>

<p>For example, type <code>fingers crossed!</code> and hit enter:</p>

<p><code>
Using milestone 1 output plugin 'http'. This plugin should work, but would benefit from use by folks like you. Please let us know if you find bugs or have suggestions on how to improve this plugin.  For more information on plugin milestones, see http://logstash.net/docs/1.5.0.beta1/plugin-milestones {:level=&gt;:warn}
fingers crossed!
</code></p>

<p>Let&rsquo;s check if the log event made it into ArangoDB. I have used the ArangoShell for this:</p>

<p>```js
db.logstash.toArray()
[
  {</p>

<pre><code>"_id" : "logstash/3507690866496", 
"_key" : "3507690866496", 
"_rev" : "3507690866496", 
"@version" : "1", 
"host" : "kalk", 
"message" : "fingers crossed!", 
"@timestamp" : "2015-02-05T23:17:39.982Z" 
</code></pre>

<p>  }
]
```</p>

<h3>Querying log events</h3>

<p>So we&rsquo;re getting log events in from Logstash.</p>

<p>We can use AQL to query the received log events in ArangoDB. But before we run a query,
we probably want to index the <code>@timestamp</code> attribute of the events, so we can efficiently
find and filter them by date and time:</p>

<p><code>js
db.logstash.ensureSkiplist("@timestamp");
</code></p>

<p>Now we can run the following AQL query to find the latest 5 log events:</p>

<p><code>plain
FOR l IN logstash
  FILTER l.`@timestamp` &lt;= '2099' /* arbitrary max value */
  SORT l.`@timestamp` DESC
  LIMIT 5
  RETURN l
</code></p>

<p>Note: the <code>@timestamp</code> attribute name needs to be enclosed in backticks because a <code>@</code> prefix
is used to designate bind parameters in AQL. Enclosing the names in backticks will make AQL treat
them as attribute name literals.</p>

<p>For the simple types of events triggered by the <code>stdin</code> input plugin, this is already sufficient.
However, log events may look different, depending on the type of input plugins that are used. For
other inputs, other attributes may need to be indexed, too.</p>

<h3>Adjusting IP, port and authentication</h3>

<p>Above I have used the default configuration of ArangoDB, that is IP 127.0.0.1, port 8529, and no
authentication. You probably want to change this.</p>

<p>To make ArangoDB listen on any other IP address or port, change the <code>endpoint</code> setting in its
configuration file <code>/etc/arangod.conf</code>. You may also want to set the <code>disable-authentication</code>
flag to <code>false</code>, meaning authentication is turned on.</p>

<p><code>
[server]
endpoint = tcp://192.168.173.13:9999
disable-authentication = false
</code></p>

<p>Before activating the new configuration, let&rsquo;s create a dedicated ArangoDB user <code>logstash</code>.
I will also change the default password of the <code>root</code> user. The following ArangoShell commands
do this:</p>

<p><code>js
require("org/arangodb/users").save("logstash", "secret-logging", true);
require("org/arangodb/users").save("root", "nobody-will-ever-guess", true);
</code></p>

<p>To make logstash use the above settings, we have to adjust the command-line:</p>

<p><code>bash
bin/logstash -e 'input { stdin } } output { http { http_method =&gt; "post" url =&gt; "http://logstash:secret-logging@192.168.173.13:9999/_api/document?collection=logstash" format =&gt; "json" } }'
</code></p>

<h3>Pitfalls</h3>

<p>Though Logstash itself can write a logfile (<code>--log</code> option) and can provide debug information
(<code>--debug</code>), I did not get it log or print errors when misconfiguring the HTTP output plugin.
For example, specifying a wrong target URL will make all HTTP requests from Logstash to ArangoDB
silently fail, with the log events being lost if not stored elsewhere.</p>

<p>Maybe this is configurable somewhere, but then I didn&rsquo;t find it. It is also possible that this
will be fixed in some future release.</p>

<h3>Disclaimer</h3>

<p>Please feel free to use this blog as a starting point but not as an endorsement.</p>

<p>Though I think it will work perfectly, I am not at all an expert for Logstash or its plugins.
I didn&rsquo;t spend much time with it yet, and I may have overlooked important things. So should you
be interested in using it, please conduct your own tests first.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Dynamic Attribute Names in AQL]]></title>
    <link href="http://jsteemann.github.io/blog/2015/02/03/using-dynamic-attribute-names-in-aql/"/>
    <updated>2015-02-03T00:12:39+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/02/03/using-dynamic-attribute-names-in-aql</id>
    <content type="html"><![CDATA[<p>On our mailing list, there is quite often the question whether attribute names in objects
returned from AQL queries can be made dynamic.</p>

<p>Here&rsquo;s a (non-working) example query for this:</p>

<p><code>plain example query that does not work
FOR doc IN collection
  RETURN { doc.type : doc.value }
</code></p>

<p>The intention in the above query obviously is to use the dynamic value from <code>doc.type</code> as
an attribute name in the result object and not to have an attribute named <code>"doc.type"</code>. This
feature is probably in the top 20 of the most-often wished features.</p>

<!-- more -->


<p>However, the above query won&rsquo;t even parse. The AQL grammar only allows string values
left of the colon in an object definition. Non-quoted strings are allowed there too, and are
implicitly turned into quoted strings. It works similar to how object literals are defined
in JavaScript:</p>

<p><code>plain using unquoted and quoted string attribute names
RETURN {
  foo : "bar",
  "baz" : "qux"
}
</code></p>

<p>Why not allow arbitrary expression left of the colon? The reason is simple: this would cause
ambiguity and probably have side-effects. For an example, have a look at the following query:</p>

<p><code>plain which attribute name to use here?
FOR doc IN collection
  LET type = doc.type;
  RETURN { type : doc.value }
</code></p>

<p>If the <code>type</code> attribute name inside the object definition is interpreted as a string literal
as it currently is an AQL (and always was), then the resulting attribute name is just <code>"type"</code>.</p>

<p>If the <code>type</code> attribute name would now be intepreted as an expression, it would get the value
that was assigned to the variable <code>type</code> by the <code>LET</code> statement. Removing the <code>LET</code> from the
query would change the attribute name in the result back to the string literal <code>"type"</code>.</p>

<p>The ambiguity could be solved by telling the parser what to do in such cases. While technically
this could be working, I think it may have too many unintended side-effects. I already mentioned
that introducing a <code>LET</code> statement into the query would change the attribute name in the result.
The same could also happen if a collection named <code>type</code> was added to the query. And it would
break compatibility with existing queries.</p>

<p>JavaScript has the same problem, and it wasn&rsquo;t solved portably yet. However, there is a proposal
for ES6 that suggests enclosing attribute name expressions in <code>[</code> and <code>]</code>.</p>

<p>To me, this looks like a good solution for the problem. It&rsquo;s two bytes more when keying in
queries, but the syntax is easy and explicit. There are no ambiguities.</p>

<p>I prototyped this solution for AQL, so I could write:</p>

<p>```plain query using dynamic attribute names
FOR i IN 1..5
  RETURN {</p>

<pre><code>[ CONCAT('test', i) ] : i, 
[ SUBSTITUTE(CONCAT('i is ', (i &lt;= 3 ? 'small' : 'not small')), { ' ' :  '_' } ) ] : i 
</code></pre>

<p>  }</p>

<p>[
  {</p>

<pre><code>"test1" : 1, 
"i_is_small" : 1 
</code></pre>

<p>  },
  {</p>

<pre><code>"test2" : 2, 
"i_is_small" : 2 
</code></pre>

<p>  },
  {</p>

<pre><code>"test3" : 3, 
"i_is_small" : 3 
</code></pre>

<p>  },
  {</p>

<pre><code>"test4" : 4, 
"i_is_not_small" : 4 
</code></pre>

<p>  },
  {</p>

<pre><code>"test5" : 5, 
"i_is_not_small" : 5 
</code></pre>

<p>  }
]
```</p>

<p>I ran a few queries with this, and they seemed to work. <del>However, I haven&rsquo;t
committed the feature yet. There might still be cases in which it doesn&rsquo;t work. Tests
for the feature are also still missing. I hope I can finalize the implementation soon
so it becomes available in some release.</del></p>

<p><strong>UPDATE</strong>: tests have been added, and the feature has been committed in devel. It is
included in ArangoDB since version 2.5.</p>

<p>Everyone is welcome to try it out already!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YAOR - Yet Another Optimizer Rule]]></title>
    <link href="http://jsteemann.github.io/blog/2015/01/31/yaor-yet-another-optimizer-rule/"/>
    <updated>2015-01-31T18:51:30+01:00</updated>
    <id>http://jsteemann.github.io/blog/2015/01/31/yaor-yet-another-optimizer-rule</id>
    <content type="html"><![CDATA[<p>A quick post that showcases the new optimizer rule <code>move-calculations-down</code> that was added
in ArangoDB 2.5 (current <code>devel</code> branch).</p>

<!-- more -->


<p>Consider the following simple query:</p>

<p><code>plain Example query
FOR doc IN test
  LET calculated = CONCAT('foo', doc.value)
  FILTER doc.value &lt; 100
  RETURN calculated
</code></p>

<p>If no indexes are present in collection <code>test</code>, the execution plan will look
like this in 2.4:</p>

<p><img src="/downloads/screenshots/explain-24.png"></p>

<p>The plan can be improved a bit. While there are no indexes to exploit, the calculation
for <code>calculated</code> can be pushed beyond the execution of the <code>FILTER</code>. This will be very
beneficial if the calculation is expensive and the <code>FILTER</code> can prune a lot of documents.</p>

<p>In 2.5, there is an optimizer rule <code>move-calculations-down</code> that will do this. It will
move all eligible calculations as far down in the plan as possible. A calculation obviously can
be moved down a step only if the successor step does not depend on its result.</p>

<p>Additionally, a calculation will not be moved beyond a <code>COLLECT</code> operation, because
<code>COLLECT</code> changes which variables are visible in a scope. Finally, a calculation will not
be moved down inside a <code>FOR</code> loop, in order to avoid repeated calculations.</p>

<p>In the example query, the step following the <code>LET</code> calculation was a <code>FILTER</code>.
The filter condition does not depend on the result of <code>calcuated</code>, so the calculation is
eligible for being moved down.</p>

<p>The resulting execution plan will look like this in 2.5:</p>

<p><img src="/downloads/screenshots/explain-25.png"></p>

<p>As we can see from the changed id sequence on the left, the calculation was moved down.</p>

<p>What does this buy us? For the simple query above, with the <code>test</code> collection containing
100.000 documents with <code>value</code> ranging from 0 to 99,999, the results are as follows:</p>

<ul>
<li>if the filter leaves only 0.1 % of the documents pass, execution time goes down from
0.87 seconds to 0.17 seconds thanks to the rule</li>
<li>if the filter lets 1 % of the documents pass, execution time is 0.21 seconds with the rule,
and 0.91 without</li>
<li>if the filter lets 10 % of the documents pass, execution time is 0.25 s, vs. 0.91 seconds
without.</li>
<li>if the filter lets all documents pss, there is no difference in execution time</li>
</ul>


<p>This is quite a nice speedup, especially when taking into account how simple the optimizer
rule is. The effects may be even greater for queries that contain multiple calculations
that can be pushed beyond filters, or for more expensive calculations.</p>

<p>Of course the best solution for the above query would be to use a skiplist index on
<code>value</code>, but that&rsquo;s a different story. The optimizer rule shown here is orthogonal to
using indexes, so queries already using indexes might still benefit from the rule if they
contain additional calculations or further filters which cannot be satifisfied by indexes.</p>
]]></content>
  </entry>
  
</feed>
