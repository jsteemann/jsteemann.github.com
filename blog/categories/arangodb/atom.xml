<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2014-11-07T22:19:22+01:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Improved Non-unique Hash Indexes in 2.3]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes/"/>
    <updated>2014-11-07T20:51:12+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/07/improved-non-unique-hash-indexes</id>
    <content type="html"><![CDATA[<p>With ArangoDB 2.3 now getting into the <a href="https://www.arangodb.com/install-beta-version">beta stage</a>,
it&rsquo;s time to spread the word about new features and improvements.</p>

<p>Today&rsquo;s post will be about the changes made to non-unique hash
indexes.</p>

<!-- more -->


<p>Hash indexes allow looking up documents quickly if the indexed
attributes are all provided in a search query. They are not
suitable for range queries, but are the perfect choice if equality
comparisons are all that&rsquo;s needed.</p>

<p>Hash indexes have been available in ArangoDB ever since. There
have always been two variants of them:</p>

<ul>
<li>unique hash indexes</li>
<li>non-unique hash indexes</li>
</ul>


<p>There wasn&rsquo;t much to be done for unique hash indexes, and so there
haven&rsquo;t been any changes to them in 2.3. However, the non-unique
hash indexes were improved significantly in the new version.</p>

<p>The non-unique indexes already performed quite well if most of the
indexed values were unique and only few repetitions occurred. But their
performance suffered severly if the indexed attribute values repeated
a lot &ndash; that is, when the indexed value had a <strong>low cardinality</strong> and thus
the index had a <strong>low selectivity</strong>.</p>

<p>This was a problem because it slowed down inserting new documents into
a collection with such an index. And it also slowed down loading collections
with low cardinality hash indexes.</p>

<p>I am happy to state that in ArangoDB 2.3 this has been fixed, and the insert
performance of non-unique hash indexes has been improved significantly.
The index insertion time now scales quite well with the number
of indexed documents regardless of the cardinality of the indexed
attribute.</p>

<p>Following are a few measurements of non-unique hash index insertion
times from ArangoDB 2.3, for different cardinalities of the indexed
attribute.</p>

<p>The times reported are the net non-unique hash index
insertion times (the documents were present already, just the index
was created on them and index creation time was measured).</p>

<p>Let&rsquo;s start with a not too challenging case: indexing documents in
a collection with 100,000 different index values (<em>cardinality 100,000</em>):</p>

<p><code>text index insertion times for cardinality 100,000
number of documents:    128,000    =&gt;    time:   0.144 s
number of documents:    256,000    =&gt;    time:   0.231 s
number of documents:    512,000    =&gt;    time:   0.347 s
number of documents:  1,024,000    =&gt;    time:   0.694 s
number of documents:  2,048,000    =&gt;    time:   1.379 s
</code></p>

<p>The picture doesn&rsquo;t change much when reducing the cardinality
by a factor or 10 (i.e. <em>cardinality 10,000</em>):</p>

<p><code>text index insertion times for cardinality 10,000
number of documents:    128,000    =&gt;    time:   0.169 s
number of documents:    256,000    =&gt;    time:   0.194 s
number of documents:    512,000    =&gt;    time:   0.355 s
number of documents:  1,024,000    =&gt;    time:   0.668 s
number of documents:  2,048,000    =&gt;    time:   1.325 s
</code></p>

<p>Let&rsquo;s again divide cardinality by 10 (now <em>cardinality 1,000</em>):</p>

<p><code>text index insertion times for cardinality 1,000
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.152 s
number of documents:    512,000    =&gt;    time:   0.261 s
number of documents:  1,024,000    =&gt;    time:   0.524 s
number of documents:  2,048,000    =&gt;    time:   0.934 s
</code></p>

<p><em>Cardinality 100</em>:</p>

<p><code>text index insertion times for cardinality 100
number of documents:    128,000    =&gt;    time:   0.114 s
number of documents:    256,000    =&gt;    time:   0.148 s
number of documents:    512,000    =&gt;    time:   0.337 s
number of documents:  1,024,000    =&gt;    time:   0.452 s
number of documents:  2,048,000    =&gt;    time:   0.907 s
</code></p>

<p><em>Cardinality 10</em>:</p>

<p><code>text index insertion times for cardinality 10
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    256,000    =&gt;    time:   0.327 s
number of documents:    512,000    =&gt;    time:   0.239 s
number of documents:  1,024,000    =&gt;    time:   0.442 s
number of documents:  2,048,000    =&gt;    time:   0.827 s
</code></p>

<p>Finally we get to <em>cardinality 1</em>, the definitive indicator
for the index being absolutely useless.</p>

<p>Let&rsquo;s create it anyway:</p>

<p><code>text index insertion times for cardinality 1
number of documents:    128,000    =&gt;    time:   0.130 s
number of documents:    128,000    =&gt;    time:   0.095 s
number of documents:    256,000    =&gt;    time:   0.146 s
number of documents:    512,000    =&gt;    time:   0.246 s
number of documents:  1,024,000    =&gt;    time:   0.445 s
number of documents:  2,048,000    =&gt;    time:   0.925 s
</code></p>

<p>All indexed values were numeric. In absolute terms, indexing string
values will be slower than indexing numbers, but insertion should still
scale nicely with the number of documents as long as everything fits
in RAM.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Test Data]]></title>
    <link href="http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data/"/>
    <updated>2014-11-04T22:14:21+01:00</updated>
    <id>http://jsteemann.github.io/blog/2014/11/04/setting-up-test-data</id>
    <content type="html"><![CDATA[<p>Today I was asked to look at code that was supposed to read data
from a MySQL data source, process it and then import it into ArangoDB.</p>

<p>To run and debug the code I had to have some MySQL data source. So I
thought I&rsquo;d quickly set up a simple example table with a few rows.
It turned out that this took more time than what I had expected.</p>

<p>Maybe I&rsquo;m spoilt by JavaScript-enabled, schema-free databases where
creating such test setups is so much easier.</p>

<!-- more -->


<p>I worked with MySQL databases in production for 10+ years and spent
much time working with the mysql client. I always liked MySQL, but in
the past few years, I was driven away from it and lost contact.
Instead, I got sucked into the NoSQL landscape and enjoy it pretty much.</p>

<p>Getting back to the original problem: I needed some MySQL table with a
few thousand rows for a test. It turned out I didn&rsquo;t even have MySQL
installed on my computer, so I needed to install it first.</p>

<p>After setting up the MySQL server, I created a table <code>examples</code> for
storing my test data:</p>

<p><code>sql
CREATE DATABASE test;
USE test;
CREATE TABLE examples (attribute1 VARCHAR(20), attribute2 VARCHAR(20));
</code></p>

<p>Not really the black belt of schema design, but good enough for a quick
test.</p>

<p>Now the table needed some rows. 100,000 rows should be enough. I wrote
some bash script to create them as there is no sane way to do this with
the MySQL client alone:</p>

<p><code>``bash
for i in</code>seq 1 100000`
  do</p>

<pre><code>echo "INSERT INTO examples VALUES (\"test$i\", \"test$i\");" &gt;&gt; import.sql 
</code></pre>

<p>  done
```</p>

<p>Time to import the data!</p>

<p><code>bash
mysql -u user test &lt; import.sql
</code></p>

<p>At first I was a bit surprised this command did not return instantly. I let it
run for about a minute, and then began checking the import progress with a second mysql
client. It turned out only very few records had been imported, and the import
script continued to create only around 30-35 records per second.</p>

<p>Seems I had forgotten that I am working with a No-NoSQL database, with full
ACID semantics for everything. My import file contained 100,000 <code>INSERT</code>
statements, so I was asking to perform 100,000 transactions and fsync operations.
That import would have taken forever with my slow HDD!</p>

<p>I quickly changed the InnoDB setting to make it commit only about once per second:
<code>
mysql&gt; SET GLOBAL innodb_flush_log_at_trx_commit = 2;
Query OK, 0 rows affected (0.00 sec)
</code></p>

<p>Now the import finished in 7 seconds.</p>

<p>I finally got the data in MySQL, but overall it took me about 10 minutes to get
it done. Probably a bit less if I still were an active user of MySQL and had
remembered the default behavior right from the start.</p>

<p>Still, my feeling is that it takes too much time to get something so simple
done.</p>

<p>I don&rsquo;t blame the database for trying to commit all 100,000 single-row
<code>INSERT</code> operations and fsync them to disk. It simply cannot know if the data
are important or just throw-away test records.</p>

<p>But there are other reasons: I had to write a bash script to produce the
test data, as there is no sane way to do this with the MySQL client alone.
Writing bash scripts is fine, and in general I like it, but I don&rsquo;t want to
do it for a dead-simple test setup.</p>

<p>And by the way, what if it turns out that I need to generate slightly more
complex test data? In the MySQL case I probably would have resorted to sed
or awk or would have thrown away my bash script and had rewritten it in some
other language. So I would have wasted even more time.</p>

<p>I personally prefer the ability to use a scripting language for such tasks.
JavaScript is ubiquituous these days, and I want to use it in a database&rsquo;s
command-line client.</p>

<p>For example, here&rsquo;s how the test setup would look like in the ArangoShell:
<code>js
db._create("examples");
for (i = 0; i &lt; 100000; ++i) {
  db.examples.save({ attribute1: "test" + i, attribute2: "test" + i });
}
</code>
I find this much easier to use: it allows to do everything in one place,
removing the need to write another script that prepares a data file or an
SQL file first.</p>

<p>As a bonus, using a programming language is much more flexible and powerful.
If I needed to generate slightly more complex test data, I can just do it,
adjust the JavaScript code and re-run it.</p>

<p>Even more annoying to me is that I needed to provide a schema for the
table first. I could have got away with declaring all text fields as
<code>VARCHAR(255)</code> or <code>TEXT</code> so I can at least ignore string
length restrictions. But I still need to type in the table schema
once, even if it feels completely useless for this particular use case.</p>

<p>It would get even more annoying if during my test I noticed I needed more
or other columns. Then I would need to adjust the table schema using <code>ALTER TABLE</code>
or adjust the <code>CREATE TABLE</code> statement and run it again, keeping me
away from the original task.</p>

<p>Maybe using schema-free databases for too long has spoilt me, but I much
more prefer starting quickly and without a schema. I know the data that
I am going to load will have a structure and will be somewhat self-describing,
so the database can still figure out what the individual parts of a record are.</p>

<p><em>On a side-note: should you be a fan of using query languages, the same
test setup can also be achieved by running the following AQL query from
the ArangoShell</em>:
```js
db._query(&ldquo;FOR i IN 1..100000 LET value = CONCAT(&lsquo;test&rsquo;, i) &rdquo; +</p>

<pre><code>      "INSERT { attribute1: value, attribute2: value } INTO examples");
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Set Up Bash Completion for ArangoDB]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/22/how-to-set-up-bash-completion-for-arangodb/"/>
    <updated>2014-10-22T23:10:32+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/22/how-to-set-up-bash-completion-for-arangodb</id>
    <content type="html"><![CDATA[<p>I was interested in how bash auto-completion works and how to write
a custom completer. After about an hour of work, I came up with a
solution that at least seems to work on Ubuntu. I now have auto-completion
for ArangoDB and all its client tools!</p>

<!-- more -->


<h2>The problem</h2>

<p>I use the command-line for almost everything, including starting
and stopping ArangoDB and its client tools. They provide lots
of options which I cannot completely memorize.</p>

<p>The bash solution for &ldquo;I don&rsquo;t know what I am looking for&rdquo; is to
press the <strong>TAB</strong> key. This will bring up a list of suggestions for
how to complete the currently entered word. I thought using the
same thing for ArangoDB&rsquo;s command-line options would be nice, too.</p>

<h2>The solution</h2>

<p>It turned out that I needed to put a shell script that generates the
auto completion for <code>arangod</code> and all the other tools into <code>/etc/bash_completion.d</code>.
From there, the system will automatically pick it up when auto-completion
is initialized.</p>

<p>The script is rather simple. For example, to have auto-completion for
<code>arangosh</code> it would look like this:</p>

<p>```bash completion script example for arangosh
_arangosh()
{</p>

<pre><code>local cur prev opts
COMPREPLY=()
cur="${COMP_WORDS[COMP_CWORD]}"
prev="${COMP_WORDS[COMP_CWORD-1]}"
opts="--help --server.endpoint --server.username" # ...all the options go here

if [[ ${cur} == -* ]] ; then
    COMPREPLY=( $(compgen -W "${opts}" -- ${cur}) )
    return 0
fi
</code></pre>

<p>}</p>

<p>complete -F _arangosh arangosh
```</p>

<p>As can be seen, the variable <code>opts</code> should be filled with the list of possible
options. Determining the options for a binary can be achieved by invoking it with its
<code>--help</code> option, e.g.:</p>

<p>```bash figuring out program options
arangosh &mdash;help | grep -o &ldquo;^\ +&mdash;[a-z-]+(.[a-z0-9-]+)\?&rdquo; | xargs</p>

<h1>this will generate something like the following output:</h1>

<h1>&mdash;audit-log &mdash;chunk-size &mdash;configuration &mdash;help &mdash;no-auto-complete &mdash;no-colors &mdash;pager &mdash;pretty-print &mdash;prompt &mdash;quiet &mdash;temp-path &mdash;use-pager &mdash;javascript.check &mdash;javascript.current-module-directory &mdash;javascript.execute &mdash;javascript.execute-string &mdash;javascript.gc-interval &mdash;javascript.startup-directory &mdash;javascript.unit-tests &mdash;jslint &mdash;log.level &mdash;log.use-local-time &mdash;server.connect-timeout &mdash;server.database &mdash;server.disable-authentication &mdash;server.endpoint &mdash;server.password &mdash;server.request-timeout &mdash;server.ssl-protocol &mdash;server.username</h1>

<p>```</p>

<p>That has to be repeated for all binaries in the ArangoDB package (i.e. arangob, arangosh,
arangoimp, arangodump, arangorestore, and arangod).</p>

<p>As the available options might change over time, I wrote a script that extracts them
from the binaries and puts together the completions file. This script can be downloaded
<a href="/downloads/code/build-completions.sh">here</a>. The script expects the already-built ArangoDB
binaries to be located in the <code>bin</code> subdirectory. Provided that ArangoDB was compiled from
source, this should already be the case.</p>

<p>The script should then be run from the base directory:
<code>bash
build-completions.sh arangodb
</code>
This will write the completions script for all binaries into the file <code>arangodb</code>.
An already generated version for devel can be found <a href="/downloads/code/completions-devel">here</a>.
Completions for current 2.2 can be found <a href="/downloads/code/completions-2.2">here</a>.</p>

<p>To activate completions, copy the appropriate file into <code>/etc/bash_completion.d/arangodb</code>.
Note that completion may need to be re-initialized once in order for it to work:
<code>bash
. /etc/bash_completion.d/arangodb
</code></p>

<h2>Quick setup</h2>

<p>For the impatient: the following command should install the completions for
2.2 and activate them:
```bash activate completion for ArangoDB 2.2
sudo \
  wget -O /etc/bash_completion.d/arangodb \</p>

<pre><code>https://jsteemann.github.io/downloads/code/completions-2.2 &amp;&amp; . /etc/bash_completion.d/arangodb
</code></pre>

<p>```</p>

<p>To see it in action, type <code>arangosh --</code> and then press <strong>TAB</strong>.</p>

<h2>Other environments (MacOS etc.)</h2>

<p><em>Note: I have checked that the above works on Ubuntu and OpenSuSE. I have no idea whether this works
with other Linux distributions let alone other shells.</em></p>

<p>Some Linux/Unix distros do not have <code>/etc/bash_completion.d</code> at all. I was told MacOS is one
of them. For such environments, downloading and sourcing the completions script should work:
```bash activate completion without bash_completion.d
wget -O ~/arangodb-completions-2.2 \</p>

<pre><code>https://jsteemann.github.io/downloads/code/completions-2.2
</code></pre>

<p>. ~/arangodb-completions-2.2
```</p>

<p>This will enable the completions in the current shell. To enable them permanently, add the
completions script to your <code>.bashrc</code> file:
<code>bash adding completions to .bashrc
echo ". ~/arangodb-completions-2.2" &gt;&gt; ~/.bashrc
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Compile ArangoDB From Source]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/16/how-to-compile-arangodb-from-source/"/>
    <updated>2014-10-16T22:24:48+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/16/how-to-compile-arangodb-from-source</id>
    <content type="html"><![CDATA[<p>Though we provide a lot of pre-built packages for the stable
versions of ArangoDB <a href="https://www.arangodb.org/install">here</a>, it
is often more interesting to play with the bleeding edge development
version. New ArangoDB features are normally added to the <em>devel</em>
branch, where they can be tested, documented and improved. When a
feature matures, it is either backported to a stable branch or will
eventually be released when the next stable branch is forked from
<em>devel</em>.</p>

<p>Contributing to the core of ArangoDB is also much easier with a
ready-to-go <em>devel</em> version. This post explains how to set one up
from scratch.</p>

<!-- more -->


<p>The following instructions are for Ubuntu 14.04 LTS, which seems to
be quite popular at the moment. Other flavors of Linux are probably
quite similar, though package manager and packages names will likely
be somewhat different.</p>

<h2>Using Vagrant</h2>

<p>If you don&rsquo;t have an Ubuntu 14 installation yet, you can easily
install one using <a href="http://www.vagrantup.com">Vagrant</a>. If you happen
to have a Linux installation already and are familiar with it, just
skip this section.</p>

<p>After installing Vagrant on your system, pick a suitable Vagrant box from
<a href="http://www.vagrantbox.es">here</a>. For example, I picked this 32 bit
box from the list:</p>

<pre><code>vagrant box add ubuntu-14.04-32 https://cloud-images.ubuntu.com/vagrant/trusty/current/trusty-server-cloudimg-i386-vagrant-disk1.box
</code></pre>

<p>After downloading the box, it can be made available via these commands:</p>

<pre><code>mkdir temp
cd temp
vagrant init ubuntu-14.04-32
vagrant up
</code></pre>

<p>After the VM is booted, connect to it via SSH:</p>

<pre><code>vagrant ssh
</code></pre>

<h2>Cloning the repository</h2>

<p>You&rsquo;re now on the Ubuntu VM. Next step is fetch the ArangoDB source
code from Github. Cloning the repository from there requires <code>git</code>. Let&rsquo;s
install it and clone the <em>devel</em> branch of the repository into a
directory named <em>devel</em> on the VM:</p>

<pre><code>sudo apt-get install git 
git clone -b devel https://github.com/triAGENS/ArangoDB.git
</code></pre>

<p>The repository contains a lot of history so cloning may take a while.
In case you don&rsquo;t need the full history, you can create a shallow
clone like this:</p>

<pre><code>git clone -b devel --single-branch --depth 1 https://github.com/triAGENS/ArangoDB.git 
</code></pre>

<p>This will reduce the download size from (currently) 375 MB to 56 MB
and should be much faster. The downside of using a shallow copy is
that there is no history and pushing and merging won&rsquo;t work most of
the time. So it&rsquo;s better used for throw-away tests only.</p>

<h2>Installing build tools and libraries</h2>

<p>Now that the repository has been cloned into directory <em>ArangoDB</em>,
we can install the required tools and libraries we need to build
from source:</p>

<pre><code>sudo apt-get install automake g++ libssl-dev libreadline-dev
</code></pre>

<p>If you prefer to install a different C++ compiler, please make sure it
has proper support for C++11.</p>

<p>Go 1.2 is also required. The official list of downloadable Go
versions can be found <a href="https://golang.org/dl/">here</a>. In the example,
I am using the 32 bit version in this example:</p>

<pre><code>wget https://storage.googleapis.com/golang/go1.2.2.linux-386.tar.gz
sudo tar -C /usr/local -xzf go1.2.2.linux-386.tar.gz
export PATH=$PATH:/usr/local/go/bin
echo "export PATH=\$PATH:/usr/local/go/bin" &gt;&gt; $HOME/.profile
</code></pre>

<h2>Compiling ArangoDB</h2>

<p>With all prerequisites set up, it&rsquo;s now time to compile ArangoDB.</p>

<p>You probably noticed that no <code>configure</code> file is shipped with ArangoDB
in the <code>devel</code> branch. To create it, we need to execute <code>make setup</code>
once. After that, <code>configure</code> can be executed to create the <code>Makefile</code>.
The <code>Makefile</code> finally contains the stuff that <code>make</code> needs:</p>

<pre><code>make setup
./configure --enable-all-in-one-icu --enable-all-in-one-v8 --enable-relative 
make
</code></pre>

<p>There first <code>make</code> run will take a while as it will compile all support
libraries (ICU, V8, libev, zlib) before it will actually compile ArangoDB.
Further invocations of <code>make</code> will not build these libraries again.
Only any changed code will be rebuilt.</p>

<p>Note that <code>make</code> can be parallelized if you have multiple processors
available. For 4 parallel <code>make</code> processes, use <code>make -j4</code>.</p>

<p><code>make</code> will produce a lot of output. The most important information, whether
or not an error occurred, can be found in its last line of its output. If
it does <strong>not</strong> say something like this, <code>make</code> has probably succeeded:
<code>
make: *** [all] Error 2
</code></p>

<h2>Starting ArangoDB</h2>

<p>When finished, <code>make</code> should have created all binaries in the <code>bin</code>
subdirectory. We can now start <code>arangod</code> and the binaries directly from
there without running a <code>make install</code>. In fact, <code>make install</code> is
awkward to do if you do many change-compile-test cycles.</p>

<pre><code>mkdir data          # creates a data directory
bin/arangod data    # starts the server
</code></pre>

<p>The server will be started as a foreground process (which is ideal
when developing the server). To stop the server, simply press CTRL-C.</p>

<h2>Connecting to ArangoDB</h2>

<p>To verify ArangoDB is actually working, open a separate terminal and
connect to it with the ArangoShell.</p>

<p>Note that if you used Vagrant, you will first need to connect to the
Vagrant box in the other terminal using <code>vagrant ssh</code> from the directory
you ran the <code>vagrant init</code> in. When connect to the Vagrant box, don&rsquo;t
forget to switch into the <code>ArangoDB</code> directory.</p>

<p>Once you&rsquo;re in the correct directory, just issue this:</p>

<pre><code>bin/arangosh
</code></pre>

<p>This should bring up the ArangoShell connected to your devel ArangoDB
instance.</p>

<h2>Making changes</h2>

<p>Time to make some changes in the code. A good place to start is usually
<code>main</code>. Here are a few places to get you started:
<code>
~/ArangoDB$ grep -r "int main" arangod/ arangosh/
arangod/RestServer/arango.cpp:int main (int argc, char* argv[]) {
arangosh/Benchmark/arangob.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangorestore.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangodump.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangoimp.cpp:int main (int argc, char* argv[]) {
arangosh/V8Client/arangosh.cpp:int main (int argc, char* argv[]) {
</code></p>

<p>Once you&rsquo;re done with your changes, you need to re-compile and run:</p>

<pre><code>make
bin/arangod data
</code></pre>

<p>Don&rsquo;t worry, <code>make</code> will only recompile what you changed plus what
depends on it and finally link it all together. This won&rsquo;t take as long
as on the previous run.</p>

<p>If you are serious about contributing to the server code, please let us
know <a href="https://groups.google.com/forum/#!forum/arangodb">here</a> so we can assist you.</p>

<h2>Getting updates</h2>

<p>We keep developing ArangoDB! To keep up to date and retrieve the latest
changes from our repository, issue the following commands:</p>

<pre><code>git pull origin devel
make
</code></pre>

<p>If <code>make</code> complains about files not found etc., the <code>Makefile</code> may have
changed. Then it&rsquo;s time for a rebuild:</p>

<pre><code>make clean
./configure --enable-all-in-one-icu --enable-all-in-one-v8 --enable-relative 
make
</code></pre>

<p>By the way, if you used special configure options and forgot them, you
can retrieve your previous options by typing <code>head config.log</code>.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Handling Binary Data in Foxx]]></title>
    <link href="http://jsteemann.github.io/blog/2014/10/15/handling-binary-data-in-foxx/"/>
    <updated>2014-10-15T20:41:30+02:00</updated>
    <id>http://jsteemann.github.io/blog/2014/10/15/handling-binary-data-in-foxx</id>
    <content type="html"><![CDATA[<p>Handling binary data in JavaScript applications is a bit
tricky because JavaScript does not provide a data type for
binary data. This post explains how to use binary data in
JavaScript actions written using ArangoDB&rsquo;s <a href="http://docs.arangodb.org/Foxx/README.html">Foxx</a>.</p>

<!-- more -->


<h1>Strings vs. binary data</h1>

<p>Internally, JavaScript strings are <a href="http://ecma-international.org/ecma-262/5.1/#sec-4.3.16">sequences of 16 bit integer values</a>.
Furthermore, the ECMAScript standard requires that a JavaScript
implementation should interpret characters in conformance with the
Unicode standard, using either UCS-2 or UTF-16 encoding.</p>

<p>While this is fine for handling natural language, it becomes problematic
when trying to work with arbitrary binary data. Binary data cannot be
used safely in a JavaScript string because it may not be valid UTF-16
data.</p>

<p>To make it work anyway, binary data needs to be stored in a wrapper
object. I won&rsquo;t go into details about ES6 typed arrays here, but will
focus on <code>Buffer</code> objects.</p>

<h1>Binary data in Foxx actions</h1>

<p>A Foxx route that shall handle HTTP POST requests containing arbitrary
(binary) body in the request body should not use <code>req.body()</code>. The
reason is that <code>req.body()</code> will return the body as a JavaScript string,
and this isn&rsquo;t going to work with arbitrary binary data.</p>

<p>Instead, the <code>req.rawBodyBuffer()</code> should be used. This will return the
request body inside a buffer.
Here&rsquo;s an example that stores the received data in a file on the server:</p>

<p>```js Foxx action that can handle binary input
controller.post(&lsquo;/receive-binary&rsquo;, function (req, res) {
  // fetch request body into the buffer
  var body = req.rawBodyBuffer();
  // create an absolute filename, local to the Foxx application directory
  var filename = applicationContext.foxxFilename(&ldquo;body&rdquo;);</p>

<p>  require(&ldquo;fs&rdquo;).write(filename, body);
});
```</p>

<p>This action can be invoked as follows if the app is mounted with name <code>app</code>:</p>

<pre><code>curl -X POST http://localhost:8529/app/receive-binary --data-binary @filename
</code></pre>

<p>This will send the contents of the file <code>filename</code> to the server. The Foxx
action will then store the received data as is in a file name <code>body</code> in the
application directory.</p>

<p>Returning binary data from a Foxx action is simple, too. Here&rsquo;s a way that
returns the contents of the file named <code>body</code> in the application&rsquo;s directory:
<code>js Foxx action that returns contents of a file
controller.get('/provide-binary-file', function (req, res) {
  // create an absolute filename, local to the Foxx application directory
  var filename = applicationContext.foxxFilename("body");
  // send the contents, this will also set mime type "application/octet-stream"
  res.sendFile(filename);
});
</code></p>

<p>It is also possible to return data from an arbitrary buffer:
```js Foxx action that returns data in a buffer
controller.get(&lsquo;/provide-binary-buffer&rsquo;, function (req, res) {
  // create an absolute filename, local to the Foxx application directory
  var filename = applicationContext.foxxFilename(&ldquo;body&rdquo;);
  // read the file content into a buffer
  var fileContent = require(&ldquo;fs&rdquo;).readBuffer(filename);</p>

<p>  // TODO: modify the contents of buffer here&hellip;</p>

<p>  // send the contents, this will also set mime type &ldquo;application/octet-stream&rdquo;
  res.send(fileContent);
});
```</p>

<h1>Example application</h1>

<p>I quickly put together an example application that shows how to handle arbitrary
binary data in Foxx actions. The example app allows uploading files to the server.
The server will then list these files and allows downloading them again.</p>

<p>The application has no CSS at all. Its only purpose is to demo the server-side code.
The application can be downloaded <a href="/downloads/code/filelist-app.tar.gz">here</a>.</p>

<p>Please note that the example application requires ArangoDB 2.3, which is currently
in development.</p>
]]></content>
  </entry>
  
</feed>
