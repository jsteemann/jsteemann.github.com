<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ArangoDB | J@ArangoDB]]></title>
  <link href="http://jsteemann.github.io/blog/categories/arangodb/atom.xml" rel="self"/>
  <link href="http://jsteemann.github.io/"/>
  <updated>2015-05-07T18:32:15+02:00</updated>
  <id>http://jsteemann.github.io/</id>
  <author>
    <name><![CDATA[jsteemann]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bulk Document Lookups]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/bulk-document-lookups/"/>
    <updated>2015-05-07T17:48:21+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/bulk-document-lookups</id>
    <content type="html"><![CDATA[<p>ArangoDB 2.6 comes with a specialized API for bulk document lookups.</p>

<p>The new API allows fetching multiple documents from the server using a single
request, making bulk document retrieval more efficient than when using
one request per document to fetch.</p>

<!-- more -->


<p>The straight-forward implementation of a client application that needs to
fetch several documents from an ArangoDB server looks like this:</p>

<p>```js fetching multiple documents from the server one by one
// list of document keys
var keys = [ &ldquo;foo&rdquo;, &ldquo;bar&rdquo;, &ldquo;baz&rdquo;, &hellip;];</p>

<p>// sequentially fetch all documents from the server
var results = [ ];
for (var i = 0; i &lt; keys.length; ++i) {
  results.push(db.collection.document(keys[i]));
}
// now all documents are contained in variable &lsquo;results&rsquo;
```</p>

<p>This works fine but causes excessive HTTP communication between the client
application and the server when many documents need to be fetched. In fact,
the above code will issue as many HTTP requests as there are documents to fetch.</p>

<p>From the performance point of view, it would be much better to reduce the
number of HTTP requests, and retrieve multiple documents from the server in
one go, using a single request.</p>

<p>This is where the new document lookup function comes into play. Provided the
documents keys are known, all the client application needs to do is to call the
collection&rsquo;s <code>lookupByKeys</code> method:</p>

<p>```js bulk method: fetching multiple documents at once
// list of document keys
var keys = [ &ldquo;foo&rdquo;, &ldquo;bar&rdquo;, &ldquo;baz&rdquo;, &hellip;];</p>

<p>var results = db.collection.lookupByKeys(keys);
// now all documents are contained in variable &lsquo;results&rsquo;
```</p>

<p>Following is a comparison of the execution times for the two different methds.
All test runs were conducted in the same ArangoDB 2.6 instance. The tests were
run from the ArangoShell. The ArangoShell and the ArangoDB server were located on
the same physical host.</p>

<p>```plain comparing single document requests and bulk requests</p>

<h2>Number of keys     Single documents        Bulk</h2>

<pre><code>     1,000               0.24 s      0.04 s
    10,000               1.23 s      0.31 s
   100,000              10.89 s      2.13 s
</code></pre>

<p>```</p>

<p>As can be seen, the bulk method can provide a substantial speedup in case lots
of documents need to be fetched by their keys at once. The actual speedups might be
even higher when using a remote ArangoDB server instead of a localhost connection.</p>

<p>In 2.6 there is currently an ArangoShell implementation for bulk document lookups.
Other drivers will follow.</p>

<p>Additionally, the server-side REST API method for bulk document lookups can be
invoked directly via HTTP as follows:</p>

<p><code>plain invoking bulk document lookups via HTTP
curl                                                  \
  -X PUT                                              \
  http://127.0.0.1:8529/_api/simple/lookup-by-keys    \
  --data '{"keys":["foo","bar","baz"]}'
</code></p>

<p>Restrictions: the bulk document API works only with document keys, not document ids.
Additionally, it works on a single collection at a time and cannot be leveraged to fetch
documents from multiple collections. Still, a client application can group document keys
by collection beforehand and send one bulk request per involved collection. Finally,
trying to fetch a document using a non-existing key will not produce an error with the
bulk API. Using the one-by-one method, trying to fetch a non-existing document will throw
an exception.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IN-list Improvements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/in-list-improvements/"/>
    <updated>2015-05-07T16:46:30+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/in-list-improvements</id>
    <content type="html"><![CDATA[<p>We have worked on many AQL optimizations for ArangoDB 2.6.</p>

<p>As a side effect of one of these optimizations, some cases involving the handling
of large IN-lists have become much faster than before. Large IN-lists are normally
used when comparing attribute or index values against some big array of lookup values
or keys provided by the application.</p>

<!-- more -->


<p>Let&rsquo;s quickly create and populate a collection named <code>keys</code> so that we can use some
IN-list queries on it later on:</p>

<p>```js setting up example data
db._create(&ldquo;keys&rdquo;);</p>

<p>// insert 100k documents with some defined keys into the collection
for (var i = 0; i &lt; 100000; ++i) {
  db.keys.insert({ _key: &ldquo;test&rdquo; + i });
}
```</p>

<p>And here is a query to all find documents with one of the provided keys <code>test0</code> to <code>test999</code>.
The IN-list here contains 1,000 values:</p>

<p><code>js using an IN-list with 1,000 values
var keys = [ ];
var n = 1000;
for (var i = 0; i &lt; n; ++i) {
  keys.push("test" + i);
}
db._query("FOR doc IN keys FILTER doc._key IN @keys RETURN doc", { keys: keys }); });
</code></p>

<p>When invoked from the ArangoShell, this takes around 0.6 seconds to complete with ArangoDB 2.5.</p>

<p>Increasing the length of the IN-list from 1,000 to 5,000 values makes this run in around 15 seconds.
With an IN-list of 10,000 values, this already takes more than 60 seconds to complete in 2.5.</p>

<p>Obviously longer IN-lists weren&rsquo;t handled terribly efficiently in 2.5, and should be avoided there
if possible.</p>

<p>I am glad this has been fixed in 2.6. Following is a comparison of the above query for different
IN-list sizes, run on both ArangoDB 2.5 and 2.6.</p>

<p>```plain 2.5 and 2.6 with different IN-list sizes</p>

<h1>of IN-list values    Execution time (2.5)   Execution time (2.6)</h1>

<hr />

<pre><code>          1,000                  0.67 s                 0.03 s
          5,000                 15.34 s                 0.12 s
         10,000                 63.48 s                 0.20 s
         50,000                   n/a                   0.81 s
        100,000                   n/a                   1.60 s
</code></pre>

<p>```</p>

<p>Looks like 2.6 handles longer IN-lists way better than 2.5! The above figures suggest that execution
times now scale about linearly with the number of IN-list values. This also leads to reductions in query
execution times of 90 % and more percent.</p>

<p>Please note that longer IN-lists will still make a the query run longer than when
using shorter IN-lists. This is expected because longer IN-lists require more comparisons to
be made and will lead (in the above example) to more documents being returned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fulltext Index Enhancements]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/07/fulltext-index-enhancements/"/>
    <updated>2015-05-07T15:08:18+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/07/fulltext-index-enhancements</id>
    <content type="html"><![CDATA[<p>This post is about improvements for the fulltext index in ArangoDB 2.6. The improvements
address the problem that non-string attributes were ignored when fulltext-indexing.</p>

<p>Effectively this prevented string values inside arrays or objects from being indexed. Though this
behavior was documented, it was limited the usefulness of the fulltext index much. Several
users requested the fulltext index to be able to index arrays and object attributes, too.</p>

<p>Finally this has been accomplished, so the fulltext index in 2.6 supports indexing arrays
and objects!</p>

<!-- more -->


<h2>Some history</h2>

<p>So far (up to including ArangoDB 2.5) the fulltext indexing in ArangoDB only supported indexing
<em>string</em> attributes. Documents in which the index attribute was an <em>array</em> of strings or an <em>object</em>
with string member values were excluded from indexing.</p>

<p>This turned out to be limiting, because more complex documents effectively couldn&rsquo;t be
fulltext-indexed.</p>

<p>Here&rsquo;s an example&hellip; Let&rsquo;s say we had a collection named <code>example</code> with a fulltext index
defined on attribute <code>value</code>, set up as follows:</p>

<p><code>js setting up a collection with a fulltext index
var c = db._create("example");
c.ensureFulltextIndex("value");
</code></p>

<p>Adding a document with a <code>value</code> attribute containing a string value adds all words contained
in the string attribute to the fulltext index:</p>

<p><code>js adding a document that is fulltext-indexed
c.insert({ value: "Fox is the English translation of the German word Fuchs" });
</code></p>

<p>Now the index can be queried using any of the words:</p>

<p><code>``js querying the fulltext index</code>
c.fulltext(&ldquo;value&rdquo;, &ldquo;fox&rdquo;).toArray();
[
  {</p>

<pre><code>... 
"value" : "Fox is the English translation of the German word Fuchs" 
</code></pre>

<p>  }
]
```</p>

<p>So far, so good. Now let&rsquo;s try it with more complex document structures.
We&rsquo;re now using arrays and objects inside the <code>value</code> attribute instead of simple
string values:</p>

<p>```js adding documents that are not fulltext-indexed  <br/>
c.insert({ value: { en: &ldquo;fox&rdquo;, de: &ldquo;Fuchs&rdquo;, fr: &ldquo;renard&rdquo;, ru: &ldquo;лиса&rdquo; } });
c.insert({ value: [ &ldquo;ArangoDB&rdquo;, &ldquo;document&rdquo;, &ldquo;database&rdquo;, &ldquo;Foxx&rdquo; ] });
c.insert({ value: [ { name: &ldquo;ArangoDB&rdquo;, type: &ldquo;database&rdquo; }, { name: &ldquo;Fox&rdquo;, type: &ldquo;animal&rdquo; } ] });</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;renard&rdquo;).toArray();
[ ]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;ArangoDB&rdquo;).toArray();
[ ]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;database&rdquo;).toArray();
[ ]
```</p>

<p>Bad luck!</p>

<p>None of the above documents made it into the fulltext index because the index attribute
did not contain string values. Though that was documented, it was not the desirable
behavior.</p>

<h2>2.6</h2>

<p>Retrying the same operations in ArangoDB 2.6 changes the picture.</p>

<p>All the above example documents are included in the fulltext index in 2.6. The fulltext index
in 2.6 can index <em>string</em> values, <em>object</em> values (it will index the object&rsquo;s members if they are strings)
and <em>array</em> values (it will index the array members if they are strings or objects). Indexing
is still limited to one sub-attribute level, so in deeply nested structures only the
top level ones will be indexed.</p>

<p>A few example queries on the index in 2.6 prove that now all the data from the more
complex documents can be queried:</p>

<p>```js querying the fulltext index in 2.6
c.fulltext(&ldquo;value&rdquo;, &ldquo;renard&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : { 
  "en" : "fox", 
  "de" : "Fuchs", 
  "fr" : "renard", 
  "ru" : "лиса" 
} 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;ArangoDB&rdquo;).toArray();
[
  {</p>

<pre><code>... 
"value" : [ 
  "ArangoDB", 
  "document", 
  "database", 
  "Foxx" 
] 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : [ 
  { 
    "name" : "ArangoDB", 
    "type" : "database" 
  }, 
  { 
    "name" : "Fox", 
    "type" : "animal" 
  } 
] 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;database&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : [ 
  "ArangoDB", 
  "document", 
  "database", 
  "Foxx" 
] 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : [ 
  { 
    "name" : "ArangoDB", 
    "type" : "database" 
  }, 
  { 
    "name" : "Fox", 
    "type" : "animal" 
  } 
] 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;лиса&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : { 
  "en" : "fox", 
  "de" : "Fuchs", 
  "fr" : "renard", 
  "ru" : "лиса" 
} 
</code></pre>

<p>  }
]</p>

<p>c.fulltext(&ldquo;value&rdquo;, &ldquo;prefix:Fox&rdquo;).toArray();
[
  {</p>

<pre><code>...
"value" : "Fox is the English translation of the German word Fuchs" 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : { 
  "en" : "fox", 
  "de" : "Fuchs", 
  "fr" : "renard", 
  "ru" : "лиса" 
} 
</code></pre>

<p>  },
  {</p>

<pre><code>... 
"value" : [ 
  "ArangoDB", 
  "document", 
  "database", 
  "Foxx" 
] 
</code></pre>

<p>  },
  {</p>

<pre><code>...
"value" : [ 
  { 
    "name" : "ArangoDB", 
    "type" : "database" 
  }, 
  { 
    "name" : "Fox", 
    "type" : "animal" 
  } 
] 
</code></pre>

<p>  }
]
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Subquery Optimizations]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/subquery-optimizations/"/>
    <updated>2015-05-04T13:26:00+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/subquery-optimizations</id>
    <content type="html"><![CDATA[<p>This is another post demonstrating some of the AQL query performance improvements
that can be expected in ArangoDB 2.6. Specifically, this post is about an optimization
for subqueries. AQL queries with multiple subqueries will likely benefit from it.</p>

<!-- more -->


<p>The following example queries use the same <code>users</code> collection with 100,000 documents
that was used in the <a href="/blog/2015/05/04/return-value-optimization-for-aql/">previous post about return value optimizations</a>.
Again, the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>.</p>

<p>We&rsquo;ll start with a query that uses a single subquery extracting all users from the
collection into a flat result array:</p>

<p><code>plain example query with single subquery
RETURN [
  (FOR u IN users RETURN u)
]
</code></p>

<p>This query is equally fast with ArangoDB 2.5 and 2.6, no changes here.</p>

<p>Let&rsquo;s ramp it up to using two subqueries, one for users with a <code>gender</code> attribute value
of <code>male</code>, and one for users with <code>gender</code> attribute value <code>female</code>. No indexes were used
for the extraction in 2.5 nor 2.6:</p>

<p><code>plain example query with two subqueries
RETURN [
  (FOR u IN users FILTER u.gender == 'male' RETURN u),
  (FOR u IN users FILTER u.gender == 'female' RETURN u)
]
</code></p>

<p>The query takes 16.6 seconds to execute in 2.5, but only 2.95 seconds with ArangoDB 2.6.
This 80 % reduction in execution time is due to ArangoDB 2.6 being a bit smarter about
subqueries than 2.5 is.</p>

<p>In the above query, the two subqueries are independent, so not only can they be executed in
any order, but they also do not rely on each other&rsquo;s results. ArangoDB 2.6 will detect that
and avoid copying variables and intermediate results into subqueries if they are actually not
needed there. 2.5 copied all variables into subqueries unconditionally, even if variables
were not needed there.</p>

<p>In 2.6, any AQL query with multiple subqueries will benefit from this optimization. The
performance improvements will be greater if subqueries late in the execution pipeline have a lot of
intermediate results created in front of them, but do not rely on these intermediate results.</p>

<p>Another nice example for a 2.6 speedup is extracting a single attribute per subquery, as is done
for the <code>name</code> attribute in the following query:</p>

<p><code>plain extracting a single attribute in two subqueries
RETURN [
  (FOR u IN users FILTER u.gender == 'male' RETURN u.name),
  (FOR u IN users FILTER u.gender == 'female' RETURN u.name)
]
</code></p>

<p>This takes 42 seconds to execute in 2.5, and only 0.86 seconds in 2.6. This is a more than
95 % reduction in execution time. It is caused by a mix of factors, one of them again being
the subquery optimization that avoids copying unneeded intermediate results.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Return Value Optimization for AQL]]></title>
    <link href="http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql/"/>
    <updated>2015-05-04T10:32:43+02:00</updated>
    <id>http://jsteemann.github.io/blog/2015/05/04/return-value-optimization-for-aql</id>
    <content type="html"><![CDATA[<p>While in search for further AQL query optimizations last week, we found that intermediate AQL
query results were copied one time too often in some cases. Precisely, the data that a query&rsquo;s
<code>ReturnNode</code> will return to the caller was copied into the <code>ReturnNode</code>&rsquo;s own register. With
<code>ReturnNode</code>s never modifying their input data, this demanded for something that is called
<em>return-value optimization</em> in compilers.</p>

<p>2.6 will now optimize away these copies in many cases, and this post shows which performance
benefits can be expected due to the optimization.</p>

<!-- more -->


<p>The effect of the optimization can be demonstrated easily with a few simple AQL queries.
Let&rsquo;s start with a query that simply returns all 100,000 documents from a collection <code>users</code>:
<code>FOR u IN users RETURN u</code> (the source data for the collection can be found <a href="/downloads/code/users-100000.json.tar.gz">here</a>).</p>

<p>This query&rsquo;s execution plan is already straight-forward and simple:</p>

<p><img src="/downloads/screenshots/return.png"></p>

<p>So where&rsquo;s the problem?</p>

<p>The <code>ReturnNode</code> in this query (and other queries too) will copy its input data into its own output
register, only to finally hand the results to the query&rsquo;s caller. This copying is most often unnecessary
as the <code>ReturnNode</code> will not modify its input. So the idea was to get rid of the copying action and
tell the query&rsquo;s calling code in which (now different) register to look for the results.</p>

<p>Optimizing away the copying inside the <code>ReturnNode</code> made the query run faster already.
The same query now returns the 100,000 documents in 0.24 to 0.26 seconds, compared to 0.27 to 0.30 s
before applying the optimization.</p>

<p>Returning just an attribute of each document shows about the same improvement rates. The execution
times of the query <code>FOR u IN users RETURN u._key</code> drop to between 0.13 and 0.14 seconds with the
optimization, from initially between 0.15 and 0.17 seconds.</p>

<p>Another example query, <code>FOR i IN 1..1000000 RETURN i</code>, now runs in 0.58 to 0.61 seconds with
the optimization, compared to between 0.77 and 0.81 seconds without it.</p>

<p>These absolute figures may not look overly impressive, but they indicate relative improvements of
between 10 and 25 %, which is quite nice. This is effectively saved CPU time that can now be used
for something more productive.</p>

<p>Of course the performance improvements may not be that high for every imaginable AQL query.
Though the optimization may be active in most AQL queries, its effect will only be measurable
for queries that return a significant number of documents/values. Otherwise the share of the
<code>ReturnNode</code>&rsquo;s work in the query&rsquo;s overall computations may be too low to have any effect.
Additionally, the more work a query spends in performing other operations (e.g. filtering,
sorting, collecting), the less relevant will be the overall effect of the optimized <code>ReturnNode</code>.
Finally, when query results need to be shipped from the server to the client over a network,
the relative effect of the optimization may diminish further.</p>

<p>So your mileage may vary. But the optimization will not do any harm, and together with some
other query optimizations already finished for 2.6 it will contribute to many AQL queries
running faster than before.</p>

<p>AQL queries will benefit from the optimization automatically in ArangoDB 2.6, without requiring
any adjustments to the query string, the server configuration etc. The optimizer will automatically
apply the optimization for the main-level <code>ReturnNode</code> of every AQL query.</p>

<p>On a side note: the optimization will not be shown in the list of applied optimizer rules for the
query. This is because the optimization is performed in some different place in the query
executor, after applying the optimizer rules.</p>
]]></content>
  </entry>
  
</feed>
